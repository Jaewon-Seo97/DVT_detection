{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgxadmin/anaconda3/envs/zzaem2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dgxadmin/anaconda3/envs/zzaem2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dgxadmin/anaconda3/envs/zzaem2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dgxadmin/anaconda3/envs/zzaem2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dgxadmin/anaconda3/envs/zzaem2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dgxadmin/anaconda3/envs/zzaem2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/dgxadmin/anaconda3/envs/zzaem2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dgxadmin/anaconda3/envs/zzaem2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dgxadmin/anaconda3/envs/zzaem2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dgxadmin/anaconda3/envs/zzaem2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dgxadmin/anaconda3/envs/zzaem2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dgxadmin/anaconda3/envs/zzaem2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "from keras.utils import multi_gpu_model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import cv2\n",
    "from skimage import io\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,confusion_matrix,accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support,roc_curve,auc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import *\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import random\n",
    "import pandas as pd\n",
    "from keras import losses\n",
    "from skimage.io import imsave, imread\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "# from tensorflow.keras.applications.resnet152 import ResNet152\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dgxadmin/gcubme_ai/Workspace/JW_Seo/DVT_detection/Radiomics/code/deep_learning'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(32), Dimension(32), Dimension(1)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img = Input(shape=(32,32,1))\n",
    "input_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.applications.VGG16(include_top=False, input_tensor=Input(shape=(32,32,1)), weights=None, input_shape=(32,32,1), pooling='max_pooling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        640       \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_6 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 14,714,049\n",
      "Trainable params: 14,714,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_app_16(inputs):\n",
    "    \n",
    "    input_x = keras.applications.VGG16(include_top=False, input_tensor=inputs, weights=None, input_shape=(32,32,1), pooling='max')\n",
    "    x = input_x.outputs[0]\n",
    "    \n",
    "#     flatten_layer = Flatten()  \n",
    "#     out = flatten_layer(x)\n",
    "    \n",
    "\n",
    "#     GAP_layer=GlobalAveragePooling2D()\n",
    "#     out = GAP_layer(x) \n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    out = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=out)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# input_img = Input(shape=(32,32,1))\n",
    "# model = vgg_app_16(input_img)\n",
    "# model.summary()   \n",
    "\n",
    "def vgg_app_19(inputs):\n",
    "    \n",
    "    input_x = keras.applications.VGG19(include_top=False, input_tensor=inputs, weights=None, input_shape=(32,32,1), pooling='max')\n",
    "    x = input_x.outputs[0]\n",
    "    \n",
    "#     flatten_layer = Flatten()  \n",
    "#     out = flatten_layer(x)\n",
    "    \n",
    "\n",
    "#     GAP_layer=GlobalAveragePooling2D()\n",
    "#     out = GAP_layer(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    out = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=out)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# def resnet_app_50(inputs):\n",
    "    \n",
    "#     resnet = keras_resnet.models.ResNet50(inputs, include_top=False, freeze_bn=False)\n",
    "#     resout=resnet.outputs[3]\n",
    "    \n",
    "# #     flatten_layer = Flatten()  \n",
    "# #     out = flatten_layer(resout)\n",
    "\n",
    "#     GAP_layer=GlobalAveragePooling2D()\n",
    "#     out = GAP_layer(resout)  \n",
    "#     out = Dense(1, activation=\"sigmoid\")(out)\n",
    "    \n",
    "    \n",
    "#     model = Model(inputs=inputs, outputs=out)\n",
    "    \n",
    "#     return model\n",
    "def resnet_app_50(inputs):\n",
    "    \n",
    "    input_x = keras.applications.resnet.ResNet50(include_top=False, input_tensor=inputs, weights=None, input_shape=(32,32,1), pooling='max')\n",
    "    x=input_x.outputs[0]\n",
    "    \n",
    "#     flatten_layer = Flatten()  \n",
    "#     out = flatten_layer(resout)\n",
    "\n",
    "#     GAP_layer=GlobalAveragePooling2D()\n",
    "#     out = GAP_layer(resout)  \n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    out = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=out)\n",
    "    \n",
    "    return model\n",
    "# input_img = Input(shape=(32,32,1))    \n",
    "# model = resnet_app_50(input_img)\n",
    "# model.summary()\n",
    "def resnet_app_152(inputs):\n",
    "    \n",
    "    input_x = keras.applications.resnet.ResNet50(include_top=False, input_tensor=inputs, weights=None, input_shape=(32,32,1), pooling='max')\n",
    "    x=input_x.outputs[0]\n",
    "    \n",
    "#     flatten_layer = Flatten()  \n",
    "#     out = flatten_layer(resout)\n",
    "\n",
    "#     GAP_layer=GlobalAveragePooling2D()\n",
    "#     out = GAP_layer(resout)  \n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "#     x = Dense(128, activation=\"relu\")(x)\n",
    "#     x = Dense(64, activation=\"relu\")(x)\n",
    "#     x = Dense(32, activation=\"relu\")(x)\n",
    "    out = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=out)\n",
    "    \n",
    "    return model\n",
    "def resnet_app_34(inputs):\n",
    "    \n",
    "    resnet = keras_resnet.models.ResNet34(inputs, include_top=False, freeze_bn=False)\n",
    "    resout=resnet.outputs[3]\n",
    "    \n",
    "#     flatten_layer = Flatten()  \n",
    "#     out = flatten_layer(resout)\n",
    "\n",
    "    GAP_layer=GlobalAveragePooling2D()\n",
    "    out = GAP_layer(resout)  \n",
    "    out = Dense(1, activation=\"sigmoid\")(out)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=out)\n",
    "    \n",
    "    return model  \n",
    "\n",
    "def resnet_app_18(inputs):\n",
    "    \n",
    "    resnet = keras_resnet.models.ResNet18(inputs, include_top=False, freeze_bn=False)\n",
    "    resout=resnet.outputs[3]\n",
    "    \n",
    "#     flatten_layer = Flatten()  \n",
    "#     out = flatten_layer(resout)\n",
    "\n",
    "    GAP_layer=GlobalAveragePooling2D()\n",
    "    out = GAP_layer(resout)  \n",
    "    out = Dense(1, activation=\"sigmoid\")(out)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=out)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance(pred, test_y):\n",
    "    tp=0\n",
    "    fp=0\n",
    "    tn=0\n",
    "    fn=0\n",
    "    alpha = 0.0001\n",
    "    for cm in range(len(test_y)):\n",
    "        if list(test_y)[cm]==0:\n",
    "            if pred[cm]<0.5:\n",
    "                tn+=1\n",
    "            else:\n",
    "                fn+=1\n",
    "        else:\n",
    "            if pred[cm]<0.5:\n",
    "                fp+=1\n",
    "            else:\n",
    "                tp+=1\n",
    "    print(tp, fp, tn, fn)\n",
    "    sensitivity= (tp+alpha)/(tp+fn+alpha)\n",
    "    specificity= (tn+alpha)/(tn+fp+alpha) \n",
    "    acc = (tp+tn+alpha)/(tp+fp+tn+fn+alpha)\n",
    "    prec = (tp+alpha)/(tp+fp+alpha)\n",
    "\n",
    "    return tp, fp, tn, fn, sensitivity, specificity, acc, prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1624, 32, 32, 1) (1624,)\n",
      "StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
      "TRAIN: 1299 TEST: 325\n",
      "RETRAIN: 1300 RETEST: 324\n",
      "(1300, 32, 32, 1) (324, 32, 32, 1)\n",
      "163 1 0 973\n",
      "TRAIN: 1299 TEST: 325\n",
      "RETRAIN: 1300 RETEST: 324\n",
      "(1300, 32, 32, 1) (324, 32, 32, 1)\n",
      "0 164 163 1135\n",
      "TRAIN: 1299 TEST: 325\n",
      "RETRAIN: 1300 RETEST: 324\n",
      "(1300, 32, 32, 1) (324, 32, 32, 1)\n",
      "0 327 326 1298\n",
      "TRAIN: 1299 TEST: 325\n",
      "RETRAIN: 1300 RETEST: 324\n",
      "(1300, 32, 32, 1) (324, 32, 32, 1)\n",
      "0 489 488 1461\n",
      "TRAIN: 1300 TEST: 324\n",
      "RETRAIN: 1300 RETEST: 324\n",
      "(1300, 32, 32, 1) (324, 32, 32, 1)\n",
      "0 650 1461 1623\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAACaCAYAAABmDna+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAtlElEQVR4nO2dW6hl13Wm/3nqopKlcqlKddXFtnwhdhRMEoTdTTf90I6xyYtNQyB5aNRg8FNDgvNgpRva5MW4+yFP/WSIsRpCGkMCFk2aIExCE2Mcq912bEVRJOtWJZWqVCXJkqxLqc6Z/VC7tsf6zj5jnFVnn3121fk/ELXnWWuvNS9jzr00x7/GaL13GWOMMcaYzbOy0xUwxhhjjLne8AOUMcYYY8xI/ABljDHGGDMSP0AZY4wxxozED1DGGGOMMSPxA5QxxhhjzEi29ADVWvtsa+3x1tqTrbUH5lUps7uwHZmtYhsy88B2ZMbQrjUOVGttj6R/lvRpSWck/UDS7/Xe/3Gj7xw8eLAfPXp0Wr58+fLg+Orq6vTzvn37BsdYz3fffTet3969e6ef19bWBsf27Nmz4X1nXZvHL126NPPzrDJhu2K5tTY4xjazHSR+P+tbaX0fVPeO56+sDJ+733rrrcH3eu/Di+V1Hm1Ht9xySz9y5Mi0vH///sHxWI52IK3vw8r+2S/ZMZar8cq+y/GqrhXHhONTjW1Wl6pNtDPOnXfeeWfDe7Mesfzaa6/pzTff3JQdXYsNtdZ6bNuYdZD9S/u76aabBuUDBw7M/CytXw84L7M+qhi7nsTzx9aD6x7HPasX78X+JfHe2fy8ePGi3njjjW1di2666aZ+yy23TMs333zz4Hgc32rNZbu5dkW72qqdcH2J5bfffntw7I033hiU43ov5XZU/baOXY+z744l9iHncBy3t956S5cuXZppR3tn/XGTfELSk733pySptfY/JX1O0obGdvToUX3lK1+Zli9cuDA4/vrrr08/33nnnYNjnKDPP//8oEyDij+wNIj3vve9g/Irr7wyKJ89e3ZQfvXVVzc8/vTTTw+OnT59elDmIB87dmxQPnny5PQzF1Ma25tvvjkoc4LFPjh//vzg2C9+8YtBOU56af3E573j+bfeeuvg2E9+8pPp5+oBcgaj7ejIkSP60pe+NC2/733vGxyP5fjALq3vBz4AENpVdoyTsFpo4oLJsfz5z38+KNOGufjGH+b3vOc9g2O8NtvMa0U7pE3SBjmHz5w5Myg/88wzg3K0q+x/VB588EGNYLQNtdYGbcseDPnDxocg2t+HP/zhQfljH/vYzM/S+vXgtttuG5Sr/0HLHpyr9YQ/jNFODh06NDjG/qFtc9179tlntRGsF9cTlnnvaCfZw9dXv/rVDeuwAaPt6JZbbtFnPvOZafnee+8dHD916tT0M9vF9YLz9vjx44Pyhz70oeln/oZxTtNO2IdcBy9evDj9/OSTTw6Offe73x2U43o/61pxTF577bXBsXPnzg3K8TdfWm+j2cM07beCthJtnHP4xIkT08/f+973NrzmVlx4d0qKs+bM5G8DWmtfbK090lp7hJ1ljK7Bjjhhza5ntA05A4OZwWg7ynbazI3PVh6gZm1prVuVeu9f773f13u/7+DBg1u4nblBGW1H3D0zu57RNpS5fsyuZbQd0V1rdhdbceGdkXR3KN8l6YXsC733wRYdt+fiFtsYvQ+/Kw23o7llXF2L59MFEo/T9cWtVfqAuY0bz+d9+X83lY88Uk1s1pvlMT8wW/wxGm1HKysrg/qyrXFrPLMxaf34bMUHP9Z9Gb/PLXjWk+NTaSAiY/sgztFKo8BrcSxo77GddEvG8kibGm1D0rBf6JaL7pTK3X333XcPyvfcc8+gHN04hw8f3rAO0vo+qVwxma1XLj22OV6LxyrdV1ZPHqOt816Vyzlzfcc2VlqqGYy2o3379g3Gl5KBON5sJ+dO1Q90n0c4Lzmnt3I+68U5TWK7eF1eKxtbafgbWLWpInMFZ+7sTOaxlR2oH0j6SGvtntbafkm/K+mhLVzP7E5sR2ar2IbMPLAdmVFc8w5U7/1ya+0/SvprSXskfaP3/ujcamZ2BbYjs1VsQ2Ye2I7MWLbiwlPv/a8k/dWc6mJ2KbYjs1VsQ2Ye2I7MGLb0ADWWtbW1weuv1DlEbUGlCaEvNbsW/fX0s1K3wddJ6XuNfln6kiv/cHw9UhqGa2Cb+NYi9RHsk/jKaPV6Pv3r1HXwNeXoI6amIfbvNYQxGM3evXsHr3/HkBXSULPCsc/84FIdLiDCsWefV9q7eG/2G22B48N2MZxAhHbCemV2VuloWK40gFFfxVANMVzINehXRtFaG8wBhg/44Ac/OP2c6Vqk9SFX7rrrrkE5zvnbb799cIx9Ty1GFvpCGtpBFstmFln8NK5jmdZo1vlxTjIMTNXGMfF9qE+LbchCkMyLAwcODMJW8HV4zoeMLH6YNJwv1EONjaVFMn0R+5g2zHpn+uOq3llYg0wDNutaVZytuH7zvvFeWcxJp3IxxhhjjBmJH6CMMcYYY0aycBde3BrLXqWvUo/wu9z25fEIwwNwK5tbllmaDd6HkYUJo8vG7X5u2XLrm+4lbo++8MIv37jltmO1nc97M1hldOll/bfdrhfpii3EMaLLNZaraMzZti7LlfuP/VKl6cjqRapwAbEdVeiBKsVHbBfbWLWpeuU5S+WSuQ7nzf79+wdzjy686Lajiy6+ti4NI0RL0kc+8pF197oKXWGVC7kKaxDXU167KmcR1umOrtyDPB77qFp7GK2a8oHMdcP1dxHrT+TAgQOD6OO09zg/Kpd/ZRtZdPyxab84XvH7XFto7+xz2mQcb44tI9wzUwGhLWSwnpzTJNoVf+9idpLtCmNgjDHGGLMr8QOUMcYYY8xI/ABljDHGGDOShWqgVldXB7oe+k6jfoj+3yolCol+2CrtBX2+1Igwh1/041KjwHrSr81X7k+ePLnhuWxjlZk9Qh0X6/Xiiy8OyvSZU8sS+7NKMbMI4phmWo8sxc8s2A9ZyIqxaU7Yp/F8jiXtagysR5XwlO2I9a5SeFAfwHtnYRCopYjzcLtz1e3fv3/wyjlfN496CqZqef/73z8oUwPFVC5xbKnnqfRB7N8xmpAq9ACPx3ryvlUYDa6RsUy7Z5m2PqaNtN1Y70UkjN6/f7/uuOOODY/H+lHnWL12z/PHhIip5nyWYoljW4XxyODvTqUv5nhmaaVokx/96EcHZWqiaNMvvfTS9DO1WNRubYR3oIwxxhhjRuIHKGOMMcaYkfgByhhjjDFmJAvVQF2+fFkvv/zytEz/aNQDVdCHS19q9O9T45Sll5DGxYWir5nX5r3pP45h8enj5X15L4afj/diyhimT2AaDR5nH8Rr89zt1qvMIks9EKlir4z5PttZpU+oUgRFqI2ptHUsZ3HPKh0TNSnxfPZXNdZV/JnYZ1nMqO3W1e3Zs2egR2T8tqhroqaJGijqpzh2MaYMY7txHlY6F45dHHdqmjh21dhk9640eWzHxYsXp5+5vrIejNfDMtebOFe4Jkb91GbXiK3QWhuMAdeb2HbadBVXju3OxoB9PDZdTuxHzkv+DlXrXrS7KiUV652tz5lOUZI++clPDsr8DaR9ZzGoYv85lYsxxhhjzBzxA5QxxhhjzEj8AGWMMcYYM5KF58KLflzmxYlU8XuoL2GMk1iu8jbRt0yfbhafqfJr0ydexVOJ0PdKX3SWO4n9x/tQZ1BpbGI7qrha2w1zKtK3ncWCoY+9ykEXNT/052djJ6230SxvV5bvi+dK68cg6gOqWFfUMXF+RG0d46FUObgqfWHsE/ZfXA+qNmyVlZWVwf2pT4uaKGotmBuPNkU9UIw3w/hr/G6l0aMNZeNeaWioH4prF3ODVRqaLM8e1y32Ndcinp/p4diGRceBYl7OTDPIdYp153idPn16UD579uz0M+2g0ilxrnEM4vm8dpVTkcR2VrHwqAlmrth4LR5j3KePf/zjgzJjLnItir/jjD12/vz56WfO54h3oIwxxhhjRuIHKGOMMcaYkfgByhhjjDFmJAvVQElDHzH9oxH6bKsy/bZZPq8q/1QV6yb6cem/pzaAsZoyv3ylTaGfm0S9CvuW12J+I/rj6feN2pZMe7UI3UHvfXDPTLPGdle5yDItWKWNq3R7WS69ykar2CuZLoE2ye9m+aioG2D/8Lucl1mevaxN2x0HqveexuiJ2iJqKTh3Lly4MCifO3duUH7hhRemn59//vnBMY5blWOO2pV4nGNBzRP1OdQ5xTWAep0qplJm29SuVHq+LA8nyebcouLTxTpk/VTpMTketKPnnntu+pl9VOWrY59nefmqdYz9mo19FSeR7ci0ubSDe++9d1BmTkrGjcrWrjNnzmxYj+w5xTtQxhhjjDEj8QOUMcYYY8xIFurCa60NtqiZjmXMq5Tc6s62hSv3B7cGK9dM/H6WQkNavxWepejItgqlOsUM+3Oj+0jDFDLS+ldoeX6WhiT2AV973w5aa6mtxO3XykVXubPitjq3vXkuwxZUaQri1jddi7xWtY0ex482x3Llwo6v9NJeWea1GJqELqjYJ5kbYbtdeHv37h24ljgf6FqIVH2SubGrV8KztBjSejvJ0rNwTtOdkrlexqaB4bWztCO0R7a5ch/GdTAbp0W48Hrvg7ZzfOL6UYVQYTtpG9Feq7A0nHfspyzcCF/pZ5m2kLWLY8lURqwH149srOlar8LGsL+j3bEesd7ZfPYOlDHGGGPMSPwAZYwxxhgzEj9AGWOMMcaMZKEaqD179gx8tXz1Mvow6d+kPoj+bfppoz4oe+1eWu8rrXyn0QfMa7GeVb3j96s0GPQnZ9ofahIYToG+ZmpAqHmK/ctXseN3L168qEWQ6WWiz7p6hZ/jQz86+zy7Fse20jjE49QRsP+pNWK74nhzrFmPLI3DrGtHMj2KtL5PsrAQWaiG7dZA7du3TydOnJiWqSGJegv2B3UcLGeakUrzVL1uzv6MGptKmzJW1zSGSqsVybRvs65Fe91u2xhD733Qj7TpWPdKf0m7oeYn0/VWaaXY55ldVeEvKi1dXAfHaOOkPO0P1+Zszs6qF+dpDD9C7W4cU2ugjDHGGGPmiB+gjDHGGGNGUj5Atda+0Vo731r7afjbkdbaw621Jyb/Hs6uYYztyGwV25CZB7YjMy82o4H6pqT/Lul/hL89IOk7vfevtdYemJS/XF1oz549A+0NdU4RhmGnz5faFMY8idA/XMXboK+VWoExGqhKJzMm9H+VRiP6jyt9A2NGcSx4r+g/Zl/HNj/11FMb3fKbmpMdMQ4U/er0dUcyPdvVa0ei7VDfwHIWQ0par3GIdaniPvFe1BbEMvujaiOvHbUE1D+MSeMwqy6xzdQsRPvdQOfyTc3Jhqo4ULEubAP1EmfPnh2UT58+PSjH77M/szhGV+sZYf9Gm6rijlV6wFjm+strcf2lbWdpeViuUphkGiiuRbFerHPgm9qmtSiL1cc1lXOefZjFQhyrnWOZNh3ryTlNTSX1bbTRaNO8L2Eb2SdxveFcoY6Xx2lXMaWSNEyN88orrwyObVYfWO5A9d7/j6SX8efPSXpw8vlBSZ/f1N3MrsV2ZLaKbcjMA9uRmRfXqoE60Xs/K0mTf49vdGJr7YuttUdaa49kb2aYXck12RETHZtdzTXZEP+v2ux6rsmOFvXWsVlOtl1E3nv/eu/9vt77fVk6EGMyoh1x69aYzRBtiKkujNks0Y7o+jW7i2uNA3WutXaq9362tXZK0vnNfGllZWXgz6b/M/qAqcugZoc+ePpt4/n0S2exfWbdm770LB5HlceJGpJY5n0rLRZ3YqJfmz5x9gGvxYdb9mf8wTl16tTg2Msv/3I3nP7wgmu2ozi+HIMs39pYYnuqGF8V7PPYx9W1slxNUqr3SDUKUq7lyuI4XQvRDmmTsTwij9k12RDjQNH+Y59RL0KNDsvclcg0MqTSXmRxzCoNVKWJinbBtaeKvcS1PN6rWhOq3HhV/sA5cc1rURYzLItJV8VXor4ranp4LseLGjbO+ex3ims/tUTVvbO1rMrnynK8Nu9Txeyj5on63BjPkG3cbKyxa92BekjS/ZPP90v69jVex+xubEdmq9iGzDywHZnRbCaMwZ9L+p6kX2mtnWmtfUHS1yR9urX2hKRPT8rGbIjtyGwV25CZB7YjMy/KvdDe++9tcOhTc66LuYGxHZmtYhsy88B2ZObFQnPhraysDHyc9JtH3yr93BR90t+ZxUSqci3xWtSEkEwXU+WuynyrmSZEWu8TZzyaeG8KratYWPQ9k6g54tjEPtgmfcKAvXv3DuL2ZP7+ajwqom4h0xlJdQywLNZTNT60UfrsYz2pd2CZcynT3fC7jFXDecc+yOK88Nws59S82bt370ADxbGKfUKtITVPjDuWxfsZ+yIN61WtTRHaDLUrmT1nedik9faatauac7QhtjmLF5blLR2rUbxW4n04lzJtEcniDUrDtnEsszVZWr/28I34uJ5U2iwez2Idsh6032qNjONb6XjZppdeemlQjrnvpGHsJ66nm/2dcCoXY4wxxpiR+AHKGGOMMWYkfoAyxhhjjBnJQjVQvfdBTBX66KM2o4qJUelNog+TPvbbbrttUKaugxoR+q6jL7bScdBfnPlW6dNl/1DPw+OxbyvdF33NjLNFf3L0RfPc2H+L0B0w/1RmC7SLKhZO5rOvtBoc+yqeTRYfKLPnWcdjucp1V+Wzy2KTca5UeogxMWKifS9CDxXrnmmgKu1bFptJymNfVd/luLP/4zpZ2UgVCyiez2OVtjHTb46N61TpXzd77c3G8tkq0Vaz/KVVTspqvOK1qvX78OFhLuTqNy3GLqsyhlS639jOKn9olbMv05cRahWpEWY8tzh3OK/iudl9vQNljDHGGDMSP0AZY4wxxoxkoS68tbW1wfYgt81iihC68Jj8k1uS2dY3j9GFx21FhgDIUh5wGzZ7jVXKt82rc6vtTvZnBsMWcAue252ZCy9ulS7Chbe6ujqwB77KHMezcmWyz3l+Fr6BY80+q7boY5nb9aTaCo91qeyGr+yOIXu1etZx2lWc15k7dbtdeGtra4N+qtxMkerczG1ZuTgJ10G6VzJXA/ue8ySbq5WcgNB+s5Rdla1XrqzMNbcTLrzYV1kogkxyIa0frxhmQxqu71y3qnRNVTiXeP7Y0Bm8VpzXPMY2U5rDeR/nB3/z+TtUzSXeO4YfienIpOEaaReeMcYYY8wc8QOUMcYYY8xI/ABljDHGGDOShWqgpKGvNgsPUIW1r15Hz/zf1P/wu/TZZ6EHqEmgDonfpZ87+pfZH/TZ8lr0TcdrV6Ho+V2mQ6HPPGqO6G+P/bUI3cHa2trAR80+z/z5JHvtmIzVu/HamY1WugN+l2MQ9RXUWlRzJ9NqVTquCrYj2jT1Pdkr3/OmtTZoS/YaPvuT6wM1ZZy3sS2V5o5kr1dLwz7k2FSpoTJNH69V6Xd4PK4X1JyyzLWGsJ5RXzWmDdtB7z2tf1wDeB7XB+qWjh07NijHPq+uVWkVs9RnTMtDG+W1s7RS1RrJ308ej3ONGqdqzaRNMqxBTOUSP0vDeWcNlDHGGGPMHPEDlDHGGGPMSPwAZYwxxhgzkoVqoPbs2TOI3ZDpK6hTqny89HfS3896ROhbZZmakXgv+n9jbAlpvV+b7Yp+2yrOE/3DrGfUAzCsPWEfZOkSpGE72cZ4Lsd0O6AGin702Mf0i1e6CNpRvE+VtqeKv8TxyzQ/WTqQqp6V/o1kqVxYjyoWVqW7yeKebTZ9wjxorQ3awnZlscQqsj6q4imRKo1MXAPY19SycC2qdKaRLNWNlGukqvh0nCe8F9fM2C6ugdF2F6XHjDq0LAbY2DQ+7Jdok1k8sFlUmtd47ypWXlYvKbfxKrUWiRqoau3hb/G5c+fSctREMR5V1PBldfQOlDHGGGPMSPwAZYwxxhgzEj9AGWOMMcaMZOEaqOh7zWLdVP76TJc069pVvbLvUiMSfcD0u9Lfz5gx9OdHn30VD4X15LWzelS5xaprx/FgPeN3F6GBYi489mmsK3UglT6I9Y/9xrhFLNMGK+1MPF7Vq4rrEu9dxUTL2kg4FzIN36x78XhsM/sniz+1HWw2bxr7h2sP20Gby+LiVPHsshhI0tDWOTbMFZbFDmNdWC9qe1gPakiivoT6nCyWlbR+LKjXiWX+TsR6LsKGGAcq0zGxTyvNa6a9qWLOcd5VeeJiP1a/FRwvxlDKftNo39WaGvszy3k6qx4XL15My9FGq9/ejfAOlDHGGGPMSPwAZYwxxhgzEj9AGWOMMcaMZOEaqJgHibGHom+cvm1SxaaI5epcUsWgidqByn9f+aJj3fjdSsNAou8601pJ6/uAcaMyjRmPxT7Y7vg90pU+i/7rLPYK9RbsY9pZ5mdnLiWWqTvj+FE3EutdaQOoS4gaMCnXQFGXwTLnR+wj2i/nLPugikeTaemy3IDXE1msmyomEqlsKOszztNKN1rF5IlwzmXznvehbfNajJeU5UCr9GnbDTVQWe63SvNU2UZsa7WujdHeSsM+5u8M18gxemO2ietWlg9TGra5sm+uRYxXyHLUL2fx7JwLzxhjjDFmjvgByhhjjDFmJAt34R06dGha5nZ0tv3P7Ttu9WXbijzGcnXt7NXjyl3C7U1uh2Zt5jYs+yvb/uS53JZlKgC68LLUDNlWdPUK7DxgKhf2adx+5RZylcIme7WeVK4xjj1dE3FMqrQwLLNd0Q4r91c1RvFaY9PCVO7y6DrIXDOLcOHFeTzmftW5dC3EsapcL1UqHLpiMjkBy1XqlsxVwfWjSn8V21W5prI0OlKeAiVbixYhJ7h06ZKefvrpaTlKVKThGs6xZplrEfuBa3aELjmuRRyDTHrC353KTZrJIhjmp3KxZmtqFRaGfcD+5NhcuHBh+pl9G38/2VcR70AZY4wxxozED1DGGGOMMSMpH6Baa3e31v6mtfZYa+3R1trvT/5+pLX2cGvticm/h7e/uuZ6xXZktoptyMwD25GZF5vRQF2W9Ie99x+21g5K+r+ttYcl/QdJ3+m9f6219oCkByR9ObvQysrKwL86Ri9TaTEy7Qp1B/R30gdfvWYb/aOVT5f+YaY8iO3aqt4k6iHYH9ROVBoy+q7j8Sy9R6IPmZsd9d4H9eH4Rj1X5b/na+Ic+2PHjk0/s91HjhxJv0ttFr8fffaVdqhKhxHbQXumFoTfzVIX0Q5YT2rnxuhdsrAcG6ThmKsNZRqo7DVmjjNfn2ZKibjecJ7R/sakoJJyfRWpQnhkqW3YB7SLTN9TtYlrVRWuJdOgRpJjc7Ojt99+W0888cS0fMcddwyOxzWCY51pN6Vca8f5wXO57tE2OH5xjKo0aoTjF78/1p4zqt8o/q6zXkwJFMeD61YciywlULkD1Xs/23v/4eTz65Iek3SnpM9JenBy2oOSPl9dy+xebEdmq9iGzDywHZl5MUoD1Vr7gKTfkPR9SSd672elKwYp6fgG3/lia+2R1tojDGRldidbtSP+n4fZfXgtMvNgq3ZEj4LZXWz6Aaq1dqukv5D0B73316rzr9J7/3rv/b7e+318jdDsPuZhR3RHmt2F1yIzD+ZhR3TLmd3FpuJAtdb26Yqh/Vnv/S8nfz7XWjvVez/bWjsl6fwmrjPQE9HvGH2N9INX6Vcyvznvk/k7pVxfwjJ9zZW/ONs9qTRQVbyq2Ef0PTMuVOVPzrRaWTyjLFbHvOxobW1t4O/mPWM/ZGlnpPXtzvz59KFn2rhZ18p86ZUGinY2RjfCa1WpGKINcy6wnlUqhiwW0Zj4aleZlw1VxLHivKvi0WS6SF6L41bpYHjt2E/ZsVn34vm8d3Yt2lDWDh6r4iFxXmWavixdTaaxnZcdvf3223rsscem5Sx+28mTJwfHsv6+eu2NrsV5Va01XAMyHXCM1TgLjgf/hzaOD49dvHhxUOZ6kdW7StVSxUGjtuv222+ffua4ZXWKbOYtvCbpTyU91nv/k3DoIUn3Tz7fL+nb1bXM7sV2ZLaKbcjMA9uRmReb2YH6V5L+vaSftNZ+NPnbf5L0NUnfaq19QdJzkn5nW2pobhRsR2ar2IbMPLAdmblQPkD13v9O0kZ7WJ+ab3XMjYrtyGwV25CZB7YjMy8WmguP8Xvo/4y+VforM1+3lOtyqlhNrEcVcyfzrfMYfc/0VUd/caVVoXYr04VVOd3o1620GLEu1EPEscr8xfNidXV1oNHKND2sa6Uzy7RglR6FfVb596OtVDbK+ZAJ6alvqOyGOaNivfhd2jfbRD1VlpOLOrt4rNI8bpXWWrrexP5lXaiFy/ISSkM7YH9QN1dpoFiXeJzaoSrOHudNtMEqFlM1z7PcjLw2c6+NiY/EeZOtU9vBpUuXdPr06WmZ9h/bdvjwMC4n14dqbcrWNY51panMtIv8Lvufdsbxi3WjTqmKT5XdO9NMSvWacfTo0UE5zkXaaNRbvfTSSxte06lcjDHGGGNG4gcoY4wxxpiR+AHKGGOMMWYkC9VAra2tpfmaoi8186tK6/33WVwc+kqrOC6V9ih+v6pXRfQJV/nPqntFf3HVRvqi6eemNiOS+akXoYFaW1sbaJVef/31dcevUml22KdsdyzzXGqHKj1Qll+sslGWM41aFZus0lpkurxK48Qyz4/jRr1ZzKs3Ni/kWFZXVwd2k9k0x40aKEJ9WuzDag6zHpUGLdpNpRMl1CJluiceq2I1xXGv7sM5Rw0UbSr2EfV+VZvnDe3o3Llzg+PHj/8ymDntvRpbkq1rtCPOW+oNWZdos7TRKqYg16J47UpLNzbnZ4RjXcXKY/Dc+IzBtfvs2bPTz88999yGdfAOlDHGGGPMSPwAZYwxxhgzkoW68FZXVwfb9Nz6i1vjlSuIW4PcvovbjrwPy9UroNz+jFvM1Xer8ABxC7N69ZSuBJJtm1evzGZb8ITbqnGrtNqGngdra2uDMcm2fXmMrgceZx9nrw6zj+i6GfNKOq9N+69cWvF45UrktTIXX1Wvym6y8CKsR3THbPcr6JcvX9aFCxemZbrlsvWDr6Nz3GlT0VVDl9Mrr7wyKPO17yo9S3TdVGFQOFZ0QUf7rL7LecQ+imtk5cJj/9GFR+Ir+NWauN2srKwMXFoMDxBtjK/DV33K+RHX2bEuO6ZQoY1Gd9bY9GQZdKOxjZmLXxq2k9eizKeyBd6b5Ui816OPPrrhed6BMsYYY4wZiR+gjDHGGGNG4gcoY4wxxpiRLFQDdfny5cFrntQdnDhxYvqZPl5qB7JXwqWhVqMKRU94L74mH33ErAf91vTTHjlyZFCO+ojz588PjrEPqA2oUh5E6POuNA7Z66VRxyYNdRrb/fr51ftF7Qj9/bHMV4Xpc+d3M90BYf/feuutgzLHnmR2VKW/YDvieFGnlL0GPut4vDdfWeZ9K80T+yjWjTYY0ydstx298847euqpp6blQ4cODY5Hez927NjgGOcwdUrUwUTYH9QhEdov+yULN8J60aaydFlcAyutYAbrWKWVImxHpnGMfb+IVC779u1Lf7di3alvq+b0mLWI96XmKb6WL62fp/fcc8+G99nK2sQ2cG5UoR2ibfC+Y8I+SHnYH4Y4iHMj+131DpQxxhhjzEj8AGWMMcYYMxI/QBljjDHGjGThGqjom81SD9AXSl83Y4dk/m76MKt0FPTTshy/X8XuoH+YOpmMTEsx69pj4oTw2pVOJp5PX34cq0WkUnj33Xf14osvTstZmoJKO/fyyy8PypnejX3EsaQfvdILRTiW1KdQR0J9VZZeKNOJzLpXFgOpioXFMudH7H/WK8bJ2W47Wl1dHdgx9RSxT9j3HGf2Cesex5b9WcV5Itm1M/uSat1SnMdZCg2pTg+UtSNbt2Z9l2tZrNsidE4Z+/bt06lTp6blqOMjPMa6VzrU2E/sM55LzdOZM2cG5aNHj25Yz7GxtXh+rBvrxfWWcdCy+FTsL16bVLEPYz05p2+//fbpZ2ugjDHGGGPmiB+gjDHGGGNG4gcoY4wxxpiRLDwXXtQdULty9913Tz8zNgjjRVTxl7Jz6Tul75n6EuoOok+Ux7I4ILOOs5xRxXKKeh/et9KU0afOa8f+Z4yRSOWXngerq6sDPQG1BbGuVf9SI8V+i7ZAPznL1OXRJrOcX7S5KtcY9S7RxitdCK9FjU88zvtU2hiSxX2JecKkoW5ju+2IOcyynH2VpmzMHGZ/cu3JcjFW16vixJFMZ1ZpJCt7jO1iPSr9KvuTa1esd7Z2LyJPXu99MM9pt7GumZZLWt+nbHf8vax+Z/jbyjWSmr9nn312+pnrAfuY48VrxXnN+/K3g/kBM91SZYNRtyStjzOZxRvL8odm3/MOlDHGGGPMSPwAZYwxxhgzEj9AGWOMMcaMZOFxoGLcB/pxY548+mGrXExZ/i/6TqscUZUGKp5faZr4XfrIM60Hfcvsg0z3xWP0iRPGNGK9o1aIfuvY5jF6kK2QaQtieWxcHWrtoj//hRdeGByjloNQXxXjxUhDHz5ttMoPRrI8WZV9sx1Rf0W7qTR91M7weOzD06dPD44988wzG15nO4h9luXWZB9Uc57lqMWo8nBWceNYz2hDleanWpsitBFqnKhrYr1jmdfid7mucU3kPIq5OLN4d4uISbe2tjaoX6aPy+Yoz5XWr0XZb2eWu1SqcypGDVSmB5Tq2G+xbqwH51KWV5WwXpUm9Y477hiU2f+xT7J4gdm88g6UMcYYY8xI/ABljDHGGDMSP0AZY4wxxoxkoRqo3vvAV8ucapkftvKjz7rXRt/ltSvfdHYvajV4beqYsjxkd9555+AY9TtVbI/o06X/nH1dxdRgPaOPmPWI5UyXNU+iRqjKy5WR5fyThn0cNXpSHSOJ16YdRm0Mz6XGgXbE86OfvtIp8VokHq80ZNRe0DaY+yrm5KIGapFxoFprqbYh9iHjVXFusY+y61LbSTgPDx48OChn2iPWg3nGqjhEkWq9ZZltjuWx6y3tlzqZOB5ZnsdFxaSL/UytUYR9xvHi2pPF6svys836bqUre/LJJzc8l/U+fPjwoHzs2LFBOer8qvx1lVYr2kJlg0eOHBmUjx8/Piizz/ibGIn1dhwoY4wxxpg5Uj5AtdYOtNb+vrX249bao621P578/Uhr7eHW2hOTfw9X1zK7E9uQmQe2IzMPbEdmXmzGhfeOpH/be3+jtbZP0t+11v63pH8n6Tu996+11h6Q9ICkL1cXi1tj3Jp9+umnp5+5PUf3Fl/vzdIB0NXCV4lZpnskc+FV4fjpLuE2egw/z61RQpcHXQnx2tyeZAh9bqPzVVUej1u+r7/++uBYdNNs8BrqXG1oZWVl4GKo0l2MgWMf28owBtyOZr+wXnzNNo591YZqvCJsA22QcyWrN22sShFEO4uhCSTpZz/72fQz7Tm6ZhZhR5kbOHs1nf3FseAaEO2E9+G5XIuy8AC8Hq9NNxzJ0siMcdHNuncs0yboCmf/0uZYjmT9k7gK52pHEa7h0QXLseY85XrCNSG6+Oju428pr80+zFyN/I2qwp7QbR/n8eOPPz449uKLL6bfZbviOsDftKpNHP9Dhw4NytHGs1BH2Vpb7kD1K1xt5b7Jf13S5yQ9OPn7g5I+X13L7E5sQ2Ye2I7MPLAdmXmxKQ1Ua21Pa+1Hks5Lerj3/n1JJ3rvZyVp8u/xDb77xdbaI621R8YmIjU3Dluxocn3p3a0iAB5ZjmZ11qU7WqYGx//ppl5sKkHqN77au/91yXdJekTrbVf2+wNeu9f773f13u/r3pzzty4bMWGJt+f2lG2pWpubOa1FlVR5M2NjX/TzDwY9UvUe3+1tfa3kj4r6Vxr7VTv/Wxr7ZSuPMlX3x/4xulrjf7Q6pVw7kJkr3nTP09/J/35vBbPj9eutAGV/iRem4s6X5+kD53HY73oT6dvOfOvz7pX7G9eO47jJl6R35INXSU+RI1Ne1LUb1CObeOuBfuQdkJdQvbKNfu/6sfsIXKMzUnr52E8v0qpUr2WzFfpYznTNFThE7ZqR621QT/wfrEd1FZUukfaRaZTqlKkjNUeRWgjlSYqXrt6wKxSVGWpcKp5wWtnbcxe59/MujCP9SjehzrH+Co968N+YT9wvKId8lpcm6qwKDw/S4HD8AAnT54clDk+URNLDSRDglSpn2I7svA50vrQDIQhROJcY3/FsUjTZKV3lNRaO9Zau23y+WZJvyXpnyQ9JOn+yWn3S/p2dS2zO7ENmXlgOzLzwHZk5sVmdqBOSXqwtbZHVx64vtV7/1+tte9J+lZr7QuSnpP0O9tYT3N9Yxsy88B2ZOaB7cjMhfIBqvf+D5J+Y8bfL0r61HZUytxY2IbMPLAdmXlgOzLzolVai7nerLWXJD0r6aikC8XpO4HrNY5Z9Xp/7/3YrJPnxcSOfjHj3svA9TRWy8BG9dpWO/JadM1cT/Va1Fq0rHa0jHWSrr96bWhHC32Amt60tUd67/ct/MYFrtc4drJe7pNxuF7Lef+NcL3GsdP12un7z2IZ6yTdWPVyLjxjjDHGmJH4AcoYY4wxZiQ79QD19R26b4XrNY6drJf7ZByu13LefyNcr3HsdL12+v6zWMY6STdQvXZEA2WMMcYYcz1jF54xxhhjzEj8AGWMMcYYM5KFPkC11j7bWnu8tfZka+2BRd57Rl2+0Vo731r7afjbkdbaw621Jyb/Hl5wne5urf1Na+2x1tqjrbXfX5J6HWit/X1r7ceTev3xTtZrWexoGW1oUoelsyPbUFqXpbOjZbShyf1tR7PrsXQ2NKnDjW1HvfeF/Cdpj6SfSfqgpP2SfizpVxd1/xn1+TeSflPST8Pf/pukByafH5D0Xxdcp1OSfnPy+aCkf5b0q0tQrybp1snnfZK+L+lf7ES9lsmOltGGltWObEPXlx0tow3Zjq4vG9oNdrTICv9LSX8dyn8k6Y92wthCHT4Ag3tc0qkw8I/vcP2+LenTy1QvSe+R9ENJn9yJei2bHS27DS2jHdmGrj87WjYbsh1dfzZ0I9rRIl14d0o6HcpnJn9bJk703s9K0uTf4ztVkdbaB3QlX9P3l6FerbU9rbUfSTov6eHe+07Va9ntaMfHKrJMdmQbGsXS2NEy2dCkPrajzbHjYxW5Ee1okQ9QbcbfHENhBq21WyX9haQ/6L2/ttP1kaTe+2rv/dcl3SXpE621X9uhqtiONsmy2ZFt6Ppj2WxIsh1dj9yodrTIB6gzku4O5bskvbDA+2+Gc621U5I0+ff8oivQWtunK4b2Z733v1yWel2l9/6qpL+V9Nkdqtey29FSjNUy25FtaFPs+Fgtsw1JtqNNsBRjdSPb0SIfoH4g6SOttXtaa/sl/a6khxZ4/83wkKT7J5/v1xV/7cJorTVJfyrpsd77nyxRvY611m6bfL5Z0m9J+qcdqtey29GOjpW0nHZkGxrNTs/5pbOhSb1sR5vHa9HG9ZqPHS1YrPXbuqLC/5mk/7xTorFJXf5c0llJ7+rK/0l8QdLtkr4j6YnJv0cWXKd/rStbwP8g6UeT/357Cer1cUn/b1Kvn0r6L5O/70i9lsWOltGGltWObEPXlx0tow3Zjq4vG9oNduRULsYYY4wxI3EkcmOMMcaYkfgByhhjjDFmJH6AMsYYY4wZiR+gjDHGGGNG4gcoY4wxxpiR+AHKGGOMMWYkfoAyxhhjjBnJ/wealazIBgVGeQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k=5\n",
    "\n",
    "abnormal = np.load('../../data/npy/abnormal_16to32.npy')\n",
    "normal = np.load('../../data/npy/normal_16to32.npy')\n",
    "data_x = np.concatenate((normal, abnormal))\n",
    "data_y = np.ndarray((len(data_x)),dtype=np.float32)\n",
    "for n in range(len(data_x)):\n",
    "    if n < len(normal):\n",
    "        data_y[n] = 0\n",
    "    else:\n",
    "        data_y[n] = 1\n",
    "print(data_x.shape, data_y.shape)        \n",
    "# train_x,train_x,test_y,test_y = train_test_split(data_x,data_y, stratify = data_y, train_size=1300, random_state = 25)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=k)\n",
    "skf.get_n_splits(data_x,data_y)\n",
    "print(skf)\n",
    "i=0\n",
    "for train_index, test_index in skf.split(data_x,data_y):\n",
    "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "    indexs = np.concatenate((train_index, test_index), axis= 0)\n",
    "    train_index = indexs[:1300]\n",
    "    test_index = indexs[1300:]\n",
    "    print(\"RETRAIN:\", len(train_index), \"RETEST:\", len(test_index))\n",
    "    train_x, test_x = data_x[train_index], data_x[test_index]\n",
    "    train_y, test_y = data_y[train_index], data_y[test_index]\n",
    "    print(train_x.shape, test_x.shape)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    print(train_index[0],test_index[0],train_index[len(train_index)-1],test_index[len(test_index)-1])\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.imshow(train_x[0], cmap='gray')\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(test_x[0], cmap='gray')\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(train_x[len(train_x)-1], cmap='gray')\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.imshow(test_x[len(test_x)-1], cmap='gray')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        640       \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_2 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 20,154,817\n",
      "Trainable params: 20,154,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape=(32,32,1))\n",
    "model = vgg_app_19(input_img)\n",
    "model.summary()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 1299 TEST: 325\n",
      "WARNING:tensorflow:From /home/dgxadmin/anaconda3/envs/zzaem2/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/dgxadmin/anaconda3/envs/zzaem2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 1040 samples, validate on 259 samples\n",
      "Epoch 1/500\n",
      "1040/1040 [==============================] - 4s 4ms/step - loss: 0.6833 - accuracy: 0.5173 - val_loss: 0.6155 - val_accuracy: 0.5367\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61550, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold0_0518.h5\n",
      "Epoch 2/500\n",
      "1040/1040 [==============================] - 0s 372us/step - loss: 0.5770 - accuracy: 0.7308 - val_loss: 0.4677 - val_accuracy: 0.7529\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61550 to 0.46768, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold0_0518.h5\n",
      "Epoch 3/500\n",
      "1040/1040 [==============================] - 0s 362us/step - loss: 0.4088 - accuracy: 0.8981 - val_loss: 0.3046 - val_accuracy: 0.8649\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.46768 to 0.30463, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold0_0518.h5\n",
      "Epoch 4/500\n",
      "1040/1040 [==============================] - 0s 370us/step - loss: 0.2140 - accuracy: 0.9163 - val_loss: 0.3521 - val_accuracy: 0.8610\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.30463\n",
      "Epoch 5/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.2052 - accuracy: 0.9173 - val_loss: 0.4934 - val_accuracy: 0.8069\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.30463\n",
      "Epoch 6/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.3262 - accuracy: 0.8856 - val_loss: 0.2790 - val_accuracy: 0.8610\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.30463 to 0.27901, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold0_0518.h5\n",
      "Epoch 7/500\n",
      "1040/1040 [==============================] - 0s 362us/step - loss: 0.2243 - accuracy: 0.9077 - val_loss: 0.2955 - val_accuracy: 0.8610\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.27901\n",
      "Epoch 8/500\n",
      "1040/1040 [==============================] - 0s 362us/step - loss: 0.1859 - accuracy: 0.9212 - val_loss: 0.2282 - val_accuracy: 0.8803\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.27901 to 0.22815, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold0_0518.h5\n",
      "Epoch 9/500\n",
      "1040/1040 [==============================] - 0s 366us/step - loss: 0.1804 - accuracy: 0.9308 - val_loss: 0.2853 - val_accuracy: 0.8610\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.22815\n",
      "Epoch 10/500\n",
      "1040/1040 [==============================] - 0s 369us/step - loss: 0.2654 - accuracy: 0.8913 - val_loss: 0.2025 - val_accuracy: 0.9459\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.22815 to 0.20253, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold0_0518.h5\n",
      "Epoch 11/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.1720 - accuracy: 0.9337 - val_loss: 0.1772 - val_accuracy: 0.9151\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.20253 to 0.17723, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold0_0518.h5\n",
      "Epoch 12/500\n",
      "1040/1040 [==============================] - 0s 361us/step - loss: 0.1916 - accuracy: 0.9154 - val_loss: 0.1509 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.17723 to 0.15087, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold0_0518.h5\n",
      "Epoch 13/500\n",
      "1040/1040 [==============================] - 0s 362us/step - loss: 0.1497 - accuracy: 0.9413 - val_loss: 0.1353 - val_accuracy: 0.9421\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.15087 to 0.13527, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold0_0518.h5\n",
      "Epoch 14/500\n",
      "1040/1040 [==============================] - 0s 364us/step - loss: 0.1461 - accuracy: 0.9442 - val_loss: 0.1344 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.13527 to 0.13443, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold0_0518.h5\n",
      "Epoch 15/500\n",
      "1040/1040 [==============================] - 0s 365us/step - loss: 0.1309 - accuracy: 0.9462 - val_loss: 0.1386 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.13443\n",
      "Epoch 16/500\n",
      "1040/1040 [==============================] - 0s 359us/step - loss: 0.1299 - accuracy: 0.9500 - val_loss: 0.1374 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.13443\n",
      "Epoch 17/500\n",
      "1040/1040 [==============================] - 0s 357us/step - loss: 0.1307 - accuracy: 0.9500 - val_loss: 0.1560 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.13443\n",
      "Epoch 18/500\n",
      "1040/1040 [==============================] - 0s 362us/step - loss: 0.1243 - accuracy: 0.9442 - val_loss: 0.1415 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.13443\n",
      "Epoch 19/500\n",
      "1040/1040 [==============================] - 0s 359us/step - loss: 0.1089 - accuracy: 0.9596 - val_loss: 0.1370 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.13443\n",
      "Epoch 20/500\n",
      "1040/1040 [==============================] - 0s 358us/step - loss: 0.0929 - accuracy: 0.9615 - val_loss: 0.1423 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.13443\n",
      "Epoch 21/500\n",
      "1040/1040 [==============================] - 0s 358us/step - loss: 0.0907 - accuracy: 0.9644 - val_loss: 0.1432 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.13443\n",
      "Epoch 22/500\n",
      "1040/1040 [==============================] - 0s 357us/step - loss: 0.0814 - accuracy: 0.9702 - val_loss: 0.1272 - val_accuracy: 0.9498\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.13443 to 0.12717, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold0_0518.h5\n",
      "Epoch 23/500\n",
      "1040/1040 [==============================] - 0s 362us/step - loss: 0.0806 - accuracy: 0.9712 - val_loss: 0.1364 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.12717\n",
      "Epoch 24/500\n",
      "1040/1040 [==============================] - 0s 366us/step - loss: 0.0676 - accuracy: 0.9740 - val_loss: 0.1385 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.12717\n",
      "Epoch 25/500\n",
      "1040/1040 [==============================] - 0s 357us/step - loss: 0.0600 - accuracy: 0.9760 - val_loss: 0.1370 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.12717\n",
      "Epoch 26/500\n",
      "1040/1040 [==============================] - 0s 364us/step - loss: 0.0650 - accuracy: 0.9750 - val_loss: 0.2036 - val_accuracy: 0.9112\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.12717\n",
      "Epoch 27/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0784 - accuracy: 0.9683 - val_loss: 0.1423 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.12717\n",
      "Epoch 28/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.0626 - accuracy: 0.9750 - val_loss: 0.1740 - val_accuracy: 0.9189\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.12717\n",
      "Epoch 29/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.0586 - accuracy: 0.9779 - val_loss: 0.1385 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.12717\n",
      "Epoch 30/500\n",
      "1040/1040 [==============================] - 0s 358us/step - loss: 0.0467 - accuracy: 0.9837 - val_loss: 0.1999 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.12717\n",
      "Epoch 31/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.0590 - accuracy: 0.9788 - val_loss: 0.2069 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.12717\n",
      "Epoch 32/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.0779 - accuracy: 0.9673 - val_loss: 0.1428 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.12717\n",
      "Epoch 33/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.0580 - accuracy: 0.9798 - val_loss: 0.1188 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.12717 to 0.11880, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold0_0518.h5\n",
      "Epoch 34/500\n",
      "1040/1040 [==============================] - 0s 366us/step - loss: 0.0434 - accuracy: 0.9837 - val_loss: 0.1196 - val_accuracy: 0.9498\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.11880\n",
      "Epoch 35/500\n",
      "1040/1040 [==============================] - 0s 359us/step - loss: 0.0338 - accuracy: 0.9856 - val_loss: 0.1231 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.11880\n",
      "Epoch 36/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0286 - accuracy: 0.9885 - val_loss: 0.1316 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.11880\n",
      "Epoch 37/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.0242 - accuracy: 0.9933 - val_loss: 0.1399 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.11880\n",
      "Epoch 38/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.0209 - accuracy: 0.9942 - val_loss: 0.1448 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.11880\n",
      "Epoch 39/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0185 - accuracy: 0.9952 - val_loss: 0.1519 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.11880\n",
      "Epoch 40/500\n",
      "1040/1040 [==============================] - 0s 354us/step - loss: 0.0163 - accuracy: 0.9971 - val_loss: 0.1583 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.11880\n",
      "Epoch 41/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0146 - accuracy: 0.9971 - val_loss: 0.1635 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.11880\n",
      "Epoch 42/500\n",
      "1040/1040 [==============================] - 0s 357us/step - loss: 0.0131 - accuracy: 0.9981 - val_loss: 0.1691 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.11880\n",
      "Epoch 43/500\n",
      "1040/1040 [==============================] - 0s 357us/step - loss: 0.0119 - accuracy: 0.9981 - val_loss: 0.1736 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.11880\n",
      "Epoch 44/500\n",
      "1040/1040 [==============================] - 0s 359us/step - loss: 0.0110 - accuracy: 0.9981 - val_loss: 0.1740 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.11880\n",
      "Epoch 45/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0109 - accuracy: 0.9981 - val_loss: 0.1744 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.11880\n",
      "Epoch 46/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.0108 - accuracy: 0.9981 - val_loss: 0.1749 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.11880\n",
      "Epoch 47/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0107 - accuracy: 0.9981 - val_loss: 0.1753 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.11880\n",
      "Epoch 48/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.0106 - accuracy: 0.9981 - val_loss: 0.1758 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.11880\n",
      "Epoch 49/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.0106 - accuracy: 0.9981 - val_loss: 0.1762 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.11880\n",
      "Epoch 50/500\n",
      "1040/1040 [==============================] - 0s 357us/step - loss: 0.0105 - accuracy: 0.9981 - val_loss: 0.1767 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.11880\n",
      "Epoch 51/500\n",
      "1040/1040 [==============================] - 0s 354us/step - loss: 0.0104 - accuracy: 0.9981 - val_loss: 0.1771 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.11880\n",
      "Epoch 52/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.0103 - accuracy: 0.9981 - val_loss: 0.1775 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.11880\n",
      "Epoch 53/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0102 - accuracy: 0.9981 - val_loss: 0.1780 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.11880\n",
      "Epoch 54/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.0101 - accuracy: 0.9981 - val_loss: 0.1780 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.11880\n",
      "Epoch 55/500\n",
      "1040/1040 [==============================] - 0s 354us/step - loss: 0.0101 - accuracy: 0.9981 - val_loss: 0.1781 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.11880\n",
      "Epoch 56/500\n",
      "1040/1040 [==============================] - 0s 359us/step - loss: 0.0101 - accuracy: 0.9981 - val_loss: 0.1781 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.11880\n",
      "Epoch 57/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.0101 - accuracy: 0.9981 - val_loss: 0.1782 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.11880\n",
      "Epoch 58/500\n",
      "1040/1040 [==============================] - 0s 354us/step - loss: 0.0101 - accuracy: 0.9981 - val_loss: 0.1782 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.11880\n",
      "Epoch 59/500\n",
      "1040/1040 [==============================] - 0s 358us/step - loss: 0.0101 - accuracy: 0.9981 - val_loss: 0.1783 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.11880\n",
      "Epoch 60/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.0101 - accuracy: 0.9981 - val_loss: 0.1783 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.11880\n",
      "Epoch 61/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0101 - accuracy: 0.9981 - val_loss: 0.1784 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.11880\n",
      "Epoch 62/500\n",
      "1040/1040 [==============================] - 0s 354us/step - loss: 0.0100 - accuracy: 0.9981 - val_loss: 0.1784 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.11880\n",
      "Epoch 63/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.0100 - accuracy: 0.9981 - val_loss: 0.1785 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.11880\n",
      "Epoch 00063: early stopping\n",
      "TRAIN: 1299 TEST: 325\n",
      "Train on 1040 samples, validate on 259 samples\n",
      "Epoch 1/500\n",
      "1040/1040 [==============================] - 1s 1ms/step - loss: 0.6755 - accuracy: 0.5106 - val_loss: 0.6133 - val_accuracy: 0.8687\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61330, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold1_0518.h5\n",
      "Epoch 2/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.5086 - accuracy: 0.7875 - val_loss: 0.2526 - val_accuracy: 0.9189\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61330 to 0.25259, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold1_0518.h5\n",
      "Epoch 3/500\n",
      "1040/1040 [==============================] - 0s 359us/step - loss: 0.2979 - accuracy: 0.9000 - val_loss: 0.6164 - val_accuracy: 0.7375\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.25259\n",
      "Epoch 4/500\n",
      "1040/1040 [==============================] - 0s 367us/step - loss: 0.3773 - accuracy: 0.8356 - val_loss: 0.2232 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.25259 to 0.22319, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold1_0518.h5\n",
      "Epoch 5/500\n",
      "1040/1040 [==============================] - 0s 362us/step - loss: 0.2148 - accuracy: 0.9183 - val_loss: 0.4716 - val_accuracy: 0.8417\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.22319\n",
      "Epoch 6/500\n",
      "1040/1040 [==============================] - 0s 358us/step - loss: 0.2772 - accuracy: 0.8865 - val_loss: 0.2830 - val_accuracy: 0.8919\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.22319\n",
      "Epoch 7/500\n",
      "1040/1040 [==============================] - 0s 359us/step - loss: 0.1991 - accuracy: 0.9192 - val_loss: 0.3507 - val_accuracy: 0.8726\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.22319\n",
      "Epoch 8/500\n",
      "1040/1040 [==============================] - 0s 357us/step - loss: 0.2049 - accuracy: 0.9106 - val_loss: 0.2845 - val_accuracy: 0.8764\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.22319\n",
      "Epoch 9/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.1813 - accuracy: 0.9269 - val_loss: 0.2321 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.22319\n",
      "Epoch 10/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.1531 - accuracy: 0.9375 - val_loss: 0.2275 - val_accuracy: 0.9151\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.22319\n",
      "Epoch 11/500\n",
      "1040/1040 [==============================] - 0s 358us/step - loss: 0.1364 - accuracy: 0.9433 - val_loss: 0.2556 - val_accuracy: 0.9151\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.22319\n",
      "Epoch 12/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.1317 - accuracy: 0.9442 - val_loss: 0.2577 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.22319\n",
      "Epoch 13/500\n",
      "1040/1040 [==============================] - 0s 359us/step - loss: 0.1225 - accuracy: 0.9481 - val_loss: 0.2306 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.22319\n",
      "Epoch 14/500\n",
      "1040/1040 [==============================] - 0s 358us/step - loss: 0.1115 - accuracy: 0.9519 - val_loss: 0.1885 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.22319 to 0.18845, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold1_0518.h5\n",
      "Epoch 15/500\n",
      "1040/1040 [==============================] - 0s 363us/step - loss: 0.0979 - accuracy: 0.9558 - val_loss: 0.3251 - val_accuracy: 0.8842\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.18845\n",
      "Epoch 16/500\n",
      "1040/1040 [==============================] - 0s 358us/step - loss: 0.1055 - accuracy: 0.9471 - val_loss: 0.1950 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.18845\n",
      "Epoch 17/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0818 - accuracy: 0.9615 - val_loss: 0.3604 - val_accuracy: 0.8726\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.18845\n",
      "Epoch 18/500\n",
      "1040/1040 [==============================] - 0s 358us/step - loss: 0.1008 - accuracy: 0.9615 - val_loss: 0.2121 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.18845\n",
      "Epoch 19/500\n",
      "1040/1040 [==============================] - 0s 354us/step - loss: 0.0738 - accuracy: 0.9683 - val_loss: 0.4198 - val_accuracy: 0.8842\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.18845\n",
      "Epoch 20/500\n",
      "1040/1040 [==============================] - 0s 354us/step - loss: 0.0900 - accuracy: 0.9615 - val_loss: 0.1823 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.18845 to 0.18229, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold1_0518.h5\n",
      "Epoch 21/500\n",
      "1040/1040 [==============================] - 0s 358us/step - loss: 0.0615 - accuracy: 0.9712 - val_loss: 0.3648 - val_accuracy: 0.8996\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.18229\n",
      "Epoch 22/500\n",
      "1040/1040 [==============================] - 0s 354us/step - loss: 0.0594 - accuracy: 0.9731 - val_loss: 0.2827 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.18229\n",
      "Epoch 23/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0431 - accuracy: 0.9837 - val_loss: 0.7291 - val_accuracy: 0.8301\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.18229\n",
      "Epoch 24/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.2037 - accuracy: 0.9173 - val_loss: 0.5640 - val_accuracy: 0.8069\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.18229\n",
      "Epoch 25/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.3725 - accuracy: 0.8692 - val_loss: 0.2136 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.18229\n",
      "Epoch 26/500\n",
      "1040/1040 [==============================] - 0s 352us/step - loss: 0.1663 - accuracy: 0.9423 - val_loss: 0.4205 - val_accuracy: 0.8880\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.18229\n",
      "Epoch 27/500\n",
      "1040/1040 [==============================] - 0s 350us/step - loss: 0.2000 - accuracy: 0.9260 - val_loss: 0.1345 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.18229 to 0.13447, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold1_0518.h5\n",
      "Epoch 28/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.1158 - accuracy: 0.9548 - val_loss: 0.2437 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.13447\n",
      "Epoch 29/500\n",
      "1040/1040 [==============================] - 0s 354us/step - loss: 0.1046 - accuracy: 0.9558 - val_loss: 0.2900 - val_accuracy: 0.9151\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.13447\n",
      "Epoch 30/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.1161 - accuracy: 0.9500 - val_loss: 0.2097 - val_accuracy: 0.9151\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.13447\n",
      "Epoch 31/500\n",
      "1040/1040 [==============================] - 0s 358us/step - loss: 0.0900 - accuracy: 0.9654 - val_loss: 0.2220 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.13447\n",
      "Epoch 32/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0869 - accuracy: 0.9615 - val_loss: 0.5162 - val_accuracy: 0.8649\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.13447\n",
      "Epoch 33/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.0746 - accuracy: 0.9635 - val_loss: 0.2367 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.13447\n",
      "Epoch 34/500\n",
      "1040/1040 [==============================] - 0s 362us/step - loss: 0.0670 - accuracy: 0.9702 - val_loss: 0.4549 - val_accuracy: 0.8880\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.13447\n",
      "Epoch 35/500\n",
      "1040/1040 [==============================] - 0s 359us/step - loss: 0.0787 - accuracy: 0.9615 - val_loss: 0.3005 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.13447\n",
      "Epoch 36/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.0994 - accuracy: 0.9577 - val_loss: 0.1619 - val_accuracy: 0.9421\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.13447\n",
      "Epoch 37/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.0896 - accuracy: 0.9702 - val_loss: 0.1744 - val_accuracy: 0.9498\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.13447\n",
      "Epoch 38/500\n",
      "1040/1040 [==============================] - 0s 358us/step - loss: 0.0515 - accuracy: 0.9817 - val_loss: 0.1347 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.13447\n",
      "Epoch 39/500\n",
      "1040/1040 [==============================] - 0s 359us/step - loss: 0.0385 - accuracy: 0.9808 - val_loss: 0.1177 - val_accuracy: 0.9691\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.13447 to 0.11774, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold1_0518.h5\n",
      "Epoch 40/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0362 - accuracy: 0.9846 - val_loss: 0.1330 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.11774\n",
      "Epoch 41/500\n",
      "1040/1040 [==============================] - 0s 358us/step - loss: 0.0311 - accuracy: 0.9875 - val_loss: 0.1433 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.11774\n",
      "Epoch 42/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.0264 - accuracy: 0.9894 - val_loss: 0.1396 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.11774\n",
      "Epoch 43/500\n",
      "1040/1040 [==============================] - 0s 354us/step - loss: 0.0238 - accuracy: 0.9913 - val_loss: 0.1468 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.11774\n",
      "Epoch 44/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0215 - accuracy: 0.9923 - val_loss: 0.1532 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.11774\n",
      "Epoch 45/500\n",
      "1040/1040 [==============================] - 0s 354us/step - loss: 0.0191 - accuracy: 0.9952 - val_loss: 0.1569 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.11774\n",
      "Epoch 46/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0172 - accuracy: 0.9962 - val_loss: 0.1624 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.11774\n",
      "Epoch 47/500\n",
      "1040/1040 [==============================] - 0s 354us/step - loss: 0.0153 - accuracy: 0.9962 - val_loss: 0.1659 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.11774\n",
      "Epoch 48/500\n",
      "1040/1040 [==============================] - 0s 357us/step - loss: 0.0137 - accuracy: 0.9962 - val_loss: 0.1692 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.11774\n",
      "Epoch 49/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0124 - accuracy: 0.9962 - val_loss: 0.1730 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.11774\n",
      "Epoch 50/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0113 - accuracy: 0.9962 - val_loss: 0.1738 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.11774\n",
      "Epoch 51/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.0112 - accuracy: 0.9962 - val_loss: 0.1748 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.11774\n",
      "Epoch 52/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0111 - accuracy: 0.9962 - val_loss: 0.1759 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.11774\n",
      "Epoch 53/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0110 - accuracy: 0.9962 - val_loss: 0.1767 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.11774\n",
      "Epoch 54/500\n",
      "1040/1040 [==============================] - 0s 357us/step - loss: 0.0109 - accuracy: 0.9962 - val_loss: 0.1774 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.11774\n",
      "Epoch 55/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0107 - accuracy: 0.9962 - val_loss: 0.1779 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.11774\n",
      "Epoch 56/500\n",
      "1040/1040 [==============================] - 0s 358us/step - loss: 0.0106 - accuracy: 0.9962 - val_loss: 0.1784 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.11774\n",
      "Epoch 57/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.0105 - accuracy: 0.9962 - val_loss: 0.1788 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.11774\n",
      "Epoch 58/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.0104 - accuracy: 0.9962 - val_loss: 0.1791 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.11774\n",
      "Epoch 59/500\n",
      "1040/1040 [==============================] - 0s 352us/step - loss: 0.0103 - accuracy: 0.9962 - val_loss: 0.1795 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.11774\n",
      "Epoch 60/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.0102 - accuracy: 0.9962 - val_loss: 0.1795 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.11774\n",
      "Epoch 61/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.0102 - accuracy: 0.9962 - val_loss: 0.1796 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.11774\n",
      "Epoch 62/500\n",
      "1040/1040 [==============================] - 0s 357us/step - loss: 0.0102 - accuracy: 0.9962 - val_loss: 0.1796 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.11774\n",
      "Epoch 63/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0102 - accuracy: 0.9962 - val_loss: 0.1797 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.11774\n",
      "Epoch 64/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0102 - accuracy: 0.9962 - val_loss: 0.1798 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.11774\n",
      "Epoch 65/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.0102 - accuracy: 0.9962 - val_loss: 0.1798 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.11774\n",
      "Epoch 66/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0102 - accuracy: 0.9962 - val_loss: 0.1799 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.11774\n",
      "Epoch 67/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0101 - accuracy: 0.9962 - val_loss: 0.1799 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.11774\n",
      "Epoch 68/500\n",
      "1040/1040 [==============================] - 0s 357us/step - loss: 0.0101 - accuracy: 0.9962 - val_loss: 0.1800 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.11774\n",
      "Epoch 69/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.0101 - accuracy: 0.9962 - val_loss: 0.1800 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.11774\n",
      "Epoch 00069: early stopping\n",
      "TRAIN: 1299 TEST: 325\n",
      "Train on 1040 samples, validate on 259 samples\n",
      "Epoch 1/500\n",
      "1040/1040 [==============================] - 1s 1ms/step - loss: 0.6748 - accuracy: 0.5587 - val_loss: 0.7337 - val_accuracy: 0.4981\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.73374, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold2_0518.h5\n",
      "Epoch 2/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.5997 - accuracy: 0.6721 - val_loss: 0.3590 - val_accuracy: 0.8996\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.73374 to 0.35904, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold2_0518.h5\n",
      "Epoch 3/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.3156 - accuracy: 0.8923 - val_loss: 0.2572 - val_accuracy: 0.8958\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35904 to 0.25724, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold2_0518.h5\n",
      "Epoch 4/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.1866 - accuracy: 0.9240 - val_loss: 0.3263 - val_accuracy: 0.8919\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.25724\n",
      "Epoch 5/500\n",
      "1040/1040 [==============================] - 0s 350us/step - loss: 0.1787 - accuracy: 0.9202 - val_loss: 0.3313 - val_accuracy: 0.8726\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.25724\n",
      "Epoch 6/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.1625 - accuracy: 0.9260 - val_loss: 0.2219 - val_accuracy: 0.8958\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.25724 to 0.22189, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold2_0518.h5\n",
      "Epoch 7/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.1151 - accuracy: 0.9587 - val_loss: 0.2154 - val_accuracy: 0.9035\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.22189 to 0.21543, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold2_0518.h5\n",
      "Epoch 8/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.0904 - accuracy: 0.9615 - val_loss: 0.2380 - val_accuracy: 0.9151\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.21543\n",
      "Epoch 9/500\n",
      "1040/1040 [==============================] - 0s 346us/step - loss: 0.0809 - accuracy: 0.9673 - val_loss: 0.2071 - val_accuracy: 0.9112\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.21543 to 0.20709, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold2_0518.h5\n",
      "Epoch 10/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.0652 - accuracy: 0.9702 - val_loss: 0.2496 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.20709\n",
      "Epoch 11/500\n",
      "1040/1040 [==============================] - 0s 346us/step - loss: 0.1022 - accuracy: 0.9644 - val_loss: 0.5113 - val_accuracy: 0.8533\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.20709\n",
      "Epoch 12/500\n",
      "1040/1040 [==============================] - 0s 346us/step - loss: 0.2316 - accuracy: 0.9038 - val_loss: 0.1992 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.20709 to 0.19924, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold2_0518.h5\n",
      "Epoch 13/500\n",
      "1040/1040 [==============================] - 0s 352us/step - loss: 0.1534 - accuracy: 0.9404 - val_loss: 0.2511 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.19924\n",
      "Epoch 14/500\n",
      "1040/1040 [==============================] - 0s 346us/step - loss: 0.1787 - accuracy: 0.9183 - val_loss: 0.2475 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.19924\n",
      "Epoch 15/500\n",
      "1040/1040 [==============================] - 0s 347us/step - loss: 0.1339 - accuracy: 0.9500 - val_loss: 0.2166 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.19924\n",
      "Epoch 16/500\n",
      "1040/1040 [==============================] - 0s 348us/step - loss: 0.1296 - accuracy: 0.9558 - val_loss: 0.2863 - val_accuracy: 0.8880\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.19924\n",
      "Epoch 17/500\n",
      "1040/1040 [==============================] - 0s 345us/step - loss: 0.1043 - accuracy: 0.9558 - val_loss: 0.2403 - val_accuracy: 0.9112\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.19924\n",
      "Epoch 18/500\n",
      "1040/1040 [==============================] - 0s 346us/step - loss: 0.1031 - accuracy: 0.9567 - val_loss: 0.1847 - val_accuracy: 0.9189\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.19924 to 0.18471, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold2_0518.h5\n",
      "Epoch 19/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.0609 - accuracy: 0.9760 - val_loss: 0.2076 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.18471\n",
      "Epoch 20/500\n",
      "1040/1040 [==============================] - 0s 347us/step - loss: 0.0624 - accuracy: 0.9731 - val_loss: 0.2367 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.18471\n",
      "Epoch 21/500\n",
      "1040/1040 [==============================] - 0s 345us/step - loss: 0.0610 - accuracy: 0.9760 - val_loss: 0.2355 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.18471\n",
      "Epoch 22/500\n",
      "1040/1040 [==============================] - 0s 344us/step - loss: 0.0581 - accuracy: 0.9750 - val_loss: 0.3219 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.18471\n",
      "Epoch 23/500\n",
      "1040/1040 [==============================] - 0s 348us/step - loss: 0.0978 - accuracy: 0.9606 - val_loss: 0.6482 - val_accuracy: 0.7876\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.18471\n",
      "Epoch 24/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.3064 - accuracy: 0.8702 - val_loss: 0.2530 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.18471\n",
      "Epoch 25/500\n",
      "1040/1040 [==============================] - 0s 345us/step - loss: 0.1666 - accuracy: 0.9385 - val_loss: 0.2431 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.18471\n",
      "Epoch 26/500\n",
      "1040/1040 [==============================] - 0s 345us/step - loss: 0.0867 - accuracy: 0.9635 - val_loss: 0.3134 - val_accuracy: 0.9112\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.18471\n",
      "Epoch 27/500\n",
      "1040/1040 [==============================] - 0s 345us/step - loss: 0.0912 - accuracy: 0.9615 - val_loss: 0.2111 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.18471\n",
      "Epoch 28/500\n",
      "1040/1040 [==============================] - 0s 346us/step - loss: 0.0522 - accuracy: 0.9817 - val_loss: 0.2502 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.18471\n",
      "Epoch 29/500\n",
      "1040/1040 [==============================] - 0s 346us/step - loss: 0.0515 - accuracy: 0.9779 - val_loss: 0.2374 - val_accuracy: 0.9189\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.18471\n",
      "Epoch 30/500\n",
      "1040/1040 [==============================] - 0s 344us/step - loss: 0.0411 - accuracy: 0.9856 - val_loss: 0.2314 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.18471\n",
      "Epoch 31/500\n",
      "1040/1040 [==============================] - 0s 345us/step - loss: 0.0388 - accuracy: 0.9827 - val_loss: 0.2335 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.18471\n",
      "Epoch 32/500\n",
      "1040/1040 [==============================] - 0s 347us/step - loss: 0.0366 - accuracy: 0.9865 - val_loss: 0.2356 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.18471\n",
      "Epoch 33/500\n",
      "1040/1040 [==============================] - 0s 346us/step - loss: 0.0336 - accuracy: 0.9865 - val_loss: 0.2434 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.18471\n",
      "Epoch 34/500\n",
      "1040/1040 [==============================] - 0s 346us/step - loss: 0.0328 - accuracy: 0.9875 - val_loss: 0.2489 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.18471\n",
      "Epoch 35/500\n",
      "1040/1040 [==============================] - 0s 345us/step - loss: 0.0310 - accuracy: 0.9875 - val_loss: 0.2549 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.18471\n",
      "Epoch 36/500\n",
      "1040/1040 [==============================] - 0s 345us/step - loss: 0.0296 - accuracy: 0.9885 - val_loss: 0.2605 - val_accuracy: 0.9189\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.18471\n",
      "Epoch 37/500\n",
      "1040/1040 [==============================] - 0s 347us/step - loss: 0.0283 - accuracy: 0.9885 - val_loss: 0.2656 - val_accuracy: 0.9189\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.18471\n",
      "Epoch 38/500\n",
      "1040/1040 [==============================] - 0s 346us/step - loss: 0.0269 - accuracy: 0.9894 - val_loss: 0.2711 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.18471\n",
      "Epoch 39/500\n",
      "1040/1040 [==============================] - 0s 346us/step - loss: 0.0254 - accuracy: 0.9904 - val_loss: 0.2717 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.18471\n",
      "Epoch 40/500\n",
      "1040/1040 [==============================] - 0s 346us/step - loss: 0.0253 - accuracy: 0.9904 - val_loss: 0.2723 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.18471\n",
      "Epoch 41/500\n",
      "1040/1040 [==============================] - 0s 347us/step - loss: 0.0252 - accuracy: 0.9904 - val_loss: 0.2729 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.18471\n",
      "Epoch 42/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.0250 - accuracy: 0.9904 - val_loss: 0.2735 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.18471\n",
      "Epoch 43/500\n",
      "1040/1040 [==============================] - 0s 348us/step - loss: 0.0249 - accuracy: 0.9913 - val_loss: 0.2741 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.18471\n",
      "Epoch 44/500\n",
      "1040/1040 [==============================] - 0s 346us/step - loss: 0.0248 - accuracy: 0.9913 - val_loss: 0.2748 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.18471\n",
      "Epoch 45/500\n",
      "1040/1040 [==============================] - 0s 348us/step - loss: 0.0246 - accuracy: 0.9913 - val_loss: 0.2754 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.18471\n",
      "Epoch 46/500\n",
      "1040/1040 [==============================] - 0s 348us/step - loss: 0.0245 - accuracy: 0.9913 - val_loss: 0.2761 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.18471\n",
      "Epoch 47/500\n",
      "1040/1040 [==============================] - 0s 348us/step - loss: 0.0244 - accuracy: 0.9913 - val_loss: 0.2767 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.18471\n",
      "Epoch 48/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0243 - accuracy: 0.9913 - val_loss: 0.2773 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.18471\n",
      "Epoch 00048: early stopping\n",
      "TRAIN: 1299 TEST: 325\n",
      "Train on 1040 samples, validate on 259 samples\n",
      "Epoch 1/500\n",
      "1040/1040 [==============================] - 1s 1ms/step - loss: 0.6889 - accuracy: 0.5144 - val_loss: 0.6902 - val_accuracy: 0.5290\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69023, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold3_0518.h5\n",
      "Epoch 2/500\n",
      "1040/1040 [==============================] - 0s 366us/step - loss: 0.6801 - accuracy: 0.5029 - val_loss: 0.6443 - val_accuracy: 0.5019\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.69023 to 0.64428, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold3_0518.h5\n",
      "Epoch 3/500\n",
      "1040/1040 [==============================] - 0s 359us/step - loss: 0.5549 - accuracy: 0.6952 - val_loss: 0.3541 - val_accuracy: 0.8880\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.64428 to 0.35406, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold3_0518.h5\n",
      "Epoch 4/500\n",
      "1040/1040 [==============================] - 0s 358us/step - loss: 0.4179 - accuracy: 0.8327 - val_loss: 0.3731 - val_accuracy: 0.8417\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35406\n",
      "Epoch 5/500\n",
      "1040/1040 [==============================] - 0s 350us/step - loss: 0.3943 - accuracy: 0.8260 - val_loss: 0.3701 - val_accuracy: 0.8726\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35406\n",
      "Epoch 6/500\n",
      "1040/1040 [==============================] - 0s 349us/step - loss: 0.2926 - accuracy: 0.8827 - val_loss: 0.3242 - val_accuracy: 0.9035\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35406 to 0.32420, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold3_0518.h5\n",
      "Epoch 7/500\n",
      "1040/1040 [==============================] - 0s 358us/step - loss: 0.2620 - accuracy: 0.8990 - val_loss: 0.2208 - val_accuracy: 0.9112\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.32420 to 0.22077, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold3_0518.h5\n",
      "Epoch 8/500\n",
      "1040/1040 [==============================] - 0s 361us/step - loss: 0.2077 - accuracy: 0.9202 - val_loss: 0.2641 - val_accuracy: 0.9151\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.22077\n",
      "Epoch 9/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.2259 - accuracy: 0.8962 - val_loss: 0.3041 - val_accuracy: 0.8996\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.22077\n",
      "Epoch 10/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.2363 - accuracy: 0.9000 - val_loss: 0.2374 - val_accuracy: 0.9112\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.22077\n",
      "Epoch 11/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.1983 - accuracy: 0.9260 - val_loss: 0.3698 - val_accuracy: 0.8378\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.22077\n",
      "Epoch 12/500\n",
      "1040/1040 [==============================] - 0s 352us/step - loss: 0.2125 - accuracy: 0.9096 - val_loss: 0.2700 - val_accuracy: 0.8958\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.22077\n",
      "Epoch 13/500\n",
      "1040/1040 [==============================] - 0s 348us/step - loss: 0.1937 - accuracy: 0.9192 - val_loss: 0.2633 - val_accuracy: 0.8958\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.22077\n",
      "Epoch 14/500\n",
      "1040/1040 [==============================] - 0s 352us/step - loss: 0.1956 - accuracy: 0.9173 - val_loss: 0.3737 - val_accuracy: 0.8919\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.22077\n",
      "Epoch 15/500\n",
      "1040/1040 [==============================] - 0s 358us/step - loss: 0.2176 - accuracy: 0.9010 - val_loss: 0.3092 - val_accuracy: 0.8958\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.22077\n",
      "Epoch 16/500\n",
      "1040/1040 [==============================] - 0s 354us/step - loss: 0.2300 - accuracy: 0.9192 - val_loss: 0.2117 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.22077 to 0.21172, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold3_0518.h5\n",
      "Epoch 17/500\n",
      "1040/1040 [==============================] - 0s 359us/step - loss: 0.1907 - accuracy: 0.9288 - val_loss: 0.4305 - val_accuracy: 0.8185\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.21172\n",
      "Epoch 18/500\n",
      "1040/1040 [==============================] - 0s 350us/step - loss: 0.1936 - accuracy: 0.9250 - val_loss: 0.3497 - val_accuracy: 0.8764\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.21172\n",
      "Epoch 19/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.1966 - accuracy: 0.9250 - val_loss: 0.2515 - val_accuracy: 0.9112\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.21172\n",
      "Epoch 20/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.1747 - accuracy: 0.9317 - val_loss: 0.3406 - val_accuracy: 0.8958\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.21172\n",
      "Epoch 21/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.1873 - accuracy: 0.9212 - val_loss: 0.2160 - val_accuracy: 0.9189\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.21172\n",
      "Epoch 22/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.2003 - accuracy: 0.9288 - val_loss: 0.1661 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.21172 to 0.16612, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold3_0518.h5\n",
      "Epoch 23/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.1453 - accuracy: 0.9413 - val_loss: 0.1664 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.16612\n",
      "Epoch 24/500\n",
      "1040/1040 [==============================] - 0s 350us/step - loss: 0.1131 - accuracy: 0.9529 - val_loss: 0.1599 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.16612 to 0.15994, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold3_0518.h5\n",
      "Epoch 25/500\n",
      "1040/1040 [==============================] - 0s 358us/step - loss: 0.1037 - accuracy: 0.9596 - val_loss: 0.1717 - val_accuracy: 0.9189\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.15994\n",
      "Epoch 26/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.0893 - accuracy: 0.9644 - val_loss: 0.1795 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.15994\n",
      "Epoch 27/500\n",
      "1040/1040 [==============================] - 0s 350us/step - loss: 0.0888 - accuracy: 0.9702 - val_loss: 0.1770 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.15994\n",
      "Epoch 28/500\n",
      "1040/1040 [==============================] - 0s 349us/step - loss: 0.0945 - accuracy: 0.9644 - val_loss: 0.3310 - val_accuracy: 0.8919\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.15994\n",
      "Epoch 29/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.1161 - accuracy: 0.9558 - val_loss: 0.3294 - val_accuracy: 0.8996\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.15994\n",
      "Epoch 30/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.1149 - accuracy: 0.9538 - val_loss: 0.2733 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.15994\n",
      "Epoch 31/500\n",
      "1040/1040 [==============================] - 0s 354us/step - loss: 0.1045 - accuracy: 0.9548 - val_loss: 0.2819 - val_accuracy: 0.9151\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.15994\n",
      "Epoch 32/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.1084 - accuracy: 0.9558 - val_loss: 0.1611 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.15994\n",
      "Epoch 33/500\n",
      "1040/1040 [==============================] - 0s 354us/step - loss: 0.0892 - accuracy: 0.9663 - val_loss: 0.2220 - val_accuracy: 0.9151\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.15994\n",
      "Epoch 34/500\n",
      "1040/1040 [==============================] - 0s 352us/step - loss: 0.1171 - accuracy: 0.9481 - val_loss: 0.2255 - val_accuracy: 0.8996\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.15994\n",
      "Epoch 35/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.1297 - accuracy: 0.9423 - val_loss: 0.1690 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.15994\n",
      "Epoch 36/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.0786 - accuracy: 0.9692 - val_loss: 0.1732 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.15994\n",
      "Epoch 37/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.0634 - accuracy: 0.9731 - val_loss: 0.1550 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.15994 to 0.15497, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold3_0518.h5\n",
      "Epoch 38/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0526 - accuracy: 0.9760 - val_loss: 0.1578 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.15497\n",
      "Epoch 39/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.0462 - accuracy: 0.9856 - val_loss: 0.1640 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.15497\n",
      "Epoch 40/500\n",
      "1040/1040 [==============================] - 0s 354us/step - loss: 0.0425 - accuracy: 0.9856 - val_loss: 0.1698 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.15497\n",
      "Epoch 41/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.0397 - accuracy: 0.9865 - val_loss: 0.1746 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.15497\n",
      "Epoch 42/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.0374 - accuracy: 0.9865 - val_loss: 0.1787 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.15497\n",
      "Epoch 43/500\n",
      "1040/1040 [==============================] - 0s 350us/step - loss: 0.0350 - accuracy: 0.9885 - val_loss: 0.1821 - val_accuracy: 0.9421\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.15497\n",
      "Epoch 44/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.0330 - accuracy: 0.9894 - val_loss: 0.1855 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.15497\n",
      "Epoch 45/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.0312 - accuracy: 0.9904 - val_loss: 0.1890 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.15497\n",
      "Epoch 46/500\n",
      "1040/1040 [==============================] - 0s 354us/step - loss: 0.0294 - accuracy: 0.9904 - val_loss: 0.1925 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.15497\n",
      "Epoch 47/500\n",
      "1040/1040 [==============================] - 0s 352us/step - loss: 0.0278 - accuracy: 0.9904 - val_loss: 0.1961 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.15497\n",
      "Epoch 48/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.0264 - accuracy: 0.9913 - val_loss: 0.1965 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.15497\n",
      "Epoch 49/500\n",
      "1040/1040 [==============================] - 0s 352us/step - loss: 0.0263 - accuracy: 0.9913 - val_loss: 0.1970 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.15497\n",
      "Epoch 50/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.0261 - accuracy: 0.9913 - val_loss: 0.1974 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.15497\n",
      "Epoch 51/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.0260 - accuracy: 0.9913 - val_loss: 0.1978 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.15497\n",
      "Epoch 52/500\n",
      "1040/1040 [==============================] - 0s 353us/step - loss: 0.0258 - accuracy: 0.9913 - val_loss: 0.1982 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.15497\n",
      "Epoch 53/500\n",
      "1040/1040 [==============================] - 0s 354us/step - loss: 0.0257 - accuracy: 0.9913 - val_loss: 0.1985 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.15497\n",
      "Epoch 54/500\n",
      "1040/1040 [==============================] - 0s 352us/step - loss: 0.0255 - accuracy: 0.9913 - val_loss: 0.1989 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.15497\n",
      "Epoch 55/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.0253 - accuracy: 0.9913 - val_loss: 0.1993 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.15497\n",
      "Epoch 56/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.0252 - accuracy: 0.9913 - val_loss: 0.1997 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.15497\n",
      "Epoch 57/500\n",
      "1040/1040 [==============================] - 0s 352us/step - loss: 0.0250 - accuracy: 0.9913 - val_loss: 0.2000 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.15497\n",
      "Epoch 58/500\n",
      "1040/1040 [==============================] - 0s 350us/step - loss: 0.0249 - accuracy: 0.9913 - val_loss: 0.2001 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.15497\n",
      "Epoch 59/500\n",
      "1040/1040 [==============================] - 0s 352us/step - loss: 0.0249 - accuracy: 0.9913 - val_loss: 0.2001 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.15497\n",
      "Epoch 60/500\n",
      "1040/1040 [==============================] - 0s 350us/step - loss: 0.0249 - accuracy: 0.9913 - val_loss: 0.2002 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.15497\n",
      "Epoch 61/500\n",
      "1040/1040 [==============================] - 0s 352us/step - loss: 0.0248 - accuracy: 0.9913 - val_loss: 0.2002 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.15497\n",
      "Epoch 62/500\n",
      "1040/1040 [==============================] - 0s 349us/step - loss: 0.0248 - accuracy: 0.9913 - val_loss: 0.2002 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.15497\n",
      "Epoch 63/500\n",
      "1040/1040 [==============================] - 0s 350us/step - loss: 0.0248 - accuracy: 0.9913 - val_loss: 0.2003 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.15497\n",
      "Epoch 64/500\n",
      "1040/1040 [==============================] - 0s 352us/step - loss: 0.0248 - accuracy: 0.9913 - val_loss: 0.2003 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.15497\n",
      "Epoch 65/500\n",
      "1040/1040 [==============================] - 0s 350us/step - loss: 0.0248 - accuracy: 0.9913 - val_loss: 0.2004 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.15497\n",
      "Epoch 66/500\n",
      "1040/1040 [==============================] - 0s 351us/step - loss: 0.0248 - accuracy: 0.9913 - val_loss: 0.2004 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.15497\n",
      "Epoch 67/500\n",
      "1040/1040 [==============================] - 0s 350us/step - loss: 0.0247 - accuracy: 0.9913 - val_loss: 0.2005 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.15497\n",
      "Epoch 00067: early stopping\n",
      "TRAIN: 1300 TEST: 324\n",
      "Train on 1040 samples, validate on 260 samples\n",
      "Epoch 1/500\n",
      "1040/1040 [==============================] - 1s 1ms/step - loss: 0.6901 - accuracy: 0.4673 - val_loss: 0.6639 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66393, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold4_0518.h5\n",
      "Epoch 2/500\n",
      "1040/1040 [==============================] - 0s 362us/step - loss: 0.5971 - accuracy: 0.6683 - val_loss: 0.5414 - val_accuracy: 0.6923\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.66393 to 0.54136, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold4_0518.h5\n",
      "Epoch 3/500\n",
      "1040/1040 [==============================] - 0s 367us/step - loss: 0.4665 - accuracy: 0.7856 - val_loss: 0.4969 - val_accuracy: 0.7385\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54136 to 0.49695, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold4_0518.h5\n",
      "Epoch 4/500\n",
      "1040/1040 [==============================] - 0s 367us/step - loss: 0.3359 - accuracy: 0.8481 - val_loss: 0.2116 - val_accuracy: 0.9115\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.49695 to 0.21163, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold4_0518.h5\n",
      "Epoch 5/500\n",
      "1040/1040 [==============================] - 0s 366us/step - loss: 0.2215 - accuracy: 0.9019 - val_loss: 0.2180 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.21163\n",
      "Epoch 6/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.1887 - accuracy: 0.9231 - val_loss: 0.1897 - val_accuracy: 0.9269\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.21163 to 0.18968, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold4_0518.h5\n",
      "Epoch 7/500\n",
      "1040/1040 [==============================] - 0s 366us/step - loss: 0.1507 - accuracy: 0.9394 - val_loss: 0.1801 - val_accuracy: 0.9346\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.18968 to 0.18014, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold4_0518.h5\n",
      "Epoch 8/500\n",
      "1040/1040 [==============================] - 0s 368us/step - loss: 0.1330 - accuracy: 0.9500 - val_loss: 0.1802 - val_accuracy: 0.9385\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.18014\n",
      "Epoch 9/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.1311 - accuracy: 0.9519 - val_loss: 0.1781 - val_accuracy: 0.9308\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.18014 to 0.17807, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold4_0518.h5\n",
      "Epoch 10/500\n",
      "1040/1040 [==============================] - 0s 366us/step - loss: 0.1207 - accuracy: 0.9510 - val_loss: 0.1579 - val_accuracy: 0.9346\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.17807 to 0.15789, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold4_0518.h5\n",
      "Epoch 11/500\n",
      "1040/1040 [==============================] - 0s 369us/step - loss: 0.0997 - accuracy: 0.9577 - val_loss: 0.1722 - val_accuracy: 0.9385\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.15789\n",
      "Epoch 12/500\n",
      "1040/1040 [==============================] - 0s 362us/step - loss: 0.0851 - accuracy: 0.9663 - val_loss: 0.1591 - val_accuracy: 0.9346\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.15789\n",
      "Epoch 13/500\n",
      "1040/1040 [==============================] - 0s 359us/step - loss: 0.0803 - accuracy: 0.9692 - val_loss: 0.1656 - val_accuracy: 0.9385\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.15789\n",
      "Epoch 14/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.0696 - accuracy: 0.9683 - val_loss: 0.1562 - val_accuracy: 0.9346\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.15789 to 0.15617, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold4_0518.h5\n",
      "Epoch 15/500\n",
      "1040/1040 [==============================] - 0s 365us/step - loss: 0.1008 - accuracy: 0.9558 - val_loss: 0.3051 - val_accuracy: 0.8769\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.15617\n",
      "Epoch 16/500\n",
      "1040/1040 [==============================] - 0s 359us/step - loss: 0.2223 - accuracy: 0.9125 - val_loss: 0.2107 - val_accuracy: 0.9154\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.15617\n",
      "Epoch 17/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.1601 - accuracy: 0.9356 - val_loss: 0.1746 - val_accuracy: 0.9346\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.15617\n",
      "Epoch 18/500\n",
      "1040/1040 [==============================] - 0s 361us/step - loss: 0.1100 - accuracy: 0.9538 - val_loss: 0.1642 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.15617\n",
      "Epoch 19/500\n",
      "1040/1040 [==============================] - 0s 361us/step - loss: 0.0852 - accuracy: 0.9702 - val_loss: 0.1702 - val_accuracy: 0.9346\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.15617\n",
      "Epoch 20/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.0672 - accuracy: 0.9808 - val_loss: 0.1701 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.15617\n",
      "Epoch 21/500\n",
      "1040/1040 [==============================] - 0s 359us/step - loss: 0.0513 - accuracy: 0.9798 - val_loss: 0.2155 - val_accuracy: 0.9346\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.15617\n",
      "Epoch 22/500\n",
      "1040/1040 [==============================] - 0s 362us/step - loss: 0.1238 - accuracy: 0.9510 - val_loss: 0.1967 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.15617\n",
      "Epoch 23/500\n",
      "1040/1040 [==============================] - 0s 362us/step - loss: 0.0983 - accuracy: 0.9654 - val_loss: 0.1545 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.15617 to 0.15445, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold4_0518.h5\n",
      "Epoch 24/500\n",
      "1040/1040 [==============================] - 0s 365us/step - loss: 0.0635 - accuracy: 0.9760 - val_loss: 0.1503 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.15445 to 0.15028, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold4_0518.h5\n",
      "Epoch 25/500\n",
      "1040/1040 [==============================] - 0s 364us/step - loss: 0.0496 - accuracy: 0.9827 - val_loss: 0.1538 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.15028\n",
      "Epoch 26/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.0351 - accuracy: 0.9875 - val_loss: 0.2097 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.15028\n",
      "Epoch 27/500\n",
      "1040/1040 [==============================] - 0s 361us/step - loss: 0.0288 - accuracy: 0.9904 - val_loss: 0.2923 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.15028\n",
      "Epoch 28/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.0322 - accuracy: 0.9846 - val_loss: 0.1778 - val_accuracy: 0.9500\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.15028\n",
      "Epoch 29/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.0251 - accuracy: 0.9894 - val_loss: 0.1331 - val_accuracy: 0.9500\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.15028 to 0.13310, saving model to ./210518_vgg19_app_max_dense2_0001/model/vgg19_app_fold4_0518.h5\n",
      "Epoch 30/500\n",
      "1040/1040 [==============================] - 0s 369us/step - loss: 0.0422 - accuracy: 0.9846 - val_loss: 0.2104 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.13310\n",
      "Epoch 31/500\n",
      "1040/1040 [==============================] - 0s 361us/step - loss: 0.0463 - accuracy: 0.9856 - val_loss: 0.1453 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.13310\n",
      "Epoch 32/500\n",
      "1040/1040 [==============================] - 0s 362us/step - loss: 0.0234 - accuracy: 0.9865 - val_loss: 0.6419 - val_accuracy: 0.8923\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.13310\n",
      "Epoch 33/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.3990 - accuracy: 0.8510 - val_loss: 0.2747 - val_accuracy: 0.8654\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.13310\n",
      "Epoch 34/500\n",
      "1040/1040 [==============================] - 0s 361us/step - loss: 0.1907 - accuracy: 0.9260 - val_loss: 0.2033 - val_accuracy: 0.9385\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.13310\n",
      "Epoch 35/500\n",
      "1040/1040 [==============================] - 0s 362us/step - loss: 0.1618 - accuracy: 0.9240 - val_loss: 0.1654 - val_accuracy: 0.9308\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.13310\n",
      "Epoch 36/500\n",
      "1040/1040 [==============================] - 0s 362us/step - loss: 0.1018 - accuracy: 0.9587 - val_loss: 0.1616 - val_accuracy: 0.9308\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.13310\n",
      "Epoch 37/500\n",
      "1040/1040 [==============================] - 0s 362us/step - loss: 0.0878 - accuracy: 0.9625 - val_loss: 0.1371 - val_accuracy: 0.9385\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.13310\n",
      "Epoch 38/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.0662 - accuracy: 0.9712 - val_loss: 0.1856 - val_accuracy: 0.9500\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.13310\n",
      "Epoch 39/500\n",
      "1040/1040 [==============================] - 0s 362us/step - loss: 0.0514 - accuracy: 0.9817 - val_loss: 0.1944 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.13310\n",
      "Epoch 40/500\n",
      "1040/1040 [==============================] - 0s 361us/step - loss: 0.0608 - accuracy: 0.9750 - val_loss: 0.1540 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.13310\n",
      "Epoch 41/500\n",
      "1040/1040 [==============================] - 0s 359us/step - loss: 0.0520 - accuracy: 0.9769 - val_loss: 0.1356 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.13310\n",
      "Epoch 42/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.0365 - accuracy: 0.9913 - val_loss: 0.1418 - val_accuracy: 0.9500\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.13310\n",
      "Epoch 43/500\n",
      "1040/1040 [==============================] - 0s 357us/step - loss: 0.0337 - accuracy: 0.9894 - val_loss: 0.1478 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.13310\n",
      "Epoch 44/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.0300 - accuracy: 0.9894 - val_loss: 0.1555 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.13310\n",
      "Epoch 45/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.0275 - accuracy: 0.9913 - val_loss: 0.1600 - val_accuracy: 0.9538\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.13310\n",
      "Epoch 46/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.0257 - accuracy: 0.9923 - val_loss: 0.1608 - val_accuracy: 0.9500\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.13310\n",
      "Epoch 47/500\n",
      "1040/1040 [==============================] - 0s 356us/step - loss: 0.0236 - accuracy: 0.9933 - val_loss: 0.1630 - val_accuracy: 0.9500\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.13310\n",
      "Epoch 48/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.0219 - accuracy: 0.9952 - val_loss: 0.1663 - val_accuracy: 0.9538\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.13310\n",
      "Epoch 49/500\n",
      "1040/1040 [==============================] - 0s 358us/step - loss: 0.0202 - accuracy: 0.9952 - val_loss: 0.1693 - val_accuracy: 0.9538\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.13310\n",
      "Epoch 50/500\n",
      "1040/1040 [==============================] - 0s 361us/step - loss: 0.0187 - accuracy: 0.9952 - val_loss: 0.1697 - val_accuracy: 0.9538\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.13310\n",
      "Epoch 51/500\n",
      "1040/1040 [==============================] - 0s 359us/step - loss: 0.0185 - accuracy: 0.9952 - val_loss: 0.1702 - val_accuracy: 0.9538\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.13310\n",
      "Epoch 52/500\n",
      "1040/1040 [==============================] - 0s 361us/step - loss: 0.0184 - accuracy: 0.9952 - val_loss: 0.1707 - val_accuracy: 0.9538\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.13310\n",
      "Epoch 53/500\n",
      "1040/1040 [==============================] - 0s 361us/step - loss: 0.0182 - accuracy: 0.9952 - val_loss: 0.1711 - val_accuracy: 0.9577\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.13310\n",
      "Epoch 54/500\n",
      "1040/1040 [==============================] - 0s 361us/step - loss: 0.0181 - accuracy: 0.9952 - val_loss: 0.1715 - val_accuracy: 0.9538\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.13310\n",
      "Epoch 55/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.0179 - accuracy: 0.9952 - val_loss: 0.1718 - val_accuracy: 0.9538\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.13310\n",
      "Epoch 56/500\n",
      "1040/1040 [==============================] - 0s 360us/step - loss: 0.0177 - accuracy: 0.9952 - val_loss: 0.1721 - val_accuracy: 0.9538\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.13310\n",
      "Epoch 57/500\n",
      "1040/1040 [==============================] - 0s 359us/step - loss: 0.0176 - accuracy: 0.9952 - val_loss: 0.1724 - val_accuracy: 0.9538\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.13310\n",
      "Epoch 58/500\n",
      "1040/1040 [==============================] - 0s 362us/step - loss: 0.0174 - accuracy: 0.9952 - val_loss: 0.1728 - val_accuracy: 0.9538\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.13310\n",
      "Epoch 59/500\n",
      "1040/1040 [==============================] - 0s 361us/step - loss: 0.0172 - accuracy: 0.9952 - val_loss: 0.1731 - val_accuracy: 0.9538\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.13310\n",
      "Epoch 00059: early stopping\n"
     ]
    }
   ],
   "source": [
    "savepath='./210518_vgg19_app_max_dense2_0001'\n",
    "model_type='vgg19_app'\n",
    "date='0518'\n",
    "os.makedirs(savepath+'/npy', exist_ok=True)\n",
    "os.makedirs(savepath+'/result', exist_ok=True)\n",
    "os.makedirs(savepath+'/model', exist_ok=True)\n",
    "i=0\n",
    "performances = np.ndarray((k,4), dtype=np.float32)\n",
    "\n",
    "for train_index, test_index in skf.split(data_x,data_y):\n",
    "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "#     indexs = np.concatenate((train_index, test_index), axis= 0)\n",
    "#     train_index = indexs[:1300]\n",
    "#     test_index = indexs[1300:]\n",
    "#     print(\"RETRAIN:\", len(train_index), \"RETEST:\", len(test_index))\n",
    "    dtrain_x, test_x = data_x[train_index], data_x[test_index]\n",
    "    dtrain_y, test_y = data_y[train_index], data_y[test_index]\n",
    "    \n",
    "    np.save(savepath+'/npy/test_x_fold{}.npy'.format(str(i)), test_x)\n",
    "    np.save(savepath+'/npy/test_y_fold{}.npy'.format(str(i)), test_y)\n",
    "    np.save(savepath+'/npy/train_x_fold{}.npy'.format(str(i)), train_x)\n",
    "    np.save(savepath+'/npy/train_y_fold{}.npy'.format(str(i)), train_y)\n",
    "    \n",
    "    train_x,val_x,train_y,val_y = train_test_split(dtrain_x,dtrain_y, stratify = dtrain_y, train_size=1040)\n",
    "\n",
    "    input_img = Input(shape=(32,32,1))\n",
    "#     model = keras.applications.resnet.ResNet50(include_top=True, weights=None, input_tensor=None, input_shape=(32,32,2), pooling=None, classes=2)\n",
    "    model= vgg_app_19(input_img)\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "    model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath=savepath+'/model/{}_fold{}_{}.h5'.format(model_type, str(i),date), verbose=1, save_best_only=True, monitor='val_loss')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                                  patience=10, min_lr=0, min_delta=0.001, verbose=1)\n",
    "    earlystopper = EarlyStopping(patience=30, verbose=1, monitor='val_loss')\n",
    "    callbacks_list = [reduce_lr, checkpointer, earlystopper]\n",
    "    \n",
    "    results = model.fit(train_x, train_y, batch_size=128, epochs=500, verbose=1, validation_data=(val_x, val_y), shuffle=False, callbacks=callbacks_list)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "325/325 [==============================] - 1s 2ms/step\n",
      "146 16 156 7\n",
      "###########RESULT########\n",
      "AUC:0.986, SEN:0.954, SPEC:0.907, ACC:0.929, PREC:0.901\n",
      "TP:146, FP:16. TN:156, FN:7\n",
      "#########################\n",
      "1\n",
      "325/325 [==============================] - 1s 2ms/step\n",
      "147 15 161 2\n",
      "###########RESULT########\n",
      "AUC:0.994, SEN:0.987, SPEC:0.915, ACC:0.948, PREC:0.907\n",
      "TP:147, FP:15. TN:161, FN:2\n",
      "#########################\n",
      "2\n",
      "325/325 [==============================] - 1s 2ms/step\n",
      "142 21 156 6\n",
      "###########RESULT########\n",
      "AUC:0.964, SEN:0.959, SPEC:0.881, ACC:0.917, PREC:0.871\n",
      "TP:142, FP:21. TN:156, FN:6\n",
      "#########################\n",
      "3\n",
      "325/325 [==============================] - 1s 3ms/step\n",
      "159 4 160 2\n",
      "###########RESULT########\n",
      "AUC:0.998, SEN:0.988, SPEC:0.976, ACC:0.982, PREC:0.975\n",
      "TP:159, FP:4. TN:160, FN:2\n",
      "#########################\n",
      "4\n",
      "324/324 [==============================] - 1s 3ms/step\n",
      "156 6 143 19\n",
      "###########RESULT########\n",
      "AUC:0.986, SEN:0.891, SPEC:0.96, ACC:0.923, PREC:0.963\n",
      "TP:156, FP:6. TN:143, FN:19\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "# check model and save result\n",
    "i=0\n",
    "performances = np.ndarray((k,4), dtype=np.float32)\n",
    "for i in range(k):\n",
    "    print(i)\n",
    "    test_model = load_model(savepath+'/model/{}_fold{}_{}.h5'.format(model_type,str(i),date))\n",
    "    test_x= np.load(savepath+'/npy/test_x_fold{}.npy'.format(str(i)))\n",
    "    test_y= np.load(savepath+'/npy/test_y_fold{}.npy'.format(str(i)))\n",
    "    test_result= test_model.predict(test_x, batch_size=128, verbose=1)\n",
    "    np.save(savepath+'/npy/test_result_fold{}.npy'.format(str(i)), test_result)\n",
    "    fpr_res,tpr_res,_=roc_curve(test_y,test_result)\n",
    "    auc_res=auc(fpr_res, tpr_res)\n",
    "    #     print(\"loss: %.2f, : %.3f  \" %(score[0], score[1]))\n",
    "    res_tp, res_fp, res_tn, res_fn, res_sensitivity, res_specificity, acc, prec= calculate_performance(test_result, test_y)\n",
    "    \n",
    "    performanceList = [auc_res, res_sensitivity, res_specificity, acc]\n",
    "    for a in range(4):\n",
    "        performances[i][a] = performanceList[a]\n",
    "    \n",
    "#     interp_tpr = np.interp(mean_fpr, _, _)\n",
    "#     interp_tpr[0] = 0.0\n",
    "#     tprs.append(interp_tpr)\n",
    "#     np.save('./210512/npy/performanceList_fold{}.npy'.format(str(i)), performances)\n",
    "    print('###########RESULT########')\n",
    "    print('AUC:{}, SEN:{}, SPEC:{}, ACC:{}, PREC:{}'.format(round(auc_res,3),round(res_sensitivity,3),round(res_specificity,3),round(acc,3),round(prec,3)))\n",
    "    print('TP:{}, FP:{}. TN:{}, FN:{}'.format(res_tp, res_fp, res_tn, res_fn))\n",
    "    print('#########################')\n",
    "    i=+1\n",
    "np.save(savepath+'/npy/{}_performanceList.npy'.format(model_type), performances)\n",
    "all_perform = pd.DataFrame(performances)\n",
    "all_perform.to_csv(savepath+'/result/all_{}_perform.csv'.format(model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "146 16 156 7\n",
      "###########RESULT########\n",
      "AUC:0.986, SEN:0.954, SPEC:0.907, ACC:0.929, PREC:0.901\n",
      "TP:146, FP:16. TN:156, FN:7\n",
      "#########################\n",
      "1\n",
      "147 15 161 2\n",
      "###########RESULT########\n",
      "AUC:0.994, SEN:0.987, SPEC:0.915, ACC:0.948, PREC:0.907\n",
      "TP:147, FP:15. TN:161, FN:2\n",
      "#########################\n",
      "2\n",
      "142 21 156 6\n",
      "###########RESULT########\n",
      "AUC:0.964, SEN:0.959, SPEC:0.881, ACC:0.917, PREC:0.871\n",
      "TP:142, FP:21. TN:156, FN:6\n",
      "#########################\n",
      "3\n",
      "159 4 160 2\n",
      "###########RESULT########\n",
      "AUC:0.998, SEN:0.988, SPEC:0.976, ACC:0.982, PREC:0.975\n",
      "TP:159, FP:4. TN:160, FN:2\n",
      "#########################\n",
      "4\n",
      "156 6 143 19\n",
      "###########RESULT########\n",
      "AUC:0.986, SEN:0.891, SPEC:0.96, ACC:0.923, PREC:0.963\n",
      "TP:156, FP:6. TN:143, FN:19\n",
      "#########################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(performanceList)\n",
    "\n",
    "k=5\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "colors = ['salmon', 'peru', 'goldenrod', 'olivedrab', 'royalblue']\n",
    "# fig=plt.figure(figsize=(20,20))\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "# performances = np.ndarray((k,4), dtype=np.float32)\n",
    "for i in range(k):\n",
    "    print(i)\n",
    "#     test_model = load_model(savepath+'/model/residual3_filter4_fold{}_0512.h5'.format(str(i)))\n",
    "    test_x= np.load(savepath+'/npy/test_x_fold{}.npy'.format(str(i)))\n",
    "    test_y= np.load(savepath+'/npy/test_y_fold{}.npy'.format(str(i)))\n",
    "    test_result= np.load(savepath+'/npy/test_result_fold{}.npy'.format(str(i)))\n",
    "#     np.save(savepath+'/npy/test_result_fold{}.npy'.format(str(i)), test_result)\n",
    "    fpr_res,tpr_res,_=roc_curve(test_y,test_result)\n",
    "    auc_res=auc(fpr_res, tpr_res)\n",
    "    #     print(\"loss: %.2f, : %.3f  \" %(score[0], score[1]))\n",
    "    res_tp, res_fp, res_tn, res_fn, res_sensitivity, res_specificity, acc, prec= calculate_performance(test_result, test_y)\n",
    "    \n",
    "    performanceList = [auc_res, res_sensitivity, res_specificity, acc]\n",
    "    for a in range(4):\n",
    "        performances[i][a] = performanceList[a]\n",
    "    \n",
    "#     interp_tpr = np.interp(mean_fpr, _, _)\n",
    "#     interp_tpr[0] = 0.0\n",
    "#     tprs.append(interp_tpr)\n",
    "#     np.save('./210512/npy/performanceList_fold{}.npy'.format(str(i)), performances)\n",
    "    print('###########RESULT########')\n",
    "    print('AUC:{}, SEN:{}, SPEC:{}, ACC:{}, PREC:{}'.format(round(auc_res,3),round(res_sensitivity,3),round(res_specificity,3),round(acc,3),round(prec,3)))\n",
    "    print('TP:{}, FP:{}. TN:{}, FN:{}'.format(res_tp, res_fp, res_tn, res_fn))\n",
    "    print('#########################')\n",
    "#     i=+1\n",
    "\n",
    "    interp_tpr = np.interp(mean_fpr, fpr_res,tpr_res)\n",
    "    interp_tpr[0] = 0.0\n",
    "    \n",
    "#     fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.plot(mean_fpr, interp_tpr, color=colors[i],\n",
    "        label=r'fold%s ROC (AUC = %0.3f)' % (str(i),auc_res),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(auc_res)\n",
    "    \n",
    "# fig, ax = plt.subplots()\n",
    "# fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='black',\n",
    "        label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr_xgb = np.mean(tprs, axis=0)\n",
    "mean_tpr_xgb[-1] = 1.0\n",
    "mean_auc_xgb = auc(mean_fpr, mean_tpr_xgb)\n",
    "std_auc_xgb = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr_xgb, color='maroon',\n",
    "        label=r'Mean ROC (AUC = %0.3f $\\pm$ %0.3f)' % (mean_auc_xgb, std_auc_xgb),\n",
    "        lw=2, alpha=.8)\n",
    "# ax.plot(mean_fpr, interp_tpr, color='navy',\n",
    "#         label=r'Mean ROC (AUC = %0.3f)' % (auc_res),\n",
    "#         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr_xgb = np.std(tprs, axis=0)\n",
    "tprs_upper_xgb = np.minimum(mean_tpr_xgb + std_tpr_xgb, 1)\n",
    "tprs_lower_xgb = np.maximum(mean_tpr_xgb - std_tpr_xgb, 0)\n",
    "ax.fill_between(mean_fpr, tprs_lower_xgb, tprs_upper_xgb, color='darkgray', alpha=.2,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlabel('False positive rate',fontsize=20)\n",
    "plt.ylabel('True positive rate',fontsize=20)\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"Receiver operating characteristic\")\n",
    "ax.set_title(\"Receiver operating characteristic\",fontsize=20)\n",
    "\n",
    "ax.legend(loc=\"lower right\")\n",
    "# fig.savefig('./result/0512/LR_embedded_cv5.jpg')\n",
    "# plt.figure(figsize=(20,20))\n",
    "plt.show()\n",
    "fig.savefig(savepath+'/result/{}_cv5_first_{}.jpg'.format(model_type, date), dpi=300)\n",
    "\n",
    "np.save(savepath+'/result/mean_tpr_{}_{}.npy'.format(model_type, date), mean_tpr_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "check2 = np.load(savepath+'/npy/test_result_fold2.npy')\n",
    "checkimg = np.load(savepath+'/npy/test_x_fold2.npy')\n",
    "checklab = np.load(savepath+'/npy/test_y_fold2.npy')\n",
    "for ck in range(len(checkimg)):\n",
    "    io.imsave(savepath+'/check/{}_lab{}.jpg'.format(ck,checklab[ck]), checkimg[ck])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_test_split in module sklearn.model_selection._split:\n",
      "\n",
      "train_test_split(*arrays, **options)\n",
      "    Split arrays or matrices into random train and test subsets\n",
      "    \n",
      "    Quick utility that wraps input validation and\n",
      "    ``next(ShuffleSplit().split(X, y))`` and application to input data\n",
      "    into a single call for splitting (and optionally subsampling) data in a\n",
      "    oneliner.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    *arrays : sequence of indexables with same length / shape[0]\n",
      "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "        matrices or pandas dataframes.\n",
      "    \n",
      "    test_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "        of the dataset to include in the test split. If int, represents the\n",
      "        absolute number of test samples. If None, the value is set to the\n",
      "        complement of the train size. If ``train_size`` is also None, it will\n",
      "        be set to 0.25.\n",
      "    \n",
      "    train_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the\n",
      "        proportion of the dataset to include in the train split. If\n",
      "        int, represents the absolute number of train samples. If None,\n",
      "        the value is automatically set to the complement of the test size.\n",
      "    \n",
      "    random_state : int or RandomState instance, default=None\n",
      "        Controls the shuffling applied to the data before applying the split.\n",
      "        Pass an int for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    \n",
      "    shuffle : bool, default=True\n",
      "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "        then stratify must be None.\n",
      "    \n",
      "    stratify : array-like, default=None\n",
      "        If not None, data is split in a stratified fashion, using this as\n",
      "        the class labels.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    splitting : list, length=2 * len(arrays)\n",
      "        List containing train-test split of inputs.\n",
      "    \n",
      "        .. versionadded:: 0.16\n",
      "            If the input is sparse, the output will be a\n",
      "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "            input type.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.model_selection import train_test_split\n",
      "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "    >>> X\n",
      "    array([[0, 1],\n",
      "           [2, 3],\n",
      "           [4, 5],\n",
      "           [6, 7],\n",
      "           [8, 9]])\n",
      "    >>> list(y)\n",
      "    [0, 1, 2, 3, 4]\n",
      "    \n",
      "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "    ...     X, y, test_size=0.33, random_state=42)\n",
      "    ...\n",
      "    >>> X_train\n",
      "    array([[4, 5],\n",
      "           [0, 1],\n",
      "           [6, 7]])\n",
      "    >>> y_train\n",
      "    [2, 0, 3]\n",
      "    >>> X_test\n",
      "    array([[2, 3],\n",
      "           [8, 9]])\n",
      "    >>> y_test\n",
      "    [1, 4]\n",
      "    \n",
      "    >>> train_test_split(y, shuffle=False)\n",
      "    [[0, 1, 2], [3, 4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit in module keras.engine.training:\n",
      "\n",
      "fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False, **kwargs) method of keras.engine.training.Model instance\n",
      "    Trains the model for a fixed number of epochs (iterations on a dataset).\n",
      "    \n",
      "    # Arguments\n",
      "        x: Input data. It could be:\n",
      "            - A Numpy array (or array-like), or a list of arrays\n",
      "              (in case the model has multiple inputs).\n",
      "            - A dict mapping input names to the corresponding\n",
      "              array/tensors, if the model has named inputs.\n",
      "            - A generator or `keras.utils.Sequence` returning\n",
      "              `(inputs, targets)` or `(inputs, targets, sample weights)`.\n",
      "            - None (default) if feeding from framework-native\n",
      "              tensors (e.g. TensorFlow data tensors).\n",
      "        y: Target data. Like the input data `x`,\n",
      "            it could be either Numpy array(s), framework-native tensor(s),\n",
      "            list of Numpy arrays (if the model has multiple outputs) or\n",
      "            None (default) if feeding from framework-native tensors\n",
      "            (e.g. TensorFlow data tensors).\n",
      "            If output layers in the model are named, you can also pass a\n",
      "            dictionary mapping output names to Numpy arrays.\n",
      "            If `x` is a generator, or `keras.utils.Sequence` instance,\n",
      "            `y` should not be specified (since targets will be obtained\n",
      "            from `x`).\n",
      "        batch_size: Integer or `None`.\n",
      "            Number of samples per gradient update.\n",
      "            If unspecified, `batch_size` will default to 32.\n",
      "            Do not specify the `batch_size` if your data is in the\n",
      "            form of symbolic tensors, generators, or `Sequence` instances\n",
      "            (since they generate batches).\n",
      "        epochs: Integer. Number of epochs to train the model.\n",
      "            An epoch is an iteration over the entire `x` and `y`\n",
      "            data provided.\n",
      "            Note that in conjunction with `initial_epoch`,\n",
      "            `epochs` is to be understood as \"final epoch\".\n",
      "            The model is not trained for a number of iterations\n",
      "            given by `epochs`, but merely until the epoch\n",
      "            of index `epochs` is reached.\n",
      "        verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
      "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "        callbacks: List of `keras.callbacks.Callback` instances.\n",
      "            List of callbacks to apply during training and validation\n",
      "            (if ).\n",
      "            See [callbacks](/callbacks).\n",
      "        validation_split: Float between 0 and 1.\n",
      "            Fraction of the training data to be used as validation data.\n",
      "            The model will set apart this fraction of the training data,\n",
      "            will not train on it, and will evaluate\n",
      "            the loss and any model metrics\n",
      "            on this data at the end of each epoch.\n",
      "            The validation data is selected from the last samples\n",
      "            in the `x` and `y` data provided, before shuffling.\n",
      "            This argument is not supported when `x` is a generator or\n",
      "            `Sequence` instance.\n",
      "        validation_data: Data on which to evaluate\n",
      "            the loss and any model metrics at the end of each epoch.\n",
      "            The model will not be trained on this data.\n",
      "            `validation_data` will override `validation_split`.\n",
      "            `validation_data` could be:\n",
      "                - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
      "                - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
      "                - dataset or a dataset iterator\n",
      "            For the first two cases, `batch_size` must be provided.\n",
      "            For the last case, `validation_steps` must be provided.\n",
      "        shuffle: Boolean (whether to shuffle the training data\n",
      "            before each epoch) or str (for 'batch').\n",
      "            'batch' is a special option for dealing with the\n",
      "            limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
      "            Has no effect when `steps_per_epoch` is not `None`.\n",
      "        class_weight: Optional dictionary mapping class indices (integers)\n",
      "            to a weight (float) value, used for weighting the loss function\n",
      "            (during training only).\n",
      "            This can be useful to tell the model to\n",
      "            \"pay more attention\" to samples from\n",
      "            an under-represented class.\n",
      "        sample_weight: Optional Numpy array of weights for\n",
      "            the training samples, used for weighting the loss function\n",
      "            (during training only). You can either pass a flat (1D)\n",
      "            Numpy array with the same length as the input samples\n",
      "            (1:1 mapping between weights and samples),\n",
      "            or in the case of temporal data,\n",
      "            you can pass a 2D array with shape\n",
      "            `(samples, sequence_length)`,\n",
      "            to apply a different weight to every timestep of every sample.\n",
      "            In this case you should make sure to specify\n",
      "            `sample_weight_mode=\"temporal\"` in `compile()`. This argument\n",
      "            is not supported when `x` generator, or `Sequence` instance,\n",
      "            instead provide the sample_weights as the third element of `x`.\n",
      "        initial_epoch: Integer.\n",
      "            Epoch at which to start training\n",
      "            (useful for resuming a previous training run).\n",
      "        steps_per_epoch: Integer or `None`.\n",
      "            Total number of steps (batches of samples)\n",
      "            before declaring one epoch finished and starting the\n",
      "            next epoch. When training with input tensors such as\n",
      "            TensorFlow data tensors, the default `None` is equal to\n",
      "            the number of samples in your dataset divided by\n",
      "            the batch size, or 1 if that cannot be determined.\n",
      "        validation_steps: Only relevant if `steps_per_epoch`\n",
      "            is specified. Total number of steps (batches of samples)\n",
      "            to validate before stopping.\n",
      "        validation_steps: Only relevant if `validation_data` is provided\n",
      "            and is a generator. Total number of steps (batches of samples)\n",
      "            to draw before stopping when performing validation at the end\n",
      "            of every epoch.\n",
      "        validation_freq: Only relevant if validation data is provided. Integer\n",
      "            or list/tuple/set. If an integer, specifies how many training\n",
      "            epochs to run before a new validation run is performed, e.g.\n",
      "            `validation_freq=2` runs validation every 2 epochs. If a list,\n",
      "            tuple, or set, specifies the epochs on which to run validation,\n",
      "            e.g. `validation_freq=[1, 2, 10]` runs validation at the end\n",
      "            of the 1st, 2nd, and 10th epochs.\n",
      "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
      "            input only. Maximum size for the generator queue.\n",
      "            If unspecified, `max_queue_size` will default to 10.\n",
      "        workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      "            only. Maximum number of processes to spin up\n",
      "            when using process-based threading. If unspecified, `workers`\n",
      "            will default to 1. If 0, will execute the generator on the main\n",
      "            thread.\n",
      "        use_multiprocessing: Boolean. Used for generator or\n",
      "            `keras.utils.Sequence` input only. If `True`, use process-based\n",
      "            threading. If unspecified, `use_multiprocessing` will default to\n",
      "            `False`. Note that because this implementation relies on\n",
      "            multiprocessing, you should not pass non-picklable arguments to\n",
      "            the generator as they can't be passed easily to children processes.\n",
      "        **kwargs: Used for backwards compatibility.\n",
      "    \n",
      "    # Returns\n",
      "        A `History` object. Its `History.history` attribute is\n",
      "        a record of training loss values and metrics values\n",
      "        at successive epochs, as well as validation loss values\n",
      "        and validation metrics values (if applicable).\n",
      "    \n",
      "    # Raises\n",
      "        RuntimeError: If the model was never compiled.\n",
      "        ValueError: In case of mismatch between the provided input data\n",
      "            and what the model expects.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(?, 15, 15, 4)\n",
      "1\n",
      "(?, 15, 15, 4) 2 1 4\n",
      "input\n",
      "shortcut: (?, 15, 15, 4)\n",
      "x: (?, 15, 15, 4)\n",
      "before add\n",
      "shortcut: (?, 15, 15, 8)\n",
      "x: (?, 15, 15, 8)\n",
      "(?, 15, 15, 8)\n",
      "8\n",
      "(?, 7, 7, 8)\n",
      "8\n",
      "4\n",
      "4\n",
      "2\n",
      "Train on 1040 samples, validate on 259 samples\n",
      "Epoch 1/500\n",
      "1040/1040 [==============================] - 4s 4ms/step - loss: 0.7393 - accuracy: 0.4808 - val_loss: 17.5577 - val_accuracy: 0.4981\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 17.55765, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 2/500\n",
      "1040/1040 [==============================] - 0s 221us/step - loss: 0.6434 - accuracy: 0.6798 - val_loss: 3.2610 - val_accuracy: 0.5019\n",
      "\n",
      "Epoch 00002: val_loss improved from 17.55765 to 3.26097, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 3/500\n",
      "1040/1040 [==============================] - 0s 186us/step - loss: 0.5888 - accuracy: 0.7692 - val_loss: 0.7991 - val_accuracy: 0.5251\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.26097 to 0.79906, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 4/500\n",
      "1040/1040 [==============================] - 0s 250us/step - loss: 0.5308 - accuracy: 0.8135 - val_loss: 0.5961 - val_accuracy: 0.6873\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.79906 to 0.59615, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 5/500\n",
      "1040/1040 [==============================] - 0s 195us/step - loss: 0.4543 - accuracy: 0.8519 - val_loss: 0.6644 - val_accuracy: 0.5483\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.59615\n",
      "Epoch 6/500\n",
      "1040/1040 [==============================] - 0s 210us/step - loss: 0.3672 - accuracy: 0.8904 - val_loss: 0.6962 - val_accuracy: 0.5251\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.59615\n",
      "Epoch 7/500\n",
      "1040/1040 [==============================] - 0s 258us/step - loss: 0.2993 - accuracy: 0.9115 - val_loss: 0.7012 - val_accuracy: 0.5135\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.59615\n",
      "Epoch 8/500\n",
      "1040/1040 [==============================] - 0s 280us/step - loss: 0.2472 - accuracy: 0.9298 - val_loss: 0.7052 - val_accuracy: 0.5290\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.59615\n",
      "Epoch 9/500\n",
      "1040/1040 [==============================] - 0s 244us/step - loss: 0.2098 - accuracy: 0.9413 - val_loss: 0.7192 - val_accuracy: 0.5328\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.59615\n",
      "Epoch 10/500\n",
      "1040/1040 [==============================] - 0s 245us/step - loss: 0.1828 - accuracy: 0.9510 - val_loss: 0.7389 - val_accuracy: 0.5405\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.59615\n",
      "Epoch 11/500\n",
      "1040/1040 [==============================] - 0s 165us/step - loss: 0.1629 - accuracy: 0.9519 - val_loss: 0.7551 - val_accuracy: 0.5483\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.59615\n",
      "Epoch 12/500\n",
      "1040/1040 [==============================] - 0s 212us/step - loss: 0.1477 - accuracy: 0.9548 - val_loss: 0.7643 - val_accuracy: 0.5637\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.59615\n",
      "Epoch 13/500\n",
      "1040/1040 [==============================] - 0s 209us/step - loss: 0.1360 - accuracy: 0.9558 - val_loss: 0.7658 - val_accuracy: 0.5676\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.59615\n",
      "Epoch 14/500\n",
      "1040/1040 [==============================] - 0s 317us/step - loss: 0.1265 - accuracy: 0.9567 - val_loss: 0.7524 - val_accuracy: 0.6062\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.59615\n",
      "Epoch 15/500\n",
      "1040/1040 [==============================] - 0s 235us/step - loss: 0.1207 - accuracy: 0.9577 - val_loss: 0.7103 - val_accuracy: 0.6255\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.59615\n",
      "Epoch 16/500\n",
      "1040/1040 [==============================] - 0s 264us/step - loss: 0.1200 - accuracy: 0.9587 - val_loss: 0.6507 - val_accuracy: 0.6486\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.59615\n",
      "Epoch 17/500\n",
      "1040/1040 [==============================] - 0s 142us/step - loss: 0.1192 - accuracy: 0.9587 - val_loss: 0.5803 - val_accuracy: 0.6988\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.59615 to 0.58034, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 18/500\n",
      "1040/1040 [==============================] - 0s 196us/step - loss: 0.1185 - accuracy: 0.9587 - val_loss: 0.5111 - val_accuracy: 0.7297\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.58034 to 0.51105, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 19/500\n",
      "1040/1040 [==============================] - 0s 213us/step - loss: 0.1178 - accuracy: 0.9587 - val_loss: 0.4469 - val_accuracy: 0.7722\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.51105 to 0.44688, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 20/500\n",
      "1040/1040 [==============================] - 0s 253us/step - loss: 0.1170 - accuracy: 0.9587 - val_loss: 0.3918 - val_accuracy: 0.7992\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.44688 to 0.39180, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 21/500\n",
      "1040/1040 [==============================] - 0s 219us/step - loss: 0.1163 - accuracy: 0.9587 - val_loss: 0.3489 - val_accuracy: 0.8456\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.39180 to 0.34885, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 22/500\n",
      "1040/1040 [==============================] - 0s 201us/step - loss: 0.1155 - accuracy: 0.9587 - val_loss: 0.3155 - val_accuracy: 0.8726\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.34885 to 0.31550, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 23/500\n",
      "1040/1040 [==============================] - 0s 200us/step - loss: 0.1147 - accuracy: 0.9587 - val_loss: 0.2893 - val_accuracy: 0.8803\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.31550 to 0.28927, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 24/500\n",
      "1040/1040 [==============================] - 0s 264us/step - loss: 0.1140 - accuracy: 0.9606 - val_loss: 0.2677 - val_accuracy: 0.8958\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.28927 to 0.26774, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 25/500\n",
      "1040/1040 [==============================] - 0s 284us/step - loss: 0.1132 - accuracy: 0.9606 - val_loss: 0.2502 - val_accuracy: 0.9035\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.26774 to 0.25018, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 26/500\n",
      "1040/1040 [==============================] - 0s 245us/step - loss: 0.1125 - accuracy: 0.9606 - val_loss: 0.2353 - val_accuracy: 0.9151\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.25018 to 0.23535, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 27/500\n",
      "1040/1040 [==============================] - 0s 257us/step - loss: 0.1117 - accuracy: 0.9606 - val_loss: 0.2224 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.23535 to 0.22239, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 28/500\n",
      "1040/1040 [==============================] - 0s 239us/step - loss: 0.1110 - accuracy: 0.9606 - val_loss: 0.2106 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.22239 to 0.21064, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 29/500\n",
      "1040/1040 [==============================] - 0s 273us/step - loss: 0.1102 - accuracy: 0.9615 - val_loss: 0.2001 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.21064 to 0.20011, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 30/500\n",
      "1040/1040 [==============================] - 0s 217us/step - loss: 0.1095 - accuracy: 0.9615 - val_loss: 0.1911 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.20011 to 0.19106, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 31/500\n",
      "1040/1040 [==============================] - 0s 224us/step - loss: 0.1088 - accuracy: 0.9615 - val_loss: 0.1835 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.19106 to 0.18348, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 32/500\n",
      "1040/1040 [==============================] - 0s 226us/step - loss: 0.1081 - accuracy: 0.9615 - val_loss: 0.1772 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.18348 to 0.17724, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 33/500\n",
      "1040/1040 [==============================] - 0s 226us/step - loss: 0.1074 - accuracy: 0.9615 - val_loss: 0.1714 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.17724 to 0.17140, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 34/500\n",
      "1040/1040 [==============================] - 0s 241us/step - loss: 0.1067 - accuracy: 0.9615 - val_loss: 0.1665 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.17140 to 0.16646, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 35/500\n",
      "1040/1040 [==============================] - 0s 226us/step - loss: 0.1060 - accuracy: 0.9615 - val_loss: 0.1623 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.16646 to 0.16231, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 36/500\n",
      "1040/1040 [==============================] - 0s 281us/step - loss: 0.1053 - accuracy: 0.9615 - val_loss: 0.1587 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.16231 to 0.15875, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 37/500\n",
      "1040/1040 [==============================] - 0s 215us/step - loss: 0.1046 - accuracy: 0.9615 - val_loss: 0.1554 - val_accuracy: 0.9344\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.15875 to 0.15542, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 38/500\n",
      "1040/1040 [==============================] - 0s 227us/step - loss: 0.1039 - accuracy: 0.9625 - val_loss: 0.1524 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.15542 to 0.15241, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 39/500\n",
      "1040/1040 [==============================] - 0s 349us/step - loss: 0.1032 - accuracy: 0.9625 - val_loss: 0.1496 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.15241 to 0.14956, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 40/500\n",
      "1040/1040 [==============================] - 0s 227us/step - loss: 0.1025 - accuracy: 0.9635 - val_loss: 0.1469 - val_accuracy: 0.9382\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.14956 to 0.14694, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 41/500\n",
      "1040/1040 [==============================] - 0s 190us/step - loss: 0.1019 - accuracy: 0.9635 - val_loss: 0.1447 - val_accuracy: 0.9459\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.14694 to 0.14473, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 42/500\n",
      "1040/1040 [==============================] - 0s 264us/step - loss: 0.1012 - accuracy: 0.9644 - val_loss: 0.1428 - val_accuracy: 0.9459\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.14473 to 0.14277, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 43/500\n",
      "1040/1040 [==============================] - 0s 219us/step - loss: 0.1005 - accuracy: 0.9654 - val_loss: 0.1409 - val_accuracy: 0.9459\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.14277 to 0.14092, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 44/500\n",
      "1040/1040 [==============================] - 0s 278us/step - loss: 0.0999 - accuracy: 0.9654 - val_loss: 0.1392 - val_accuracy: 0.9498\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.14092 to 0.13923, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 45/500\n",
      "1040/1040 [==============================] - 0s 197us/step - loss: 0.0992 - accuracy: 0.9654 - val_loss: 0.1377 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.13923 to 0.13770, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 46/500\n",
      "1040/1040 [==============================] - 0s 298us/step - loss: 0.0986 - accuracy: 0.9654 - val_loss: 0.1363 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.13770 to 0.13630, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 47/500\n",
      "1040/1040 [==============================] - 0s 231us/step - loss: 0.0980 - accuracy: 0.9663 - val_loss: 0.1350 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.13630 to 0.13495, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 48/500\n",
      "1040/1040 [==============================] - 0s 215us/step - loss: 0.0973 - accuracy: 0.9663 - val_loss: 0.1337 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.13495 to 0.13370, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 49/500\n",
      "1040/1040 [==============================] - 0s 305us/step - loss: 0.0967 - accuracy: 0.9663 - val_loss: 0.1325 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.13370 to 0.13253, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 50/500\n",
      "1040/1040 [==============================] - 0s 213us/step - loss: 0.0961 - accuracy: 0.9663 - val_loss: 0.1315 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.13253 to 0.13151, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 51/500\n",
      "1040/1040 [==============================] - 0s 257us/step - loss: 0.0955 - accuracy: 0.9663 - val_loss: 0.1305 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.13151 to 0.13054, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 52/500\n",
      "1040/1040 [==============================] - 0s 262us/step - loss: 0.0949 - accuracy: 0.9663 - val_loss: 0.1296 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.13054 to 0.12960, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 53/500\n",
      "1040/1040 [==============================] - 0s 196us/step - loss: 0.0943 - accuracy: 0.9663 - val_loss: 0.1287 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.12960 to 0.12871, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 54/500\n",
      "1040/1040 [==============================] - 0s 310us/step - loss: 0.0937 - accuracy: 0.9663 - val_loss: 0.1278 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.12871 to 0.12780, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 55/500\n",
      "1040/1040 [==============================] - 0s 186us/step - loss: 0.0931 - accuracy: 0.9663 - val_loss: 0.1269 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.12780 to 0.12693, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 56/500\n",
      "1040/1040 [==============================] - 0s 281us/step - loss: 0.0925 - accuracy: 0.9663 - val_loss: 0.1261 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.12693 to 0.12609, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 57/500\n",
      "1040/1040 [==============================] - 0s 260us/step - loss: 0.0919 - accuracy: 0.9663 - val_loss: 0.1253 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.12609 to 0.12527, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 58/500\n",
      "1040/1040 [==============================] - 0s 230us/step - loss: 0.0914 - accuracy: 0.9663 - val_loss: 0.1245 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.12527 to 0.12451, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 59/500\n",
      "1040/1040 [==============================] - 0s 255us/step - loss: 0.0908 - accuracy: 0.9663 - val_loss: 0.1238 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.12451 to 0.12378, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 60/500\n",
      "1040/1040 [==============================] - 0s 309us/step - loss: 0.0902 - accuracy: 0.9663 - val_loss: 0.1231 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.12378 to 0.12307, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 61/500\n",
      "1040/1040 [==============================] - 0s 216us/step - loss: 0.0897 - accuracy: 0.9663 - val_loss: 0.1224 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.12307 to 0.12237, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 62/500\n",
      "1040/1040 [==============================] - 0s 127us/step - loss: 0.0891 - accuracy: 0.9663 - val_loss: 0.1217 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.12237 to 0.12167, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 63/500\n",
      "1040/1040 [==============================] - 0s 221us/step - loss: 0.0886 - accuracy: 0.9663 - val_loss: 0.1210 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.12167 to 0.12098, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 64/500\n",
      "1040/1040 [==============================] - 0s 243us/step - loss: 0.0880 - accuracy: 0.9663 - val_loss: 0.1203 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.12098 to 0.12029, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 65/500\n",
      "1040/1040 [==============================] - 0s 252us/step - loss: 0.0875 - accuracy: 0.9663 - val_loss: 0.1197 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.12029 to 0.11968, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 66/500\n",
      "1040/1040 [==============================] - 0s 332us/step - loss: 0.0870 - accuracy: 0.9663 - val_loss: 0.1191 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.11968 to 0.11910, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 67/500\n",
      "1040/1040 [==============================] - 0s 218us/step - loss: 0.0864 - accuracy: 0.9663 - val_loss: 0.1185 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.11910 to 0.11852, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 68/500\n",
      "1040/1040 [==============================] - 0s 237us/step - loss: 0.0859 - accuracy: 0.9663 - val_loss: 0.1180 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.11852 to 0.11799, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 69/500\n",
      "1040/1040 [==============================] - 0s 279us/step - loss: 0.0854 - accuracy: 0.9663 - val_loss: 0.1175 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.11799 to 0.11751, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 70/500\n",
      "1040/1040 [==============================] - 0s 234us/step - loss: 0.0849 - accuracy: 0.9663 - val_loss: 0.1170 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.11751 to 0.11701, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 71/500\n",
      "1040/1040 [==============================] - 0s 241us/step - loss: 0.0844 - accuracy: 0.9663 - val_loss: 0.1165 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.11701 to 0.11652, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 72/500\n",
      "1040/1040 [==============================] - 0s 235us/step - loss: 0.0839 - accuracy: 0.9663 - val_loss: 0.1160 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.11652 to 0.11601, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 73/500\n",
      "1040/1040 [==============================] - 0s 258us/step - loss: 0.0834 - accuracy: 0.9663 - val_loss: 0.1155 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.11601 to 0.11549, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 74/500\n",
      "1040/1040 [==============================] - 0s 271us/step - loss: 0.0829 - accuracy: 0.9663 - val_loss: 0.1150 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.11549 to 0.11498, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 75/500\n",
      "1040/1040 [==============================] - 0s 250us/step - loss: 0.0824 - accuracy: 0.9663 - val_loss: 0.1145 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.11498 to 0.11450, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 76/500\n",
      "1040/1040 [==============================] - 0s 231us/step - loss: 0.0819 - accuracy: 0.9663 - val_loss: 0.1140 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.11450 to 0.11401, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 77/500\n",
      "1040/1040 [==============================] - 0s 302us/step - loss: 0.0814 - accuracy: 0.9663 - val_loss: 0.1135 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.11401 to 0.11352, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 78/500\n",
      "1040/1040 [==============================] - 0s 219us/step - loss: 0.0809 - accuracy: 0.9663 - val_loss: 0.1131 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.11352 to 0.11305, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 79/500\n",
      "1040/1040 [==============================] - 0s 242us/step - loss: 0.0805 - accuracy: 0.9663 - val_loss: 0.1126 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.11305 to 0.11262, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 80/500\n",
      "1040/1040 [==============================] - 0s 284us/step - loss: 0.0800 - accuracy: 0.9673 - val_loss: 0.1121 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.11262 to 0.11213, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 81/500\n",
      "1040/1040 [==============================] - 0s 230us/step - loss: 0.0796 - accuracy: 0.9673 - val_loss: 0.1117 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.11213 to 0.11166, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 82/500\n",
      "1040/1040 [==============================] - 0s 221us/step - loss: 0.0791 - accuracy: 0.9673 - val_loss: 0.1112 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.11166 to 0.11123, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 83/500\n",
      "1040/1040 [==============================] - 0s 215us/step - loss: 0.0786 - accuracy: 0.9673 - val_loss: 0.1108 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.11123 to 0.11075, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 84/500\n",
      "1040/1040 [==============================] - 0s 223us/step - loss: 0.0782 - accuracy: 0.9673 - val_loss: 0.1102 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.11075 to 0.11024, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 85/500\n",
      "1040/1040 [==============================] - 0s 251us/step - loss: 0.0777 - accuracy: 0.9673 - val_loss: 0.1097 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.11024 to 0.10974, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 86/500\n",
      "1040/1040 [==============================] - 0s 343us/step - loss: 0.0772 - accuracy: 0.9673 - val_loss: 0.1093 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.10974 to 0.10927, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 87/500\n",
      "1040/1040 [==============================] - 0s 220us/step - loss: 0.0768 - accuracy: 0.9673 - val_loss: 0.1088 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.10927 to 0.10882, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 88/500\n",
      "1040/1040 [==============================] - 0s 224us/step - loss: 0.0763 - accuracy: 0.9673 - val_loss: 0.1084 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.10882 to 0.10835, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 89/500\n",
      "1040/1040 [==============================] - 0s 326us/step - loss: 0.0758 - accuracy: 0.9683 - val_loss: 0.1079 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.10835 to 0.10788, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 90/500\n",
      "1040/1040 [==============================] - 0s 224us/step - loss: 0.0754 - accuracy: 0.9683 - val_loss: 0.1074 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.10788 to 0.10741, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 91/500\n",
      "1040/1040 [==============================] - 0s 234us/step - loss: 0.0749 - accuracy: 0.9683 - val_loss: 0.1070 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.10741 to 0.10695, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 92/500\n",
      "1040/1040 [==============================] - 0s 241us/step - loss: 0.0745 - accuracy: 0.9683 - val_loss: 0.1065 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.10695 to 0.10651, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 93/500\n",
      "1040/1040 [==============================] - 0s 242us/step - loss: 0.0741 - accuracy: 0.9683 - val_loss: 0.1061 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.10651 to 0.10612, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 94/500\n",
      "1040/1040 [==============================] - 0s 285us/step - loss: 0.0736 - accuracy: 0.9683 - val_loss: 0.1057 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.10612 to 0.10574, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 95/500\n",
      "1040/1040 [==============================] - 0s 251us/step - loss: 0.0732 - accuracy: 0.9692 - val_loss: 0.1054 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.10574 to 0.10535, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 96/500\n",
      "1040/1040 [==============================] - 0s 228us/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.1050 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.10535 to 0.10497, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 97/500\n",
      "1040/1040 [==============================] - 0s 276us/step - loss: 0.0723 - accuracy: 0.9692 - val_loss: 0.1046 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.10497 to 0.10461, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 98/500\n",
      "1040/1040 [==============================] - 0s 212us/step - loss: 0.0719 - accuracy: 0.9692 - val_loss: 0.1042 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.10461 to 0.10425, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 99/500\n",
      "1040/1040 [==============================] - 0s 219us/step - loss: 0.0715 - accuracy: 0.9692 - val_loss: 0.1039 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.10425 to 0.10390, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 100/500\n",
      "1040/1040 [==============================] - 0s 247us/step - loss: 0.0711 - accuracy: 0.9702 - val_loss: 0.1036 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.10390 to 0.10356, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 101/500\n",
      "1040/1040 [==============================] - 0s 241us/step - loss: 0.0706 - accuracy: 0.9702 - val_loss: 0.1032 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.10356 to 0.10323, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 102/500\n",
      "1040/1040 [==============================] - 0s 298us/step - loss: 0.0702 - accuracy: 0.9702 - val_loss: 0.1029 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.10323 to 0.10290, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 103/500\n",
      "1040/1040 [==============================] - 0s 230us/step - loss: 0.0698 - accuracy: 0.9712 - val_loss: 0.1026 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.10290 to 0.10256, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 104/500\n",
      "1040/1040 [==============================] - 0s 190us/step - loss: 0.0694 - accuracy: 0.9721 - val_loss: 0.1022 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.10256 to 0.10221, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 105/500\n",
      "1040/1040 [==============================] - 0s 291us/step - loss: 0.0690 - accuracy: 0.9721 - val_loss: 0.1019 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.10221 to 0.10185, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 106/500\n",
      "1040/1040 [==============================] - 0s 225us/step - loss: 0.0686 - accuracy: 0.9721 - val_loss: 0.1015 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.10185 to 0.10151, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 107/500\n",
      "1040/1040 [==============================] - 0s 200us/step - loss: 0.0682 - accuracy: 0.9721 - val_loss: 0.1012 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.10151 to 0.10120, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 108/500\n",
      "1040/1040 [==============================] - 0s 252us/step - loss: 0.0678 - accuracy: 0.9731 - val_loss: 0.1009 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.10120 to 0.10091, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 109/500\n",
      "1040/1040 [==============================] - 0s 223us/step - loss: 0.0674 - accuracy: 0.9740 - val_loss: 0.1006 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.10091 to 0.10059, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 110/500\n",
      "1040/1040 [==============================] - 0s 222us/step - loss: 0.0670 - accuracy: 0.9740 - val_loss: 0.1003 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.10059 to 0.10027, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 111/500\n",
      "1040/1040 [==============================] - 0s 274us/step - loss: 0.0666 - accuracy: 0.9740 - val_loss: 0.0999 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.10027 to 0.09995, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 112/500\n",
      "1040/1040 [==============================] - 0s 236us/step - loss: 0.0662 - accuracy: 0.9740 - val_loss: 0.0996 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.09995 to 0.09964, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 113/500\n",
      "1040/1040 [==============================] - 0s 153us/step - loss: 0.0658 - accuracy: 0.9740 - val_loss: 0.0993 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.09964 to 0.09932, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 114/500\n",
      "1040/1040 [==============================] - 0s 262us/step - loss: 0.0654 - accuracy: 0.9740 - val_loss: 0.0990 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.09932 to 0.09902, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 115/500\n",
      "1040/1040 [==============================] - 0s 197us/step - loss: 0.0650 - accuracy: 0.9740 - val_loss: 0.0987 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.09902 to 0.09870, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 116/500\n",
      "1040/1040 [==============================] - 0s 215us/step - loss: 0.0646 - accuracy: 0.9750 - val_loss: 0.0984 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.09870 to 0.09838, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 117/500\n",
      "1040/1040 [==============================] - 0s 224us/step - loss: 0.0642 - accuracy: 0.9760 - val_loss: 0.0981 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.09838 to 0.09810, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 118/500\n",
      "1040/1040 [==============================] - 0s 211us/step - loss: 0.0638 - accuracy: 0.9760 - val_loss: 0.0978 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.09810 to 0.09777, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 119/500\n",
      "1040/1040 [==============================] - 0s 217us/step - loss: 0.0634 - accuracy: 0.9769 - val_loss: 0.0975 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.09777 to 0.09748, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 120/500\n",
      "1040/1040 [==============================] - 0s 228us/step - loss: 0.0631 - accuracy: 0.9779 - val_loss: 0.0972 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.09748 to 0.09719, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 121/500\n",
      "1040/1040 [==============================] - 0s 224us/step - loss: 0.0627 - accuracy: 0.9779 - val_loss: 0.0969 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.09719 to 0.09692, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 122/500\n",
      "1040/1040 [==============================] - 0s 216us/step - loss: 0.0623 - accuracy: 0.9779 - val_loss: 0.0967 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.09692 to 0.09668, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 123/500\n",
      "1040/1040 [==============================] - 0s 193us/step - loss: 0.0619 - accuracy: 0.9779 - val_loss: 0.0964 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.09668 to 0.09643, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 124/500\n",
      "1040/1040 [==============================] - 0s 242us/step - loss: 0.0615 - accuracy: 0.9779 - val_loss: 0.0962 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.09643 to 0.09617, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 125/500\n",
      "1040/1040 [==============================] - 0s 187us/step - loss: 0.0612 - accuracy: 0.9779 - val_loss: 0.0959 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.09617 to 0.09589, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 126/500\n",
      "1040/1040 [==============================] - 0s 241us/step - loss: 0.0608 - accuracy: 0.9788 - val_loss: 0.0956 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.09589 to 0.09562, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 127/500\n",
      "1040/1040 [==============================] - 0s 187us/step - loss: 0.0604 - accuracy: 0.9798 - val_loss: 0.0953 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.09562 to 0.09534, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 128/500\n",
      "1040/1040 [==============================] - 0s 297us/step - loss: 0.0600 - accuracy: 0.9808 - val_loss: 0.0950 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.09534 to 0.09505, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 129/500\n",
      "1040/1040 [==============================] - 0s 225us/step - loss: 0.0596 - accuracy: 0.9808 - val_loss: 0.0947 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.09505 to 0.09472, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 130/500\n",
      "1040/1040 [==============================] - 0s 211us/step - loss: 0.0593 - accuracy: 0.9808 - val_loss: 0.0944 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.09472 to 0.09440, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 131/500\n",
      "1040/1040 [==============================] - 0s 210us/step - loss: 0.0589 - accuracy: 0.9808 - val_loss: 0.0941 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.09440 to 0.09412, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 132/500\n",
      "1040/1040 [==============================] - 0s 178us/step - loss: 0.0585 - accuracy: 0.9808 - val_loss: 0.0938 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.09412 to 0.09383, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 133/500\n",
      "1040/1040 [==============================] - 0s 302us/step - loss: 0.0581 - accuracy: 0.9808 - val_loss: 0.0936 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.09383 to 0.09356, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 134/500\n",
      "1040/1040 [==============================] - 0s 210us/step - loss: 0.0577 - accuracy: 0.9808 - val_loss: 0.0932 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.09356 to 0.09321, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 135/500\n",
      "1040/1040 [==============================] - 0s 223us/step - loss: 0.0573 - accuracy: 0.9808 - val_loss: 0.0929 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.09321 to 0.09289, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 136/500\n",
      "1040/1040 [==============================] - 0s 242us/step - loss: 0.0570 - accuracy: 0.9808 - val_loss: 0.0926 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.09289 to 0.09260, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 137/500\n",
      "1040/1040 [==============================] - 0s 181us/step - loss: 0.0566 - accuracy: 0.9808 - val_loss: 0.0924 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.09260 to 0.09235, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 138/500\n",
      "1040/1040 [==============================] - 0s 269us/step - loss: 0.0562 - accuracy: 0.9817 - val_loss: 0.0921 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.09235 to 0.09207, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 139/500\n",
      "1040/1040 [==============================] - 0s 239us/step - loss: 0.0558 - accuracy: 0.9817 - val_loss: 0.0917 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.09207 to 0.09175, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 140/500\n",
      "1040/1040 [==============================] - 0s 200us/step - loss: 0.0554 - accuracy: 0.9827 - val_loss: 0.0915 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.09175 to 0.09148, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 141/500\n",
      "1040/1040 [==============================] - 0s 222us/step - loss: 0.0551 - accuracy: 0.9827 - val_loss: 0.0912 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.09148 to 0.09124, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 142/500\n",
      "1040/1040 [==============================] - 0s 226us/step - loss: 0.0547 - accuracy: 0.9827 - val_loss: 0.0910 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.09124 to 0.09100, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 143/500\n",
      "1040/1040 [==============================] - 0s 282us/step - loss: 0.0543 - accuracy: 0.9837 - val_loss: 0.0907 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.09100 to 0.09073, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 144/500\n",
      "1040/1040 [==============================] - 0s 220us/step - loss: 0.0540 - accuracy: 0.9837 - val_loss: 0.0905 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.09073 to 0.09048, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 145/500\n",
      "1040/1040 [==============================] - 0s 214us/step - loss: 0.0536 - accuracy: 0.9837 - val_loss: 0.0902 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.09048 to 0.09024, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 146/500\n",
      "1040/1040 [==============================] - 0s 378us/step - loss: 0.0533 - accuracy: 0.9846 - val_loss: 0.0900 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.09024 to 0.08999, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 147/500\n",
      "1040/1040 [==============================] - 0s 225us/step - loss: 0.0530 - accuracy: 0.9856 - val_loss: 0.0897 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.08999 to 0.08970, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 148/500\n",
      "1040/1040 [==============================] - 0s 239us/step - loss: 0.0526 - accuracy: 0.9865 - val_loss: 0.0894 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.08970 to 0.08942, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 149/500\n",
      "1040/1040 [==============================] - 0s 259us/step - loss: 0.0523 - accuracy: 0.9865 - val_loss: 0.0891 - val_accuracy: 0.9691\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.08942 to 0.08912, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 150/500\n",
      "1040/1040 [==============================] - 0s 192us/step - loss: 0.0520 - accuracy: 0.9865 - val_loss: 0.0889 - val_accuracy: 0.9691\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.08912 to 0.08887, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 151/500\n",
      "1040/1040 [==============================] - 0s 243us/step - loss: 0.0516 - accuracy: 0.9865 - val_loss: 0.0886 - val_accuracy: 0.9691\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.08887 to 0.08862, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 152/500\n",
      "1040/1040 [==============================] - 0s 268us/step - loss: 0.0513 - accuracy: 0.9865 - val_loss: 0.0884 - val_accuracy: 0.9691\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.08862 to 0.08836, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 153/500\n",
      "1040/1040 [==============================] - 0s 252us/step - loss: 0.0509 - accuracy: 0.9865 - val_loss: 0.0881 - val_accuracy: 0.9691\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.08836 to 0.08811, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 154/500\n",
      "1040/1040 [==============================] - 0s 221us/step - loss: 0.0506 - accuracy: 0.9865 - val_loss: 0.0878 - val_accuracy: 0.9691\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.08811 to 0.08784, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 155/500\n",
      "1040/1040 [==============================] - 0s 209us/step - loss: 0.0503 - accuracy: 0.9865 - val_loss: 0.0876 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.08784 to 0.08759, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 156/500\n",
      "1040/1040 [==============================] - 0s 252us/step - loss: 0.0500 - accuracy: 0.9865 - val_loss: 0.0874 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.08759 to 0.08739, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 157/500\n",
      "1040/1040 [==============================] - 0s 209us/step - loss: 0.0496 - accuracy: 0.9865 - val_loss: 0.0872 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.08739 to 0.08716, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 158/500\n",
      "1040/1040 [==============================] - 0s 259us/step - loss: 0.0493 - accuracy: 0.9865 - val_loss: 0.0869 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.08716 to 0.08694, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 159/500\n",
      "1040/1040 [==============================] - 0s 231us/step - loss: 0.0490 - accuracy: 0.9865 - val_loss: 0.0867 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.08694 to 0.08670, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 160/500\n",
      "1040/1040 [==============================] - 0s 176us/step - loss: 0.0487 - accuracy: 0.9865 - val_loss: 0.0865 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.08670 to 0.08652, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 161/500\n",
      "1040/1040 [==============================] - 0s 284us/step - loss: 0.0483 - accuracy: 0.9865 - val_loss: 0.0863 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.08652 to 0.08633, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 162/500\n",
      "1040/1040 [==============================] - 0s 219us/step - loss: 0.0480 - accuracy: 0.9865 - val_loss: 0.0861 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.08633 to 0.08614, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 163/500\n",
      "1040/1040 [==============================] - 0s 274us/step - loss: 0.0477 - accuracy: 0.9865 - val_loss: 0.0859 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.08614 to 0.08595, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 164/500\n",
      "1040/1040 [==============================] - 0s 219us/step - loss: 0.0474 - accuracy: 0.9865 - val_loss: 0.0858 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.08595 to 0.08577, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 165/500\n",
      "1040/1040 [==============================] - 0s 211us/step - loss: 0.0471 - accuracy: 0.9865 - val_loss: 0.0856 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.08577 to 0.08558, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 166/500\n",
      "1040/1040 [==============================] - 0s 216us/step - loss: 0.0468 - accuracy: 0.9865 - val_loss: 0.0854 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.08558 to 0.08543, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 167/500\n",
      "1040/1040 [==============================] - 0s 218us/step - loss: 0.0465 - accuracy: 0.9865 - val_loss: 0.0852 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.08543 to 0.08522, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 168/500\n",
      "1040/1040 [==============================] - 0s 255us/step - loss: 0.0462 - accuracy: 0.9865 - val_loss: 0.0850 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.08522 to 0.08500, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 169/500\n",
      "1040/1040 [==============================] - 0s 246us/step - loss: 0.0459 - accuracy: 0.9865 - val_loss: 0.0848 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.08500 to 0.08479, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 170/500\n",
      "1040/1040 [==============================] - 0s 187us/step - loss: 0.0456 - accuracy: 0.9875 - val_loss: 0.0846 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.08479 to 0.08464, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 171/500\n",
      "1040/1040 [==============================] - 0s 270us/step - loss: 0.0453 - accuracy: 0.9875 - val_loss: 0.0845 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.08464 to 0.08449, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 172/500\n",
      "1040/1040 [==============================] - 0s 224us/step - loss: 0.0450 - accuracy: 0.9875 - val_loss: 0.0843 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.08449 to 0.08434, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 173/500\n",
      "1040/1040 [==============================] - 0s 228us/step - loss: 0.0447 - accuracy: 0.9875 - val_loss: 0.0842 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.08434 to 0.08420, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 174/500\n",
      "1040/1040 [==============================] - 0s 249us/step - loss: 0.0444 - accuracy: 0.9875 - val_loss: 0.0841 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.08420 to 0.08407, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 175/500\n",
      "1040/1040 [==============================] - 0s 184us/step - loss: 0.0441 - accuracy: 0.9875 - val_loss: 0.0839 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.08407 to 0.08391, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 176/500\n",
      "1040/1040 [==============================] - 0s 246us/step - loss: 0.0439 - accuracy: 0.9875 - val_loss: 0.0838 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.08391 to 0.08377, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 177/500\n",
      "1040/1040 [==============================] - 0s 235us/step - loss: 0.0436 - accuracy: 0.9885 - val_loss: 0.0836 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.08377 to 0.08361, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 178/500\n",
      "1040/1040 [==============================] - 0s 201us/step - loss: 0.0433 - accuracy: 0.9885 - val_loss: 0.0835 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.08361 to 0.08346, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 179/500\n",
      "1040/1040 [==============================] - 0s 251us/step - loss: 0.0430 - accuracy: 0.9885 - val_loss: 0.0834 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.08346 to 0.08336, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 180/500\n",
      "1040/1040 [==============================] - 0s 259us/step - loss: 0.0427 - accuracy: 0.9885 - val_loss: 0.0832 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.08336 to 0.08319, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 181/500\n",
      "1040/1040 [==============================] - 0s 210us/step - loss: 0.0425 - accuracy: 0.9885 - val_loss: 0.0830 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.08319 to 0.08303, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 182/500\n",
      "1040/1040 [==============================] - 0s 244us/step - loss: 0.0422 - accuracy: 0.9885 - val_loss: 0.0829 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.08303 to 0.08293, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 183/500\n",
      "1040/1040 [==============================] - 0s 224us/step - loss: 0.0419 - accuracy: 0.9885 - val_loss: 0.0828 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.08293 to 0.08281, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 184/500\n",
      "1040/1040 [==============================] - 0s 254us/step - loss: 0.0417 - accuracy: 0.9885 - val_loss: 0.0827 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.08281 to 0.08270, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 185/500\n",
      "1040/1040 [==============================] - 0s 216us/step - loss: 0.0414 - accuracy: 0.9885 - val_loss: 0.0826 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.08270 to 0.08264, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 186/500\n",
      "1040/1040 [==============================] - 0s 267us/step - loss: 0.0411 - accuracy: 0.9885 - val_loss: 0.0825 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.08264 to 0.08250, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 187/500\n",
      "1040/1040 [==============================] - 0s 240us/step - loss: 0.0409 - accuracy: 0.9885 - val_loss: 0.0824 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.08250 to 0.08237, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 188/500\n",
      "1040/1040 [==============================] - 0s 192us/step - loss: 0.0406 - accuracy: 0.9885 - val_loss: 0.0822 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.08237 to 0.08225, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 189/500\n",
      "1040/1040 [==============================] - 0s 261us/step - loss: 0.0403 - accuracy: 0.9894 - val_loss: 0.0821 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.08225 to 0.08210, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 190/500\n",
      "1040/1040 [==============================] - 0s 221us/step - loss: 0.0401 - accuracy: 0.9904 - val_loss: 0.0819 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.08210 to 0.08193, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 191/500\n",
      "1040/1040 [==============================] - 0s 279us/step - loss: 0.0398 - accuracy: 0.9904 - val_loss: 0.0818 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.08193 to 0.08184, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 192/500\n",
      "1040/1040 [==============================] - 0s 221us/step - loss: 0.0396 - accuracy: 0.9904 - val_loss: 0.0817 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.08184 to 0.08167, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 193/500\n",
      "1040/1040 [==============================] - 0s 170us/step - loss: 0.0393 - accuracy: 0.9904 - val_loss: 0.0815 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.08167 to 0.08151, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 194/500\n",
      "1040/1040 [==============================] - 0s 269us/step - loss: 0.0391 - accuracy: 0.9904 - val_loss: 0.0814 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.08151 to 0.08142, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 195/500\n",
      "1040/1040 [==============================] - 0s 221us/step - loss: 0.0388 - accuracy: 0.9904 - val_loss: 0.0813 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.08142 to 0.08126, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 196/500\n",
      "1040/1040 [==============================] - 0s 241us/step - loss: 0.0386 - accuracy: 0.9904 - val_loss: 0.0812 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.08126 to 0.08115, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 197/500\n",
      "1040/1040 [==============================] - 0s 275us/step - loss: 0.0383 - accuracy: 0.9904 - val_loss: 0.0810 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.08115 to 0.08101, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 198/500\n",
      "1040/1040 [==============================] - 0s 194us/step - loss: 0.0381 - accuracy: 0.9904 - val_loss: 0.0809 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.08101 to 0.08091, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 199/500\n",
      "1040/1040 [==============================] - 0s 247us/step - loss: 0.0378 - accuracy: 0.9923 - val_loss: 0.0808 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.08091 to 0.08079, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 200/500\n",
      "1040/1040 [==============================] - 0s 218us/step - loss: 0.0375 - accuracy: 0.9923 - val_loss: 0.0807 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.08079 to 0.08066, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 201/500\n",
      "1040/1040 [==============================] - 0s 241us/step - loss: 0.0373 - accuracy: 0.9923 - val_loss: 0.0805 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.08066 to 0.08055, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 202/500\n",
      "1040/1040 [==============================] - 0s 357us/step - loss: 0.0370 - accuracy: 0.9923 - val_loss: 0.0804 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.08055 to 0.08043, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 203/500\n",
      "1040/1040 [==============================] - 0s 146us/step - loss: 0.0368 - accuracy: 0.9923 - val_loss: 0.0803 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.08043 to 0.08029, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 204/500\n",
      "1040/1040 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.99 - 0s 256us/step - loss: 0.0365 - accuracy: 0.9923 - val_loss: 0.0801 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.08029 to 0.08012, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 205/500\n",
      "1040/1040 [==============================] - 0s 308us/step - loss: 0.0363 - accuracy: 0.9923 - val_loss: 0.0800 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.08012 to 0.08000, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 206/500\n",
      "1040/1040 [==============================] - 0s 250us/step - loss: 0.0360 - accuracy: 0.9923 - val_loss: 0.0799 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.08000 to 0.07994, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 207/500\n",
      "1040/1040 [==============================] - 0s 322us/step - loss: 0.0358 - accuracy: 0.9923 - val_loss: 0.0798 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.07994 to 0.07980, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 208/500\n",
      "1040/1040 [==============================] - 0s 158us/step - loss: 0.0355 - accuracy: 0.9923 - val_loss: 0.0797 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.07980 to 0.07967, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 209/500\n",
      "1040/1040 [==============================] - 0s 219us/step - loss: 0.0353 - accuracy: 0.9923 - val_loss: 0.0796 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.07967 to 0.07961, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 210/500\n",
      "1040/1040 [==============================] - 0s 223us/step - loss: 0.0351 - accuracy: 0.9923 - val_loss: 0.0795 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.07961 to 0.07950, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 211/500\n",
      "1040/1040 [==============================] - 0s 236us/step - loss: 0.0348 - accuracy: 0.9923 - val_loss: 0.0794 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.07950 to 0.07940, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 212/500\n",
      "1040/1040 [==============================] - 0s 226us/step - loss: 0.0346 - accuracy: 0.9933 - val_loss: 0.0792 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.07940 to 0.07923, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 213/500\n",
      "1040/1040 [==============================] - 0s 291us/step - loss: 0.0344 - accuracy: 0.9933 - val_loss: 0.0791 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.07923 to 0.07912, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 214/500\n",
      "1040/1040 [==============================] - 0s 260us/step - loss: 0.0341 - accuracy: 0.9933 - val_loss: 0.0790 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.07912 to 0.07902, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 215/500\n",
      "1040/1040 [==============================] - 0s 255us/step - loss: 0.0339 - accuracy: 0.9933 - val_loss: 0.0789 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.07902 to 0.07894, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 216/500\n",
      "1040/1040 [==============================] - 0s 203us/step - loss: 0.0337 - accuracy: 0.9933 - val_loss: 0.0788 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.07894 to 0.07885, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 217/500\n",
      "1040/1040 [==============================] - 0s 221us/step - loss: 0.0334 - accuracy: 0.9933 - val_loss: 0.0788 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.07885 to 0.07876, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 218/500\n",
      "1040/1040 [==============================] - 0s 326us/step - loss: 0.0332 - accuracy: 0.9933 - val_loss: 0.0787 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.07876 to 0.07865, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 219/500\n",
      "1040/1040 [==============================] - 0s 227us/step - loss: 0.0330 - accuracy: 0.9933 - val_loss: 0.0785 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.07865 to 0.07853, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 220/500\n",
      "1040/1040 [==============================] - 0s 178us/step - loss: 0.0328 - accuracy: 0.9933 - val_loss: 0.0784 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.07853 to 0.07841, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 221/500\n",
      "1040/1040 [==============================] - 0s 277us/step - loss: 0.0325 - accuracy: 0.9933 - val_loss: 0.0783 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.07841 to 0.07831, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 222/500\n",
      "1040/1040 [==============================] - 0s 117us/step - loss: 0.0323 - accuracy: 0.9933 - val_loss: 0.0782 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.07831 to 0.07820, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 223/500\n",
      "1040/1040 [==============================] - 0s 208us/step - loss: 0.0321 - accuracy: 0.9933 - val_loss: 0.0781 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.07820 to 0.07806, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 224/500\n",
      "1040/1040 [==============================] - 0s 290us/step - loss: 0.0319 - accuracy: 0.9933 - val_loss: 0.0779 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.07806 to 0.07793, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 225/500\n",
      "1040/1040 [==============================] - 0s 229us/step - loss: 0.0317 - accuracy: 0.9933 - val_loss: 0.0778 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.07793 to 0.07782, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 226/500\n",
      "1040/1040 [==============================] - 0s 214us/step - loss: 0.0315 - accuracy: 0.9933 - val_loss: 0.0777 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.07782 to 0.07771, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 227/500\n",
      "1040/1040 [==============================] - 0s 233us/step - loss: 0.0313 - accuracy: 0.9933 - val_loss: 0.0776 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.07771 to 0.07758, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 228/500\n",
      "1040/1040 [==============================] - 0s 223us/step - loss: 0.0311 - accuracy: 0.9933 - val_loss: 0.0775 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.07758 to 0.07752, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 229/500\n",
      "1040/1040 [==============================] - 0s 213us/step - loss: 0.0309 - accuracy: 0.9933 - val_loss: 0.0774 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.07752 to 0.07738, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 230/500\n",
      "1040/1040 [==============================] - 0s 227us/step - loss: 0.0307 - accuracy: 0.9942 - val_loss: 0.0773 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.07738 to 0.07731, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 231/500\n",
      "1040/1040 [==============================] - 0s 254us/step - loss: 0.0305 - accuracy: 0.9942 - val_loss: 0.0773 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.07731 to 0.07727, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 232/500\n",
      "1040/1040 [==============================] - 0s 251us/step - loss: 0.0303 - accuracy: 0.9942 - val_loss: 0.0772 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.07727 to 0.07718, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 233/500\n",
      "1040/1040 [==============================] - 0s 269us/step - loss: 0.0301 - accuracy: 0.9942 - val_loss: 0.0770 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.07718 to 0.07704, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 234/500\n",
      "1040/1040 [==============================] - 0s 228us/step - loss: 0.0299 - accuracy: 0.9942 - val_loss: 0.0769 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00234: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.07704 to 0.07694, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 235/500\n",
      "1040/1040 [==============================] - 0s 255us/step - loss: 0.0297 - accuracy: 0.9942 - val_loss: 0.0769 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.07694 to 0.07691, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 236/500\n",
      "1040/1040 [==============================] - 0s 254us/step - loss: 0.0297 - accuracy: 0.9942 - val_loss: 0.0769 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.07691 to 0.07689, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 237/500\n",
      "1040/1040 [==============================] - 0s 208us/step - loss: 0.0296 - accuracy: 0.9942 - val_loss: 0.0769 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.07689 to 0.07686, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 238/500\n",
      "1040/1040 [==============================] - 0s 325us/step - loss: 0.0296 - accuracy: 0.9942 - val_loss: 0.0768 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.07686 to 0.07682, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 239/500\n",
      "1040/1040 [==============================] - 0s 248us/step - loss: 0.0296 - accuracy: 0.9942 - val_loss: 0.0768 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.07682 to 0.07679, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 240/500\n",
      "1040/1040 [==============================] - 0s 182us/step - loss: 0.0296 - accuracy: 0.9942 - val_loss: 0.0768 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.07679 to 0.07677, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 241/500\n",
      "1040/1040 [==============================] - 0s 233us/step - loss: 0.0296 - accuracy: 0.9942 - val_loss: 0.0767 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.07677 to 0.07675, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 242/500\n",
      "1040/1040 [==============================] - 0s 189us/step - loss: 0.0295 - accuracy: 0.9942 - val_loss: 0.0767 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.07675 to 0.07673, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 243/500\n",
      "1040/1040 [==============================] - 0s 268us/step - loss: 0.0295 - accuracy: 0.9942 - val_loss: 0.0767 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.07673 to 0.07671, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 244/500\n",
      "1040/1040 [==============================] - 0s 302us/step - loss: 0.0295 - accuracy: 0.9942 - val_loss: 0.0767 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.07671 to 0.07669, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 245/500\n",
      "1040/1040 [==============================] - 0s 251us/step - loss: 0.0295 - accuracy: 0.9942 - val_loss: 0.0767 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00245: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.07669 to 0.07668, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 246/500\n",
      "1040/1040 [==============================] - 0s 211us/step - loss: 0.0295 - accuracy: 0.9942 - val_loss: 0.0767 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.07668 to 0.07667, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 247/500\n",
      "1040/1040 [==============================] - 0s 201us/step - loss: 0.0295 - accuracy: 0.9942 - val_loss: 0.0767 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.07667 to 0.07666, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 248/500\n",
      "1040/1040 [==============================] - 0s 274us/step - loss: 0.0295 - accuracy: 0.9942 - val_loss: 0.0767 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.07666 to 0.07666, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 249/500\n",
      "1040/1040 [==============================] - 0s 288us/step - loss: 0.0295 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.07666 to 0.07665, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 250/500\n",
      "1040/1040 [==============================] - 0s 165us/step - loss: 0.0295 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00250: val_loss improved from 0.07665 to 0.07664, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 251/500\n",
      "1040/1040 [==============================] - 0s 266us/step - loss: 0.0295 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00251: val_loss improved from 0.07664 to 0.07664, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 252/500\n",
      "1040/1040 [==============================] - 0s 224us/step - loss: 0.0295 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.07664 to 0.07663, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 253/500\n",
      "1040/1040 [==============================] - 0s 201us/step - loss: 0.0295 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.07663 to 0.07663, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 254/500\n",
      "1040/1040 [==============================] - 0s 301us/step - loss: 0.0295 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.07663 to 0.07662, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 255/500\n",
      "1040/1040 [==============================] - 0s 237us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00255: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.07662 to 0.07662, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 256/500\n",
      "1040/1040 [==============================] - 0s 208us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00256: val_loss improved from 0.07662 to 0.07662, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 257/500\n",
      "1040/1040 [==============================] - 0s 230us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00257: val_loss improved from 0.07662 to 0.07661, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 258/500\n",
      "1040/1040 [==============================] - 0s 171us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00258: val_loss improved from 0.07661 to 0.07661, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 259/500\n",
      "1040/1040 [==============================] - 0s 258us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00259: val_loss improved from 0.07661 to 0.07661, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 260/500\n",
      "1040/1040 [==============================] - 0s 307us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00260: val_loss improved from 0.07661 to 0.07661, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 261/500\n",
      "1040/1040 [==============================] - 0s 220us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00261: val_loss improved from 0.07661 to 0.07660, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 262/500\n",
      "1040/1040 [==============================] - 0s 244us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.07660 to 0.07660, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 263/500\n",
      "1040/1040 [==============================] - 0s 218us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.07660 to 0.07660, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 264/500\n",
      "1040/1040 [==============================] - 0s 244us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.07660 to 0.07660, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 265/500\n",
      "1040/1040 [==============================] - 0s 316us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00265: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.07660 to 0.07660, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 266/500\n",
      "1040/1040 [==============================] - 0s 230us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00266: val_loss improved from 0.07660 to 0.07660, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 267/500\n",
      "1040/1040 [==============================] - 0s 223us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.07660 to 0.07659, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 268/500\n",
      "1040/1040 [==============================] - 0s 329us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00268: val_loss improved from 0.07659 to 0.07659, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 269/500\n",
      "1040/1040 [==============================] - 0s 226us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00269: val_loss improved from 0.07659 to 0.07659, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 270/500\n",
      "1040/1040 [==============================] - 0s 329us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00270: val_loss improved from 0.07659 to 0.07659, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 271/500\n",
      "1040/1040 [==============================] - 0s 207us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00271: val_loss improved from 0.07659 to 0.07659, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 272/500\n",
      "1040/1040 [==============================] - 0s 241us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00272: val_loss improved from 0.07659 to 0.07659, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 273/500\n",
      "1040/1040 [==============================] - 0s 274us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00273: val_loss improved from 0.07659 to 0.07659, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 274/500\n",
      "1040/1040 [==============================] - 0s 227us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00274: val_loss improved from 0.07659 to 0.07659, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 275/500\n",
      "1040/1040 [==============================] - 0s 217us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00275: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.07659 to 0.07659, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 276/500\n",
      "1040/1040 [==============================] - 0s 221us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00276: val_loss improved from 0.07659 to 0.07659, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 277/500\n",
      "1040/1040 [==============================] - 0s 231us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00277: val_loss improved from 0.07659 to 0.07659, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 278/500\n",
      "1040/1040 [==============================] - 0s 251us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.07659 to 0.07659, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 279/500\n",
      "1040/1040 [==============================] - 0s 246us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00279: val_loss improved from 0.07659 to 0.07659, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 280/500\n",
      "1040/1040 [==============================] - 0s 212us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.07659 to 0.07659, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 281/500\n",
      "1040/1040 [==============================] - 0s 217us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.07659 to 0.07659, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 282/500\n",
      "1040/1040 [==============================] - 0s 200us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00282: val_loss improved from 0.07659 to 0.07659, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 283/500\n",
      "1040/1040 [==============================] - 0s 242us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00283: val_loss improved from 0.07659 to 0.07659, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 284/500\n",
      "1040/1040 [==============================] - 0s 241us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00284: val_loss improved from 0.07659 to 0.07659, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 285/500\n",
      "1040/1040 [==============================] - 0s 201us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00285: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "\n",
      "Epoch 00285: val_loss improved from 0.07659 to 0.07659, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 286/500\n",
      "1040/1040 [==============================] - 0s 233us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00286: val_loss improved from 0.07659 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 287/500\n",
      "1040/1040 [==============================] - 0s 212us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00287: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 288/500\n",
      "1040/1040 [==============================] - 0s 305us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00288: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 289/500\n",
      "1040/1040 [==============================] - 0s 245us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00289: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 290/500\n",
      "1040/1040 [==============================] - 0s 355us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00290: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 291/500\n",
      "1040/1040 [==============================] - 0s 256us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00291: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 292/500\n",
      "1040/1040 [==============================] - 0s 246us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00292: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 293/500\n",
      "1040/1040 [==============================] - 0s 260us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00293: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 294/500\n",
      "1040/1040 [==============================] - 0s 229us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00294: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 295/500\n",
      "1040/1040 [==============================] - 0s 241us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00295: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "\n",
      "Epoch 00295: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 296/500\n",
      "1040/1040 [==============================] - 0s 247us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00296: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 297/500\n",
      "1040/1040 [==============================] - 0s 224us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00297: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 298/500\n",
      "1040/1040 [==============================] - 0s 271us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00298: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 299/500\n",
      "1040/1040 [==============================] - 0s 258us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00299: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 300/500\n",
      "1040/1040 [==============================] - 0s 216us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 301/500\n",
      "1040/1040 [==============================] - 0s 327us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00301: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 302/500\n",
      "1040/1040 [==============================] - 0s 214us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 303/500\n",
      "1040/1040 [==============================] - 0s 269us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00303: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 304/500\n",
      "1040/1040 [==============================] - 0s 250us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00304: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 305/500\n",
      "1040/1040 [==============================] - 0s 221us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00305: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "\n",
      "Epoch 00305: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 306/500\n",
      "1040/1040 [==============================] - 0s 242us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 307/500\n",
      "1040/1040 [==============================] - 0s 221us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.07658\n",
      "Epoch 308/500\n",
      "1040/1040 [==============================] - 0s 294us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00308: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 309/500\n",
      "1040/1040 [==============================] - 0s 236us/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0766 - val_accuracy: 0.9614\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.07658 to 0.07658, saving model to ./210512_maxpool3/model/Check_residual3_filter4_fold2_0512.h5\n",
      "Epoch 00309: early stopping\n"
     ]
    }
   ],
   "source": [
    "data_x = np.load(savepath+'/npy/train_x_fold2.npy')\n",
    "data_y = np.load(savepath+'/npy/train_y_fold2.npy')\n",
    "# train_x,test_x,val_y,val_y = train_test_split(data_x,data_y, stratify = data_y, train_size=1300, random_state = 25)\n",
    "train_x,val_x,train_y,val_y = train_test_split(data_x,data_y, stratify = data_y, train_size=1040)\n",
    "input_img = Input(shape=(32,32,1))\n",
    "model = base_classification_jw(input_img, filters = 4, scale = 2)\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=savepath+'/model/Check_residual3_filter4_fold2_0512.h5', verbose=1, save_best_only=True, monitor='val_loss')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=10, min_lr=0, min_delta=0.001, verbose=1)\n",
    "earlystopper = EarlyStopping(patience=30, verbose=1, monitor='loss')\n",
    "callbacks_list = [reduce_lr, checkpointer, earlystopper]\n",
    "\n",
    "# results = model.fit(train_x, train_y, batch_size=128, epochs=500, verbose=1, validation_split=0.2, shuffle=False, callbacks=callbacks_list)\n",
    "results = model.fit(train_x, train_y, batch_size=128, epochs=500, verbose=1, validation_data=(val_x,val_y), shuffle=False, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 2s 6ms/step\n",
      "149 14 151 11\n",
      "###########RESULT########\n",
      "AUC:0.974, SEN:0.931, SPEC:0.915, ACC:0.923, PREC:149.0\n",
      "TP:149, FP:14. TN:151, FN:11\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "cknum = 2\n",
    "test_model = load_model(savepath+'/model/Check_residual3_filter4_fold{}_0512.h5'.format(str(cknum)))\n",
    "test_x= np.load(savepath+'/npy/test_x_fold{}.npy'.format(str(cknum)))\n",
    "test_y= np.load(savepath+'/npy/test_y_fold{}.npy'.format(str(cknum)))\n",
    "test_result= test_model.predict(test_x, batch_size=128, verbose=1)\n",
    "np.save(savepath+'/npy/Check_test_result_fold{}.npy'.format(str(cknum)), test_result)\n",
    "fpr_res,tpr_res,_=roc_curve(test_y,test_result)\n",
    "auc_res=auc(fpr_res, tpr_res)\n",
    "#     print(\"loss: %.2f, : %.3f  \" %(score[0], score[1]))\n",
    "res_tp, res_fp, res_tn, res_fn, res_sensitivity, res_specificity, acc, prec= calculate_performance(test_result, test_y)\n",
    "\n",
    "performanceList = [auc_res, res_sensitivity, res_specificity, acc]\n",
    "for a in range(4):\n",
    "    performances[i][a] = performanceList[a]\n",
    "\n",
    "#     interp_tpr = np.interp(mean_fpr, _, _)\n",
    "#     interp_tpr[0] = 0.0\n",
    "#     tprs.append(interp_tpr)\n",
    "#     np.save('./210512/npy/performanceList_fold{}.npy'.format(str(i)), performances)\n",
    "print('###########RESULT########')\n",
    "print('AUC:{}, SEN:{}, SPEC:{}, ACC:{}, PREC:{}'.format(round(auc_res,3),round(res_sensitivity,3),round(res_specificity,3),round(acc,3),round(prec,3)))\n",
    "print('TP:{}, FP:{}. TN:{}, FN:{}'.format(res_tp, res_fp, res_tn, res_fn))\n",
    "print('#########################')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9999869 ]\n",
      " [0.99959576]\n",
      " [0.99994624]\n",
      " [0.9999691 ]\n",
      " [0.999501  ]\n",
      " [0.99930984]\n",
      " [0.9998659 ]\n",
      " [0.999858  ]\n",
      " [0.99731463]\n",
      " [0.9999136 ]\n",
      " [0.99833953]\n",
      " [0.99961674]\n",
      " [0.9996518 ]\n",
      " [0.9096571 ]\n",
      " [0.9994147 ]\n",
      " [0.99616784]\n",
      " [0.99957055]\n",
      " [0.99980974]\n",
      " [0.9999469 ]\n",
      " [0.99958754]\n",
      " [0.99999595]\n",
      " [0.9996172 ]\n",
      " [0.9999841 ]\n",
      " [0.99999905]\n",
      " [0.9999962 ]\n",
      " [0.99925053]\n",
      " [0.99989355]\n",
      " [0.99996257]\n",
      " [1.        ]\n",
      " [0.9999984 ]\n",
      " [0.99999654]\n",
      " [0.9999059 ]\n",
      " [0.99999106]\n",
      " [0.9999883 ]\n",
      " [0.9998714 ]\n",
      " [0.99992007]\n",
      " [0.99757457]\n",
      " [0.9997965 ]\n",
      " [0.99993545]\n",
      " [0.9996078 ]\n",
      " [0.9999801 ]\n",
      " [0.99996257]\n",
      " [0.9998597 ]\n",
      " [0.9999454 ]\n",
      " [0.99967587]\n",
      " [0.99974585]\n",
      " [0.9999724 ]\n",
      " [0.99998724]\n",
      " [0.9998498 ]\n",
      " [0.9999635 ]\n",
      " [0.9985348 ]\n",
      " [0.99986833]\n",
      " [0.9999944 ]\n",
      " [0.9999957 ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [0.9999994 ]\n",
      " [0.99999887]\n",
      " [0.9996622 ]\n",
      " [0.9999901 ]\n",
      " [0.9999881 ]\n",
      " [0.9999879 ]\n",
      " [0.9999871 ]\n",
      " [0.999969  ]\n",
      " [0.9999974 ]\n",
      " [0.99999404]\n",
      " [0.9999993 ]\n",
      " [0.9999363 ]\n",
      " [0.9999872 ]\n",
      " [0.99543816]\n",
      " [0.9998066 ]\n",
      " [0.9990057 ]\n",
      " [0.99999106]\n",
      " [0.9995928 ]\n",
      " [0.9797224 ]\n",
      " [0.9981595 ]\n",
      " [0.9999056 ]\n",
      " [0.99934113]\n",
      " [0.9999522 ]\n",
      " [0.9995282 ]\n",
      " [0.9999901 ]\n",
      " [0.99989307]\n",
      " [0.9998264 ]\n",
      " [0.99998474]\n",
      " [0.99967754]\n",
      " [0.9998944 ]\n",
      " [0.99997556]\n",
      " [0.9999988 ]\n",
      " [0.9999895 ]\n",
      " [0.99973   ]\n",
      " [0.99959743]\n",
      " [0.9999421 ]\n",
      " [0.9952128 ]\n",
      " [0.99936193]\n",
      " [0.9993117 ]\n",
      " [0.9997973 ]\n",
      " [0.9997361 ]\n",
      " [0.99988973]\n",
      " [0.9999149 ]\n",
      " [0.9945041 ]\n",
      " [0.9932054 ]\n",
      " [0.9995388 ]\n",
      " [0.998796  ]\n",
      " [0.9999787 ]\n",
      " [0.999987  ]\n",
      " [0.9999595 ]\n",
      " [0.9999918 ]\n",
      " [0.998472  ]\n",
      " [0.99975955]\n",
      " [0.99997205]\n",
      " [0.99957895]\n",
      " [0.9984876 ]\n",
      " [0.99998647]\n",
      " [0.99998635]\n",
      " [0.99998045]\n",
      " [0.9999819 ]\n",
      " [0.99948454]\n",
      " [0.9999447 ]\n",
      " [0.99905825]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9991884 ]\n",
      " [0.9997146 ]\n",
      " [0.99669826]\n",
      " [0.99990404]\n",
      " [0.9930264 ]\n",
      " [0.9995302 ]\n",
      " [0.9999993 ]\n",
      " [0.99999976]\n",
      " [0.9999983 ]\n",
      " [0.99999124]\n",
      " [0.9996529 ]\n",
      " [0.9984654 ]\n",
      " [0.9988595 ]\n",
      " [0.99898875]\n",
      " [0.99795824]\n",
      " [0.9904523 ]\n",
      " [0.998702  ]\n",
      " [0.99980116]\n",
      " [0.99159503]\n",
      " [0.9997777 ]\n",
      " [0.9997172 ]\n",
      " [0.9999655 ]\n",
      " [0.9999145 ]\n",
      " [0.999981  ]\n",
      " [0.9999993 ]\n",
      " [0.99999905]\n",
      " [0.9999986 ]\n",
      " [0.99997497]\n",
      " [0.9999659 ]\n",
      " [0.9999963 ]\n",
      " [0.99760526]\n",
      " [0.99988794]\n",
      " [0.9998932 ]\n",
      " [0.9999607 ]\n",
      " [0.9999356 ]\n",
      " [0.999992  ]\n",
      " [0.9998334 ]\n",
      " [0.99951726]\n",
      " [0.99985117]\n",
      " [0.9996793 ]\n",
      " [0.999123  ]\n",
      " [0.99966335]\n",
      " [0.99999404]\n",
      " [0.999949  ]\n",
      " [0.99986637]\n",
      " [0.9986583 ]\n",
      " [0.99982697]\n",
      " [0.99964106]\n",
      " [0.99999094]\n",
      " [0.9999809 ]\n",
      " [0.9998907 ]\n",
      " [0.99990183]\n",
      " [0.99999726]\n",
      " [0.999989  ]\n",
      " [0.9999278 ]\n",
      " [0.99999905]\n",
      " [0.9997748 ]\n",
      " [0.8566812 ]\n",
      " [0.99999607]\n",
      " [0.99998224]\n",
      " [0.9996201 ]\n",
      " [0.9514519 ]\n",
      " [0.9611634 ]\n",
      " [0.99925566]\n",
      " [0.9952997 ]\n",
      " [0.9961673 ]\n",
      " [0.9987217 ]\n",
      " [0.9997335 ]\n",
      " [0.99999845]\n",
      " [0.99995327]\n",
      " [0.9999967 ]\n",
      " [0.9989142 ]\n",
      " [0.999009  ]\n",
      " [0.999645  ]\n",
      " [0.9992939 ]\n",
      " [0.9999229 ]\n",
      " [0.99884737]\n",
      " [0.99978197]\n",
      " [0.99910975]\n",
      " [0.99931204]\n",
      " [0.9999478 ]\n",
      " [0.99996376]\n",
      " [0.9980733 ]\n",
      " [0.9992118 ]\n",
      " [0.9999987 ]\n",
      " [0.9999903 ]\n",
      " [0.9999002 ]\n",
      " [0.999998  ]\n",
      " [0.9999566 ]\n",
      " [0.9999844 ]\n",
      " [0.99998176]\n",
      " [0.99998116]\n",
      " [0.99973464]\n",
      " [0.99977547]\n",
      " [0.99906826]\n",
      " [0.9999599 ]\n",
      " [0.99985284]\n",
      " [0.999899  ]\n",
      " [0.9999614 ]\n",
      " [0.99982464]\n",
      " [0.99994373]\n",
      " [0.999946  ]\n",
      " [0.99995244]\n",
      " [0.9995971 ]\n",
      " [0.999893  ]\n",
      " [0.99986273]\n",
      " [0.9998642 ]\n",
      " [0.99995005]\n",
      " [0.9999536 ]\n",
      " [0.9999756 ]\n",
      " [0.99974144]\n",
      " [0.99992216]\n",
      " [0.99999416]\n",
      " [0.9999794 ]\n",
      " [0.9999097 ]\n",
      " [0.99998   ]\n",
      " [0.99997896]\n",
      " [0.9999903 ]\n",
      " [0.99996144]\n",
      " [0.999984  ]\n",
      " [0.9999987 ]\n",
      " [0.9999974 ]\n",
      " [0.99989843]\n",
      " [0.99999964]\n",
      " [0.99999976]\n",
      " [0.9999621 ]\n",
      " [0.99999726]\n",
      " [0.999996  ]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.99994326]\n",
      " [0.99996   ]\n",
      " [0.9990196 ]\n",
      " [0.9996701 ]\n",
      " [0.99921954]\n",
      " [0.9980761 ]\n",
      " [0.9968609 ]\n",
      " [0.99996513]\n",
      " [0.9996487 ]\n",
      " [0.99989825]\n",
      " [0.9997224 ]\n",
      " [0.99997115]\n",
      " [0.99995047]\n",
      " [0.99971104]\n",
      " [0.9995927 ]\n",
      " [0.99993694]\n",
      " [0.99999833]\n",
      " [0.9999346 ]\n",
      " [0.9999665 ]\n",
      " [0.9999867 ]\n",
      " [0.9998189 ]\n",
      " [0.99894255]\n",
      " [0.9999274 ]\n",
      " [0.99993205]\n",
      " [0.99997735]\n",
      " [0.99986434]\n",
      " [0.9999999 ]\n",
      " [0.9999599 ]\n",
      " [0.99879444]\n",
      " [0.99998754]\n",
      " [0.99997926]\n",
      " [0.99930304]\n",
      " [0.9999694 ]\n",
      " [0.99978054]\n",
      " [0.99999547]\n",
      " [0.99999654]\n",
      " [0.9999707 ]\n",
      " [0.99999905]\n",
      " [0.99968106]\n",
      " [0.9999947 ]\n",
      " [0.999938  ]\n",
      " [0.9999175 ]\n",
      " [0.9998894 ]\n",
      " [0.99999976]\n",
      " [0.99999547]\n",
      " [0.9999997 ]\n",
      " [0.99999875]\n",
      " [0.9999894 ]\n",
      " [0.9999897 ]\n",
      " [0.99989736]\n",
      " [0.9999944 ]\n",
      " [0.99999666]\n",
      " [0.9999967 ]\n",
      " [0.9999934 ]\n",
      " [0.9999986 ]\n",
      " [0.9999993 ]\n",
      " [0.99999833]\n",
      " [0.9999714 ]\n",
      " [0.9997935 ]\n",
      " [0.99952614]\n",
      " [0.9995827 ]\n",
      " [0.9999565 ]\n",
      " [0.9995654 ]\n",
      " [0.9937936 ]\n",
      " [0.99998784]\n",
      " [0.9999997 ]\n",
      " [0.9998354 ]\n",
      " [0.99873585]\n",
      " [0.9997381 ]\n",
      " [0.998255  ]\n",
      " [0.9999931 ]\n",
      " [0.9998579 ]\n",
      " [0.9999893 ]\n",
      " [0.9987233 ]]\n",
      "163 0 0 162\n",
      "###########RESULT########\n",
      "AUC:0.553, SEN:0.502, SPEC:1.0, ACC:0.502, PREC:163.0\n",
      "TP:163, FP:0. TN:0, FN:162\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "fpr_res,tpr_res,_=roc_curve(test_y,test_result)\n",
    "print(test_result)\n",
    "auc_res=auc(fpr_res, tpr_res)\n",
    "#     print(\"loss: %.2f, : %.3f  \" %(score[0], score[1]))\n",
    "res_tp, res_fp, res_tn, res_fn, res_sensitivity, res_specificity, acc, prec= calculate_performance(test_result, test_y)\n",
    "\n",
    "performanceList = [auc_res, res_sensitivity, res_specificity, acc]\n",
    "for a in range(4):\n",
    "    performances[i][a] = performanceList[a]\n",
    "\n",
    "print('###########RESULT########')\n",
    "print('AUC:{}, SEN:{}, SPEC:{}, ACC:{}, PREC:{}'.format(round(auc_res,3),round(res_sensitivity,3),round(res_specificity,3),round(acc,3),round(prec,3)))\n",
    "print('TP:{}, FP:{}. TN:{}, FN:{}'.format(res_tp, res_fp, res_tn, res_fn))\n",
    "print('#########################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./210512/npy/performanceList.npy'.format(str(i)), performances)\n",
    "\n",
    "all_perform = pd.DataFrame(performances)\n",
    "all_perform.to_csv('./210512/results/all_perform.csv',columns = ['AUC', 'SEN', 'SPEC', 'ACC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 0s 29us/step\n",
      "###########RESULT########\n",
      "AUC:0.95, SEN:0.931, SPEC:0.793, ACC:0.849, PREC:0.753\n",
      "TP:122, FP:40. TN:153, FN:9\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "test_result= test_model.predict(test_x, batch_size=128, verbose=1)\n",
    "fpr_res,tpr_res,_=roc_curve(test_y,test_result)\n",
    "auc_res=auc(fpr_res, tpr_res)\n",
    "#     print(\"loss: %.2f, : %.3f  \" %(score[0], score[1]))\n",
    "res_tp, res_fp, res_tn, res_fn, res_sensitivity, res_specificity, acc, prec= calculate_performance(test_result, test_y)\n",
    "\n",
    "print('###########RESULT########')\n",
    "print('AUC:{}, SEN:{}, SPEC:{}, ACC:{}, PREC:{}'.format(round(auc_res,3),round(res_sensitivity,3),round(res_specificity,3),round(acc,3),round(prec,3)))\n",
    "print('TP:{}, FP:{}. TN:{}, FN:{}'.format(res_tp, res_fp, res_tn, res_fn))\n",
    "print('#########################')\n",
    "i=+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04585224]\n",
      " [0.08142057]\n",
      " [0.04620174]\n",
      " [0.05059871]\n",
      " [0.06361416]\n",
      " [0.06568268]\n",
      " [0.08683297]\n",
      " [0.10264271]\n",
      " [0.01083297]\n",
      " [0.13174912]\n",
      " [0.05577385]\n",
      " [0.07661471]\n",
      " [0.01543257]\n",
      " [0.01388636]\n",
      " [0.02048415]\n",
      " [0.32580173]\n",
      " [0.07358596]\n",
      " [0.0502499 ]\n",
      " [0.04689789]\n",
      " [0.13231772]\n",
      " [0.293616  ]\n",
      " [0.09583902]\n",
      " [0.07739753]\n",
      " [0.03675097]\n",
      " [0.6848077 ]\n",
      " [0.2552364 ]\n",
      " [0.07375526]\n",
      " [0.02188268]\n",
      " [0.14345899]\n",
      " [0.15111724]\n",
      " [0.07153687]\n",
      " [0.20676428]\n",
      " [0.13148901]\n",
      " [0.06644467]\n",
      " [0.06767535]\n",
      " [0.08709502]\n",
      " [0.11061043]\n",
      " [0.09251931]\n",
      " [0.04429752]\n",
      " [0.06953326]\n",
      " [0.17437395]\n",
      " [0.06012797]\n",
      " [0.13633728]\n",
      " [0.28120077]\n",
      " [0.7549337 ]\n",
      " [0.2826321 ]\n",
      " [0.09157735]\n",
      " [0.07570335]\n",
      " [0.22305301]\n",
      " [0.05381519]\n",
      " [0.1612123 ]\n",
      " [0.07818568]\n",
      " [0.17686266]\n",
      " [0.22221968]\n",
      " [0.07077688]\n",
      " [0.07095507]\n",
      " [0.20233962]\n",
      " [0.07852384]\n",
      " [0.03948566]\n",
      " [0.05530366]\n",
      " [0.01457918]\n",
      " [0.0611915 ]\n",
      " [0.34116894]\n",
      " [0.8074063 ]\n",
      " [0.76968   ]\n",
      " [0.03116155]\n",
      " [0.11389956]\n",
      " [0.02769816]\n",
      " [0.0381847 ]\n",
      " [0.1099675 ]\n",
      " [0.07078376]\n",
      " [0.03052995]\n",
      " [0.01281568]\n",
      " [0.02692646]\n",
      " [0.25689858]\n",
      " [0.08843756]\n",
      " [0.07588631]\n",
      " [0.14876476]\n",
      " [0.3588823 ]\n",
      " [0.11788344]\n",
      " [0.5414401 ]\n",
      " [0.03785095]\n",
      " [0.07485497]\n",
      " [0.05719608]\n",
      " [0.01831555]\n",
      " [0.01179942]\n",
      " [0.04944289]\n",
      " [0.09846574]\n",
      " [0.30322608]\n",
      " [0.5647507 ]\n",
      " [0.07069385]\n",
      " [0.05898952]\n",
      " [0.10330653]\n",
      " [0.03386962]\n",
      " [0.01151431]\n",
      " [0.00471753]\n",
      " [0.13662589]\n",
      " [0.4444648 ]\n",
      " [0.35921955]\n",
      " [0.43210956]\n",
      " [0.1372956 ]\n",
      " [0.12969357]\n",
      " [0.23861143]\n",
      " [0.33694363]\n",
      " [0.28541392]\n",
      " [0.07855937]\n",
      " [0.06941253]\n",
      " [0.23642653]\n",
      " [0.12906823]\n",
      " [0.4145582 ]\n",
      " [0.76964474]\n",
      " [0.40962034]\n",
      " [0.10008526]\n",
      " [0.43599543]\n",
      " [0.02719662]\n",
      " [0.01902914]\n",
      " [0.29813057]\n",
      " [0.08490571]\n",
      " [0.00562924]\n",
      " [0.04659322]\n",
      " [0.0132497 ]\n",
      " [0.01696986]\n",
      " [0.0263232 ]\n",
      " [0.04740816]\n",
      " [0.14904502]\n",
      " [0.14247555]\n",
      " [0.45542368]\n",
      " [0.31936345]\n",
      " [0.13782665]\n",
      " [0.37918508]\n",
      " [0.16953513]\n",
      " [0.25212312]\n",
      " [0.1161961 ]\n",
      " [0.12040415]\n",
      " [0.13422662]\n",
      " [0.6138927 ]\n",
      " [0.2652617 ]\n",
      " [0.6662878 ]\n",
      " [0.07912311]\n",
      " [0.1051304 ]\n",
      " [0.08576068]\n",
      " [0.08161417]\n",
      " [0.08875167]\n",
      " [0.11049709]\n",
      " [0.11421689]\n",
      " [0.02098846]\n",
      " [0.066448  ]\n",
      " [0.05381605]\n",
      " [0.2246654 ]\n",
      " [0.0838626 ]\n",
      " [0.10799605]\n",
      " [0.09224233]\n",
      " [0.15057796]\n",
      " [0.01403111]\n",
      " [0.03643957]\n",
      " [0.1033127 ]\n",
      " [0.19228673]\n",
      " [0.14256805]\n",
      " [0.2543817 ]\n",
      " [0.15967208]\n",
      " [0.22010416]\n",
      " [0.09612703]\n",
      " [0.7373867 ]\n",
      " [0.90323013]\n",
      " [0.69690704]\n",
      " [0.9551693 ]\n",
      " [0.50356674]\n",
      " [0.7536328 ]\n",
      " [0.3663612 ]\n",
      " [0.6610267 ]\n",
      " [0.43144342]\n",
      " [0.5601765 ]\n",
      " [0.6832157 ]\n",
      " [0.46610978]\n",
      " [0.52560127]\n",
      " [0.68410087]\n",
      " [0.5659513 ]\n",
      " [0.38300502]\n",
      " [0.565784  ]\n",
      " [0.65282327]\n",
      " [0.54681796]\n",
      " [0.56115586]\n",
      " [0.7237379 ]\n",
      " [0.7505541 ]\n",
      " [0.53172827]\n",
      " [0.66750324]\n",
      " [0.4904546 ]\n",
      " [0.41395965]\n",
      " [0.53227806]\n",
      " [0.39984185]\n",
      " [0.5031531 ]\n",
      " [0.48348826]\n",
      " [0.6534432 ]\n",
      " [0.6626208 ]\n",
      " [0.648144  ]\n",
      " [0.38426137]\n",
      " [0.4592289 ]\n",
      " [0.5263451 ]\n",
      " [0.5167649 ]\n",
      " [0.6886922 ]\n",
      " [0.57137203]\n",
      " [0.5928765 ]\n",
      " [0.65977424]\n",
      " [0.9663687 ]\n",
      " [0.75588536]\n",
      " [0.9838816 ]\n",
      " [0.9927522 ]\n",
      " [0.9912733 ]\n",
      " [0.99339986]\n",
      " [0.8798431 ]\n",
      " [0.50850666]\n",
      " [0.39378312]\n",
      " [0.16780648]\n",
      " [0.24818128]\n",
      " [0.858055  ]\n",
      " [0.98538995]\n",
      " [0.4674278 ]\n",
      " [0.32697207]\n",
      " [0.9968046 ]\n",
      " [0.99425185]\n",
      " [0.9928397 ]\n",
      " [0.8851098 ]\n",
      " [0.99294484]\n",
      " [0.94318163]\n",
      " [0.859586  ]\n",
      " [0.84690845]\n",
      " [0.9101776 ]\n",
      " [0.9593706 ]\n",
      " [0.9820266 ]\n",
      " [0.99697614]\n",
      " [0.99053144]\n",
      " [0.9921559 ]\n",
      " [0.9850831 ]\n",
      " [0.6465403 ]\n",
      " [0.8686774 ]\n",
      " [0.9929975 ]\n",
      " [0.6475397 ]\n",
      " [0.8600753 ]\n",
      " [0.34214735]\n",
      " [0.43919814]\n",
      " [0.8617289 ]\n",
      " [0.965792  ]\n",
      " [0.25241303]\n",
      " [0.29278427]\n",
      " [0.37489697]\n",
      " [0.5108579 ]\n",
      " [0.7666546 ]\n",
      " [0.630919  ]\n",
      " [0.6371854 ]\n",
      " [0.76276577]\n",
      " [0.47652674]\n",
      " [0.26530877]\n",
      " [0.48208198]\n",
      " [0.4020308 ]\n",
      " [0.5925922 ]\n",
      " [0.63812125]\n",
      " [0.97187287]\n",
      " [0.93667096]\n",
      " [0.97474945]\n",
      " [0.9498349 ]\n",
      " [0.8849673 ]\n",
      " [0.96052885]\n",
      " [0.9408744 ]\n",
      " [0.9995483 ]\n",
      " [0.97763145]\n",
      " [0.9997572 ]\n",
      " [0.8717637 ]\n",
      " [0.9984536 ]\n",
      " [0.98084104]\n",
      " [0.99832904]\n",
      " [0.9945189 ]\n",
      " [0.91461706]\n",
      " [0.99363387]\n",
      " [0.7304238 ]\n",
      " [0.973673  ]\n",
      " [0.74664307]\n",
      " [0.9349345 ]\n",
      " [0.82797253]\n",
      " [0.7015208 ]\n",
      " [0.8553775 ]\n",
      " [0.34603792]\n",
      " [0.8502    ]\n",
      " [0.74927074]\n",
      " [0.6681927 ]\n",
      " [0.97965395]\n",
      " [0.70909816]\n",
      " [0.5188092 ]\n",
      " [0.28869325]\n",
      " [0.7263409 ]\n",
      " [0.8526435 ]\n",
      " [0.9299102 ]\n",
      " [0.9362294 ]\n",
      " [0.9371567 ]\n",
      " [0.9717936 ]\n",
      " [0.92525554]\n",
      " [0.22481334]\n",
      " [0.80367607]\n",
      " [0.30697256]\n",
      " [0.22923696]\n",
      " [0.963712  ]\n",
      " [0.90885365]\n",
      " [0.97260654]\n",
      " [0.99072   ]\n",
      " [0.8891759 ]\n",
      " [0.18483558]\n",
      " [0.27581483]\n",
      " [0.87310183]\n",
      " [0.23626193]\n",
      " [0.90277624]\n",
      " [0.96784794]\n",
      " [0.97070247]\n",
      " [0.97127485]\n",
      " [0.9114494 ]\n",
      " [0.68468   ]\n",
      " [0.15269998]\n",
      " [0.18519613]\n",
      " [0.1187104 ]\n",
      " [0.13702944]\n",
      " [0.12660837]\n",
      " [0.49269396]\n",
      " [0.13274503]\n",
      " [0.41833138]\n",
      " [0.58671606]\n",
      " [0.62300014]]\n"
     ]
    }
   ],
   "source": [
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1040 samples, validate on 260 samples\n",
      "Epoch 1/500\n",
      "1040/1040 [==============================] - 3s 3ms/step - loss: 0.7580 - accuracy: 0.5750 - val_loss: 1.1770 - val_accuracy: 0.1538\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.17698, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 2/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.6640 - accuracy: 0.6327 - val_loss: 1.0278 - val_accuracy: 0.1615\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.17698 to 1.02781, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 3/500\n",
      "1040/1040 [==============================] - 0s 154us/step - loss: 0.6349 - accuracy: 0.6375 - val_loss: 0.9300 - val_accuracy: 0.2115\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02781 to 0.93001, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 4/500\n",
      "1040/1040 [==============================] - 0s 122us/step - loss: 0.6196 - accuracy: 0.6692 - val_loss: 0.8798 - val_accuracy: 0.2192\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.93001 to 0.87983, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 5/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.6068 - accuracy: 0.6971 - val_loss: 0.8584 - val_accuracy: 0.2346\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.87983 to 0.85845, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 6/500\n",
      "1040/1040 [==============================] - 0s 118us/step - loss: 0.5930 - accuracy: 0.7163 - val_loss: 0.8486 - val_accuracy: 0.2423\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.85845 to 0.84855, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 7/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.5768 - accuracy: 0.7442 - val_loss: 0.8466 - val_accuracy: 0.2500\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.84855 to 0.84655, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 8/500\n",
      "1040/1040 [==============================] - 0s 124us/step - loss: 0.5589 - accuracy: 0.7673 - val_loss: 0.8491 - val_accuracy: 0.2385\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.84655\n",
      "Epoch 9/500\n",
      "1040/1040 [==============================] - 0s 124us/step - loss: 0.5402 - accuracy: 0.7856 - val_loss: 0.8542 - val_accuracy: 0.2308\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.84655\n",
      "Epoch 10/500\n",
      "1040/1040 [==============================] - 0s 125us/step - loss: 0.5199 - accuracy: 0.7942 - val_loss: 0.8483 - val_accuracy: 0.2654\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.84655\n",
      "Epoch 11/500\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.4992 - accuracy: 0.8135 - val_loss: 0.8342 - val_accuracy: 0.2846\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.84655 to 0.83415, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 12/500\n",
      "1040/1040 [==============================] - 0s 124us/step - loss: 0.4788 - accuracy: 0.8288 - val_loss: 0.8061 - val_accuracy: 0.3692\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.83415 to 0.80606, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 13/500\n",
      "1040/1040 [==============================] - 0s 128us/step - loss: 0.4588 - accuracy: 0.8423 - val_loss: 0.7611 - val_accuracy: 0.4462\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.80606 to 0.76108, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 14/500\n",
      "1040/1040 [==============================] - 0s 127us/step - loss: 0.4373 - accuracy: 0.8519 - val_loss: 0.7013 - val_accuracy: 0.5115\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.76108 to 0.70130, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 15/500\n",
      "1040/1040 [==============================] - 0s 119us/step - loss: 0.4180 - accuracy: 0.8644 - val_loss: 0.6407 - val_accuracy: 0.6192\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.70130 to 0.64067, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 16/500\n",
      "1040/1040 [==============================] - 0s 124us/step - loss: 0.3973 - accuracy: 0.8721 - val_loss: 0.5834 - val_accuracy: 0.7038\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.64067 to 0.58335, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 17/500\n",
      "1040/1040 [==============================] - 0s 122us/step - loss: 0.3765 - accuracy: 0.8856 - val_loss: 0.5294 - val_accuracy: 0.7462\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.58335 to 0.52945, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 18/500\n",
      "1040/1040 [==============================] - 0s 121us/step - loss: 0.3556 - accuracy: 0.8923 - val_loss: 0.4704 - val_accuracy: 0.7923\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.52945 to 0.47043, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 19/500\n",
      "1040/1040 [==============================] - 0s 137us/step - loss: 0.3364 - accuracy: 0.8933 - val_loss: 0.4241 - val_accuracy: 0.8308\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.47043 to 0.42406, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 20/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.3168 - accuracy: 0.8971 - val_loss: 0.3873 - val_accuracy: 0.8385\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.42406 to 0.38729, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 21/500\n",
      "1040/1040 [==============================] - 0s 119us/step - loss: 0.2970 - accuracy: 0.9077 - val_loss: 0.3617 - val_accuracy: 0.8385\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.38729 to 0.36174, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 22/500\n",
      "1040/1040 [==============================] - 0s 126us/step - loss: 0.2791 - accuracy: 0.9144 - val_loss: 0.3452 - val_accuracy: 0.8385\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.36174 to 0.34516, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 23/500\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.2617 - accuracy: 0.9192 - val_loss: 0.3313 - val_accuracy: 0.8346\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.34516 to 0.33132, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 24/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.2448 - accuracy: 0.9240 - val_loss: 0.3169 - val_accuracy: 0.8538\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.33132 to 0.31689, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 25/500\n",
      "1040/1040 [==============================] - 0s 122us/step - loss: 0.2296 - accuracy: 0.9269 - val_loss: 0.3037 - val_accuracy: 0.8654\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.31689 to 0.30371, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 26/500\n",
      "1040/1040 [==============================] - 0s 122us/step - loss: 0.2149 - accuracy: 0.9308 - val_loss: 0.2908 - val_accuracy: 0.8692\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.30371 to 0.29077, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 27/500\n",
      "1040/1040 [==============================] - 0s 118us/step - loss: 0.2011 - accuracy: 0.9394 - val_loss: 0.2731 - val_accuracy: 0.8808\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.29077 to 0.27314, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 28/500\n",
      "1040/1040 [==============================] - 0s 124us/step - loss: 0.1885 - accuracy: 0.9462 - val_loss: 0.2573 - val_accuracy: 0.8923\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.27314 to 0.25725, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 29/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.1757 - accuracy: 0.9538 - val_loss: 0.2480 - val_accuracy: 0.8962\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.25725 to 0.24798, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 30/500\n",
      "1040/1040 [==============================] - 0s 142us/step - loss: 0.1641 - accuracy: 0.9567 - val_loss: 0.2392 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.24798 to 0.23916, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 31/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.1532 - accuracy: 0.9606 - val_loss: 0.2237 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.23916 to 0.22367, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 32/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.1428 - accuracy: 0.9625 - val_loss: 0.2143 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.22367 to 0.21429, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 33/500\n",
      "1040/1040 [==============================] - 0s 125us/step - loss: 0.1334 - accuracy: 0.9663 - val_loss: 0.2103 - val_accuracy: 0.9154\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.21429 to 0.21034, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 34/500\n",
      "1040/1040 [==============================] - 0s 118us/step - loss: 0.1240 - accuracy: 0.9673 - val_loss: 0.2048 - val_accuracy: 0.9154\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.21034 to 0.20479, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 35/500\n",
      "1040/1040 [==============================] - 0s 122us/step - loss: 0.1155 - accuracy: 0.9740 - val_loss: 0.1974 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.20479 to 0.19738, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 36/500\n",
      "1040/1040 [==============================] - 0s 122us/step - loss: 0.1079 - accuracy: 0.9779 - val_loss: 0.1941 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.19738 to 0.19415, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 37/500\n",
      "1040/1040 [==============================] - 0s 124us/step - loss: 0.1006 - accuracy: 0.9817 - val_loss: 0.1938 - val_accuracy: 0.9192\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.19415 to 0.19383, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 38/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.0940 - accuracy: 0.9837 - val_loss: 0.1932 - val_accuracy: 0.9192\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.19383 to 0.19320, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 39/500\n",
      "1040/1040 [==============================] - 0s 121us/step - loss: 0.0879 - accuracy: 0.9856 - val_loss: 0.1932 - val_accuracy: 0.9192\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.19320\n",
      "Epoch 40/500\n",
      "1040/1040 [==============================] - 0s 126us/step - loss: 0.0823 - accuracy: 0.9865 - val_loss: 0.1914 - val_accuracy: 0.9192\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.19320 to 0.19140, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 41/500\n",
      "1040/1040 [==============================] - 0s 124us/step - loss: 0.0769 - accuracy: 0.9885 - val_loss: 0.1897 - val_accuracy: 0.9192\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.19140 to 0.18969, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 42/500\n",
      "1040/1040 [==============================] - 0s 128us/step - loss: 0.0721 - accuracy: 0.9894 - val_loss: 0.1888 - val_accuracy: 0.9154\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.18969 to 0.18879, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 43/500\n",
      "1040/1040 [==============================] - 0s 126us/step - loss: 0.0674 - accuracy: 0.9894 - val_loss: 0.1874 - val_accuracy: 0.9154\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.18879 to 0.18735, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 44/500\n",
      "1040/1040 [==============================] - 0s 126us/step - loss: 0.0630 - accuracy: 0.9904 - val_loss: 0.1861 - val_accuracy: 0.9192\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.18735 to 0.18614, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 45/500\n",
      "1040/1040 [==============================] - 0s 125us/step - loss: 0.0586 - accuracy: 0.9933 - val_loss: 0.1826 - val_accuracy: 0.9192\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.18614 to 0.18256, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 46/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.0547 - accuracy: 0.9942 - val_loss: 0.1820 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.18256 to 0.18200, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 47/500\n",
      "1040/1040 [==============================] - 0s 118us/step - loss: 0.0511 - accuracy: 0.9962 - val_loss: 0.1829 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.18200\n",
      "Epoch 48/500\n",
      "1040/1040 [==============================] - 0s 121us/step - loss: 0.0475 - accuracy: 0.9962 - val_loss: 0.1842 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.18200\n",
      "Epoch 49/500\n",
      "1040/1040 [==============================] - 0s 121us/step - loss: 0.0444 - accuracy: 0.9962 - val_loss: 0.1868 - val_accuracy: 0.9269\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.18200\n",
      "Epoch 50/500\n",
      "1040/1040 [==============================] - 0s 128us/step - loss: 0.0413 - accuracy: 0.9962 - val_loss: 0.1885 - val_accuracy: 0.9269\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.18200\n",
      "Epoch 51/500\n",
      "1040/1040 [==============================] - 0s 126us/step - loss: 0.0384 - accuracy: 0.9971 - val_loss: 0.1887 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.18200\n",
      "Epoch 52/500\n",
      "1040/1040 [==============================] - 0s 124us/step - loss: 0.0357 - accuracy: 0.9971 - val_loss: 0.1882 - val_accuracy: 0.9269\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.18200\n",
      "Epoch 53/500\n",
      "1040/1040 [==============================] - 0s 121us/step - loss: 0.0333 - accuracy: 0.9971 - val_loss: 0.1894 - val_accuracy: 0.9269\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.18200\n",
      "Epoch 54/500\n",
      "1040/1040 [==============================] - 0s 126us/step - loss: 0.0311 - accuracy: 0.9981 - val_loss: 0.1915 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.18200\n",
      "Epoch 55/500\n",
      "1040/1040 [==============================] - 0s 124us/step - loss: 0.0290 - accuracy: 0.9990 - val_loss: 0.1944 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.18200\n",
      "Epoch 56/500\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.0262 - accuracy: 0.9981 - val_loss: 0.1915 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.18200\n",
      "Epoch 57/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.0261 - accuracy: 0.9990 - val_loss: 0.1911 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.18200\n",
      "Epoch 58/500\n",
      "1040/1040 [==============================] - 0s 130us/step - loss: 0.0258 - accuracy: 0.9990 - val_loss: 0.1911 - val_accuracy: 0.9269\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.18200\n",
      "Epoch 59/500\n",
      "1040/1040 [==============================] - 0s 127us/step - loss: 0.0256 - accuracy: 0.9990 - val_loss: 0.1909 - val_accuracy: 0.9308\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.18200\n",
      "Epoch 60/500\n",
      "1040/1040 [==============================] - 0s 121us/step - loss: 0.0254 - accuracy: 0.9990 - val_loss: 0.1904 - val_accuracy: 0.9269\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.18200\n",
      "Epoch 61/500\n",
      "1040/1040 [==============================] - 0s 121us/step - loss: 0.0252 - accuracy: 0.9990 - val_loss: 0.1895 - val_accuracy: 0.9269\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.18200\n",
      "Epoch 62/500\n",
      "1040/1040 [==============================] - 0s 118us/step - loss: 0.0251 - accuracy: 0.9990 - val_loss: 0.1884 - val_accuracy: 0.9269\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.18200\n",
      "Epoch 63/500\n",
      "1040/1040 [==============================] - 0s 124us/step - loss: 0.0249 - accuracy: 0.9990 - val_loss: 0.1872 - val_accuracy: 0.9269\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.18200\n",
      "Epoch 64/500\n",
      "1040/1040 [==============================] - 0s 126us/step - loss: 0.0247 - accuracy: 0.9990 - val_loss: 0.1859 - val_accuracy: 0.9269\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.18200\n",
      "Epoch 65/500\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.0246 - accuracy: 0.9990 - val_loss: 0.1847 - val_accuracy: 0.9269\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.18200\n",
      "Epoch 66/500\n",
      "1040/1040 [==============================] - 0s 127us/step - loss: 0.0243 - accuracy: 0.9990 - val_loss: 0.1830 - val_accuracy: 0.9308\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.18200\n",
      "Epoch 67/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.0243 - accuracy: 0.9990 - val_loss: 0.1815 - val_accuracy: 0.9346\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.18200 to 0.18153, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 68/500\n",
      "1040/1040 [==============================] - 0s 124us/step - loss: 0.0243 - accuracy: 0.9990 - val_loss: 0.1802 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.18153 to 0.18018, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 69/500\n",
      "1040/1040 [==============================] - 0s 125us/step - loss: 0.0243 - accuracy: 0.9990 - val_loss: 0.1790 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.18018 to 0.17896, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 70/500\n",
      "1040/1040 [==============================] - 0s 128us/step - loss: 0.0243 - accuracy: 0.9990 - val_loss: 0.1779 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.17896 to 0.17792, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 71/500\n",
      "1040/1040 [==============================] - 0s 127us/step - loss: 0.0242 - accuracy: 0.9990 - val_loss: 0.1770 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.17792 to 0.17697, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 72/500\n",
      "1040/1040 [==============================] - 0s 127us/step - loss: 0.0242 - accuracy: 0.9990 - val_loss: 0.1761 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.17697 to 0.17610, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 73/500\n",
      "1040/1040 [==============================] - 0s 129us/step - loss: 0.0242 - accuracy: 0.9990 - val_loss: 0.1753 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.17610 to 0.17531, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 74/500\n",
      "1040/1040 [==============================] - 0s 127us/step - loss: 0.0242 - accuracy: 0.9990 - val_loss: 0.1746 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.17531 to 0.17462, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 75/500\n",
      "1040/1040 [==============================] - 0s 121us/step - loss: 0.0242 - accuracy: 0.9990 - val_loss: 0.1740 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.17462 to 0.17399, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 76/500\n",
      "1040/1040 [==============================] - 0s 125us/step - loss: 0.0242 - accuracy: 0.9990 - val_loss: 0.1734 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.17399 to 0.17340, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 77/500\n",
      "1040/1040 [==============================] - 0s 122us/step - loss: 0.0241 - accuracy: 0.9990 - val_loss: 0.1729 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.17340 to 0.17288, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 78/500\n",
      "1040/1040 [==============================] - 0s 128us/step - loss: 0.0241 - accuracy: 0.9990 - val_loss: 0.1724 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.17288 to 0.17239, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 79/500\n",
      "1040/1040 [==============================] - 0s 125us/step - loss: 0.0241 - accuracy: 0.9990 - val_loss: 0.1720 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.17239 to 0.17201, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 80/500\n",
      "1040/1040 [==============================] - 0s 122us/step - loss: 0.0241 - accuracy: 0.9990 - val_loss: 0.1717 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.17201 to 0.17169, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 81/500\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.0241 - accuracy: 0.9990 - val_loss: 0.1714 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.17169 to 0.17142, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 82/500\n",
      "1040/1040 [==============================] - 0s 116us/step - loss: 0.0240 - accuracy: 0.9990 - val_loss: 0.1712 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.17142 to 0.17118, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 83/500\n",
      "1040/1040 [==============================] - 0s 127us/step - loss: 0.0240 - accuracy: 0.9990 - val_loss: 0.1710 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.17118 to 0.17096, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 84/500\n",
      "1040/1040 [==============================] - 0s 119us/step - loss: 0.0240 - accuracy: 0.9990 - val_loss: 0.1708 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.17096 to 0.17076, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 85/500\n",
      "1040/1040 [==============================] - 0s 131us/step - loss: 0.0240 - accuracy: 0.9990 - val_loss: 0.1706 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.17076 to 0.17057, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 86/500\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.0240 - accuracy: 0.9990 - val_loss: 0.1704 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.17057 to 0.17040, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 87/500\n",
      "1040/1040 [==============================] - 0s 119us/step - loss: 0.0240 - accuracy: 0.9990 - val_loss: 0.1702 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.17040 to 0.17024, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 88/500\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.0239 - accuracy: 0.9990 - val_loss: 0.1701 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.17024 to 0.17009, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 89/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.0239 - accuracy: 0.9990 - val_loss: 0.1699 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.17009 to 0.16995, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 90/500\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.0239 - accuracy: 0.9990 - val_loss: 0.1698 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.16995 to 0.16982, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 91/500\n",
      "1040/1040 [==============================] - 0s 132us/step - loss: 0.0239 - accuracy: 0.9990 - val_loss: 0.1697 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.16982 to 0.16969, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 92/500\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.0239 - accuracy: 0.9990 - val_loss: 0.1696 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.16969 to 0.16958, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 93/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.0238 - accuracy: 0.9990 - val_loss: 0.1695 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.16958 to 0.16947, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 94/500\n",
      "1040/1040 [==============================] - 0s 124us/step - loss: 0.0238 - accuracy: 0.9990 - val_loss: 0.1694 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.16947 to 0.16937, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 95/500\n",
      "1040/1040 [==============================] - 0s 125us/step - loss: 0.0238 - accuracy: 0.9990 - val_loss: 0.1693 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.16937 to 0.16928, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 96/500\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.0238 - accuracy: 0.9990 - val_loss: 0.1692 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.16928 to 0.16920, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 97/500\n",
      "1040/1040 [==============================] - 0s 136us/step - loss: 0.0238 - accuracy: 0.9990 - val_loss: 0.1691 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.16920 to 0.16912, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 98/500\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.0238 - accuracy: 0.9990 - val_loss: 0.1691 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.16912 to 0.16905, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 99/500\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.0237 - accuracy: 0.9990 - val_loss: 0.1690 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.16905 to 0.16898, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 100/500\n",
      "1040/1040 [==============================] - 0s 119us/step - loss: 0.0237 - accuracy: 0.9990 - val_loss: 0.1689 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.16898 to 0.16891, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 101/500\n",
      "1040/1040 [==============================] - 0s 125us/step - loss: 0.0237 - accuracy: 0.9990 - val_loss: 0.1688 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.16891 to 0.16885, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 102/500\n",
      "1040/1040 [==============================] - 0s 124us/step - loss: 0.0237 - accuracy: 0.9990 - val_loss: 0.1688 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.16885 to 0.16879, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 103/500\n",
      "1040/1040 [==============================] - 0s 121us/step - loss: 0.0237 - accuracy: 0.9990 - val_loss: 0.1687 - val_accuracy: 0.9423\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.16879 to 0.16874, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 104/500\n",
      "1040/1040 [==============================] - 0s 116us/step - loss: 0.0236 - accuracy: 0.9990 - val_loss: 0.1687 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.16874 to 0.16870, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 105/500\n",
      "1040/1040 [==============================] - 0s 117us/step - loss: 0.0236 - accuracy: 0.9990 - val_loss: 0.1687 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.16870 to 0.16865, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 106/500\n",
      "1040/1040 [==============================] - 0s 127us/step - loss: 0.0236 - accuracy: 0.9990 - val_loss: 0.1686 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.16865 to 0.16861, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 107/500\n",
      "1040/1040 [==============================] - 0s 125us/step - loss: 0.0236 - accuracy: 0.9990 - val_loss: 0.1686 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.16861 to 0.16857, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 108/500\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.0236 - accuracy: 0.9990 - val_loss: 0.1685 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.16857 to 0.16854, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 109/500\n",
      "1040/1040 [==============================] - 0s 130us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1685 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.16854 to 0.16847, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 110/500\n",
      "1040/1040 [==============================] - 0s 126us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1684 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.16847 to 0.16840, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 111/500\n",
      "1040/1040 [==============================] - 0s 130us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1683 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.16840 to 0.16835, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 112/500\n",
      "1040/1040 [==============================] - 0s 133us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1683 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.16835 to 0.16829, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 113/500\n",
      "1040/1040 [==============================] - 0s 135us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1682 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.16829 to 0.16825, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 114/500\n",
      "1040/1040 [==============================] - 0s 128us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1682 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.16825 to 0.16820, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 115/500\n",
      "1040/1040 [==============================] - 0s 135us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1682 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.16820 to 0.16816, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 116/500\n",
      "1040/1040 [==============================] - 0s 124us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1681 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.16816 to 0.16812, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 117/500\n",
      "1040/1040 [==============================] - 0s 124us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1681 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.16812 to 0.16809, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 118/500\n",
      "1040/1040 [==============================] - 0s 126us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1681 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00118: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.16809 to 0.16806, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 119/500\n",
      "1040/1040 [==============================] - 0s 128us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1680 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.16806 to 0.16802, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 120/500\n",
      "1040/1040 [==============================] - 0s 128us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1680 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.16802 to 0.16799, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 121/500\n",
      "1040/1040 [==============================] - 0s 131us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1680 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.16799 to 0.16797, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 122/500\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1679 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.16797 to 0.16794, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 123/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1679 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.16794 to 0.16792, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 124/500\n",
      "1040/1040 [==============================] - 0s 122us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1679 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.16792 to 0.16790, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 125/500\n",
      "1040/1040 [==============================] - 0s 121us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1679 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.16790 to 0.16788, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 126/500\n",
      "1040/1040 [==============================] - 0s 121us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1679 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.16788 to 0.16786, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 127/500\n",
      "1040/1040 [==============================] - 0s 119us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1678 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.16786 to 0.16785, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 128/500\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1678 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.16785 to 0.16783, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 129/500\n",
      "1040/1040 [==============================] - 0s 122us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1678 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00129: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.16783 to 0.16782, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 130/500\n",
      "1040/1040 [==============================] - 0s 119us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1678 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.16782 to 0.16781, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 131/500\n",
      "1040/1040 [==============================] - 0s 130us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1678 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.16781 to 0.16780, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 132/500\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1678 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.16780 to 0.16779, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 133/500\n",
      "1040/1040 [==============================] - 0s 122us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1678 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.16779 to 0.16778, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 134/500\n",
      "1040/1040 [==============================] - 0s 119us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1678 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.16778 to 0.16777, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 135/500\n",
      "1040/1040 [==============================] - 0s 122us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1678 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.16777 to 0.16776, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 136/500\n",
      "1040/1040 [==============================] - 0s 126us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1678 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.16776 to 0.16776, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 137/500\n",
      "1040/1040 [==============================] - 0s 124us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1678 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.16776 to 0.16775, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 138/500\n",
      "1040/1040 [==============================] - 0s 119us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.16775 to 0.16774, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 139/500\n",
      "1040/1040 [==============================] - 0s 126us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.16774 to 0.16774, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 140/500\n",
      "1040/1040 [==============================] - 0s 119us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.16774 to 0.16773, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 141/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.16773 to 0.16773, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 142/500\n",
      "1040/1040 [==============================] - 0s 130us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.16773 to 0.16773, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 143/500\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.16773 to 0.16772, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 144/500\n",
      "1040/1040 [==============================] - 0s 125us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.16772 to 0.16772, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 145/500\n",
      "1040/1040 [==============================] - 0s 126us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.16772 to 0.16772, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 146/500\n",
      "1040/1040 [==============================] - 0s 121us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.16772 to 0.16771, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 147/500\n",
      "1040/1040 [==============================] - 0s 128us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.16771 to 0.16771, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 148/500\n",
      "1040/1040 [==============================] - 0s 119us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.16771 to 0.16771, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 149/500\n",
      "1040/1040 [==============================] - 0s 118us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00149: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.16771 to 0.16771, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 150/500\n",
      "1040/1040 [==============================] - 0s 118us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.16771 to 0.16770, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 151/500\n",
      "1040/1040 [==============================] - 0s 131us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.16770 to 0.16770, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 152/500\n",
      "1040/1040 [==============================] - 0s 132us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.16770 to 0.16770, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 153/500\n",
      "1040/1040 [==============================] - 0s 122us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.16770 to 0.16770, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 154/500\n",
      "1040/1040 [==============================] - 0s 118us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.16770 to 0.16770, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 155/500\n",
      "1040/1040 [==============================] - 0s 115us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.16770 to 0.16770, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 156/500\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.16770 to 0.16770, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 157/500\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.16770 to 0.16770, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 158/500\n",
      "1040/1040 [==============================] - 0s 125us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.16770 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 159/500\n",
      "1040/1040 [==============================] - 0s 119us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00159: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 160/500\n",
      "1040/1040 [==============================] - 0s 122us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 161/500\n",
      "1040/1040 [==============================] - 0s 121us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 162/500\n",
      "1040/1040 [==============================] - 0s 125us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 163/500\n",
      "1040/1040 [==============================] - 0s 121us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 164/500\n",
      "1040/1040 [==============================] - 0s 119us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 165/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 166/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 167/500\n",
      "1040/1040 [==============================] - 0s 122us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 168/500\n",
      "1040/1040 [==============================] - 0s 122us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 169/500\n",
      "1040/1040 [==============================] - 0s 119us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00169: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 170/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 171/500\n",
      "1040/1040 [==============================] - 0s 125us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 172/500\n",
      "1040/1040 [==============================] - 0s 141us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 173/500\n",
      "1040/1040 [==============================] - 0s 125us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 174/500\n",
      "1040/1040 [==============================] - 0s 124us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 175/500\n",
      "1040/1040 [==============================] - 0s 128us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 176/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 177/500\n",
      "1040/1040 [==============================] - 0s 125us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 178/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 179/500\n",
      "1040/1040 [==============================] - 0s 123us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00179: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 180/500\n",
      "1040/1040 [==============================] - 0s 124us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 181/500\n",
      "1040/1040 [==============================] - 0s 130us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 182/500\n",
      "1040/1040 [==============================] - 0s 121us/step - loss: 0.0235 - accuracy: 0.9990 - val_loss: 0.1677 - val_accuracy: 0.9462\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.16769 to 0.16769, saving model to ./210303/residual_filter4_0303.h5\n",
      "Epoch 00182: early stopping\n"
     ]
    }
   ],
   "source": [
    "results = model.fit(img_train, lab_train, batch_size=128, epochs=500, verbose=1, validation_split=0.2, shuffle=False, callbacks=callbacks_list)\n",
    "# results = model.fit(img_train, lab_train, batch_size=128, epochs=500, verbose=1, shuffle=False,  callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('basic_class_32_bilinear_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.axes(facecolor='white')\n",
    "plt.plot(results.history['loss'])\n",
    "# plt.plot(results.history['accuracy'])\n",
    "# plt.plot(results.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'acc'], loc='upper left')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.savefig('./210303/residual_filter4_0303.jpg',dpi=300, facecolor='w', edgecolor='w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 0s 775us/step\n"
     ]
    }
   ],
   "source": [
    "test_results= model.predict(img_test, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "# plt.axes(facecolor='white')\n",
    "plt.plot(test_results)\n",
    "plt.plot(lab_test)\n",
    "plt.title('test')\n",
    "plt.legend(['prediction', 'GT'], loc='upper left')\n",
    "plt.savefig('./result_res_loss_32_169prox400',dpi=200, facecolor='w', edgecolor='w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 0s 289us/step\n",
      "loss: 0.20, : 0.923  \n"
     ]
    }
   ],
   "source": [
    "score= model.evaluate(img_test, lab_test)\n",
    "print(\"loss: %.2f, : %.3f  \" %(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.0060118e-01]\n",
      " [6.7817378e-01]\n",
      " [8.9892894e-02]\n",
      " [5.8406210e-01]\n",
      " [2.8877646e-01]\n",
      " [8.1870347e-02]\n",
      " [1.6748384e-01]\n",
      " [6.3778693e-03]\n",
      " [3.7915963e-01]\n",
      " [8.8254735e-02]\n",
      " [3.8430326e-02]\n",
      " [4.6315421e-03]\n",
      " [7.4479294e-01]\n",
      " [3.9892825e-01]\n",
      " [9.4616103e-01]\n",
      " [9.7020668e-01]\n",
      " [1.8380910e-02]\n",
      " [3.2021514e-01]\n",
      " [6.2913162e-01]\n",
      " [3.2677647e-01]\n",
      " [6.8366688e-01]\n",
      " [8.5860056e-01]\n",
      " [2.0336865e-02]\n",
      " [1.5174237e-01]\n",
      " [1.3121969e-02]\n",
      " [7.4305296e-02]\n",
      " [8.9012412e-04]\n",
      " [8.2124732e-03]\n",
      " [2.0217292e-02]\n",
      " [1.7631860e-01]\n",
      " [9.1882591e-04]\n",
      " [1.4407821e-02]\n",
      " [1.3999150e-02]\n",
      " [2.1091526e-02]\n",
      " [1.3625953e-01]\n",
      " [3.3138773e-01]\n",
      " [5.2840787e-01]\n",
      " [9.7708941e-02]\n",
      " [2.3281140e-02]\n",
      " [2.2622342e-03]\n",
      " [3.1449743e-02]\n",
      " [6.5144682e-01]\n",
      " [3.0314288e-01]\n",
      " [9.1121638e-01]\n",
      " [2.2471367e-01]\n",
      " [5.3340232e-01]\n",
      " [6.4104241e-01]\n",
      " [7.2139210e-01]\n",
      " [4.8776734e-01]\n",
      " [4.4714373e-02]\n",
      " [2.7965719e-01]\n",
      " [3.2943148e-02]\n",
      " [3.9206159e-01]\n",
      " [3.1421232e-01]\n",
      " [9.5734292e-01]\n",
      " [3.4261438e-01]\n",
      " [1.3404512e-01]\n",
      " [3.2555050e-01]\n",
      " [1.3441288e-01]\n",
      " [1.7464785e-01]\n",
      " [5.6469772e-02]\n",
      " [3.1799561e-01]\n",
      " [6.0690778e-01]\n",
      " [3.8867560e-01]\n",
      " [6.9011563e-01]\n",
      " [4.1389126e-02]\n",
      " [1.4414906e-02]\n",
      " [1.7848246e-02]\n",
      " [4.0327978e-01]\n",
      " [7.8538692e-01]\n",
      " [5.0393546e-01]\n",
      " [1.7417914e-01]\n",
      " [2.1853298e-01]\n",
      " [5.1527660e-02]\n",
      " [7.7341491e-01]\n",
      " [3.4900092e-02]\n",
      " [1.5634480e-01]\n",
      " [1.1306225e-02]\n",
      " [6.3255411e-01]\n",
      " [2.5099891e-01]\n",
      " [9.0931810e-02]\n",
      " [1.8356333e-02]\n",
      " [4.8880186e-02]\n",
      " [3.3184224e-01]\n",
      " [1.9407524e-01]\n",
      " [4.9469030e-01]\n",
      " [2.0035334e-02]\n",
      " [6.5592460e-02]\n",
      " [9.3099207e-01]\n",
      " [4.2227596e-01]\n",
      " [2.3232145e-02]\n",
      " [1.1802762e-01]\n",
      " [2.2031586e-01]\n",
      " [9.9587396e-02]\n",
      " [9.0132870e-02]\n",
      " [4.8728299e-01]\n",
      " [8.1246859e-01]\n",
      " [6.1728124e-02]\n",
      " [7.2012149e-02]\n",
      " [2.9680592e-01]\n",
      " [9.6022940e-01]\n",
      " [8.8894862e-01]\n",
      " [9.9903262e-01]\n",
      " [9.9983835e-01]\n",
      " [9.7854954e-01]\n",
      " [8.8842565e-01]\n",
      " [9.9785030e-01]\n",
      " [9.9847895e-01]\n",
      " [9.9952757e-01]\n",
      " [8.6786377e-01]\n",
      " [9.8777044e-01]\n",
      " [9.9999976e-01]\n",
      " [9.9999607e-01]\n",
      " [9.9998486e-01]\n",
      " [9.8362261e-01]\n",
      " [9.4807452e-01]\n",
      " [7.1273154e-01]\n",
      " [9.8602808e-01]\n",
      " [7.9520905e-01]\n",
      " [9.5344746e-01]\n",
      " [9.9408627e-01]\n",
      " [9.2584872e-01]\n",
      " [9.7028279e-01]\n",
      " [6.9437367e-01]\n",
      " [6.8137908e-01]\n",
      " [9.9952245e-01]\n",
      " [9.6787238e-01]\n",
      " [1.7765726e-01]\n",
      " [9.2288330e-02]\n",
      " [1.7512435e-01]\n",
      " [1.9520016e-01]\n",
      " [6.0639346e-01]\n",
      " [9.4529027e-01]\n",
      " [9.2482495e-01]\n",
      " [9.1736203e-01]\n",
      " [1.0918893e-01]\n",
      " [4.2827758e-01]\n",
      " [6.0771808e-02]\n",
      " [6.7610526e-01]\n",
      " [4.1307053e-01]\n",
      " [7.6633728e-01]\n",
      " [1.5957071e-02]\n",
      " [9.9748164e-01]\n",
      " [9.9717772e-01]\n",
      " [6.0504925e-01]\n",
      " [4.3103680e-02]\n",
      " [9.5267129e-01]\n",
      " [9.6091104e-01]\n",
      " [8.0902964e-01]\n",
      " [9.9604034e-01]\n",
      " [9.9622899e-01]\n",
      " [9.9961060e-01]\n",
      " [9.9997830e-01]\n",
      " [9.9995148e-01]\n",
      " [9.9978799e-01]\n",
      " [9.9864095e-01]\n",
      " [9.9740857e-01]\n",
      " [9.8371279e-01]\n",
      " [9.9397755e-01]\n",
      " [9.9950480e-01]\n",
      " [9.9915755e-01]\n",
      " [9.9999118e-01]\n",
      " [9.9998808e-01]\n",
      " [9.9537873e-01]\n",
      " [9.9955767e-01]\n",
      " [9.9562126e-01]\n",
      " [9.9844801e-01]\n",
      " [9.9994361e-01]\n",
      " [9.9999118e-01]\n",
      " [9.9991465e-01]\n",
      " [9.9941432e-01]\n",
      " [9.9999297e-01]\n",
      " [9.9859720e-01]\n",
      " [3.8723004e-01]\n",
      " [9.9178892e-01]\n",
      " [9.9829143e-01]\n",
      " [9.9901175e-01]\n",
      " [5.0421715e-01]\n",
      " [5.6465650e-01]\n",
      " [7.8226435e-01]\n",
      " [8.9221168e-01]\n",
      " [6.0381913e-01]\n",
      " [4.7777402e-01]\n",
      " [2.1469381e-01]\n",
      " [9.8706901e-01]\n",
      " [9.9958116e-01]\n",
      " [9.6889323e-01]\n",
      " [9.8609734e-01]\n",
      " [9.9753845e-01]\n",
      " [3.9580616e-01]\n",
      " [9.1227543e-01]\n",
      " [9.0328276e-01]\n",
      " [4.5508465e-01]\n",
      " [9.8074430e-01]\n",
      " [6.9035930e-03]\n",
      " [9.9940777e-01]\n",
      " [5.0538689e-01]\n",
      " [9.9084502e-01]\n",
      " [5.3806365e-01]\n",
      " [9.9390984e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324 324\n"
     ]
    }
   ],
   "source": [
    "print(len(test_results), len(img_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN=0\n",
    "TP=0\n",
    "FN=0\n",
    "FP=0\n",
    "# predicts=[]\n",
    "for i in range(len(test_results)):\n",
    "    if i<int(len(test_results)/2):\n",
    "#         label= 'label: 0'\n",
    "        if test_results[i]<0.5:\n",
    "            TN+=1\n",
    "#             plt.figure()\n",
    "#             plt.title(\"num:\"+ str(i)+', label: 0, probability:'+str(test_results[i])+ ', predict: 0, normal')\n",
    "#             plt.imshow(img_test[i,:,:,0],cmap='gray')\n",
    "#             plt.savefig('./210303/result_img/'+str(i),dpi=200, facecolor='w', edgecolor='w')\n",
    "            \n",
    "        else:\n",
    "            FP+=1\n",
    "#             plt.figure()\n",
    "#             plt.title(\"num:\"+ str(i)+', label: 0, probability:'+str(test_results[i])+ ', predict: 1, abnormal')\n",
    "#             plt.imshow(img_test[i,:,:,0],cmap='gray')\n",
    "#             plt.savefig('./210303/result_img/'+str(i),dpi=200, facecolor='w', edgecolor='w')\n",
    "            \n",
    "    else:\n",
    "        label= 'label: 1'\n",
    "        if test_results[i]<0.5:\n",
    "            FN+=1\n",
    "#             plt.figure()\n",
    "#             plt.title(\"num:\"+ str(i)+', label: 1, probability:'+str(test_results[i])+ ', predict: 0, normal')\n",
    "#             plt.imshow(img_test[i,:,:,0],cmap='gray')\n",
    "#             plt.savefig('./210303/result_img/'+str(i),dpi=200, facecolor='w', edgecolor='w')\n",
    "            \n",
    "        else:\n",
    "            TP+=1\n",
    "#             plt.figure()\n",
    "#             plt.title(\"num:\"+ str(i)+', label: 1, probability:'+str(test_results[i])+ ', predict: 1, abnormal')\n",
    "#             plt.imshow(img_test[i,:,:,0],cmap='gray')\n",
    "#             plt.savefig('./210303/result_img/'+str(i),dpi=200, facecolor='w', edgecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155 18 7 144\n"
     ]
    }
   ],
   "source": [
    "print(TP, FP, FN, TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9228395061728395 0.8959537572254336 0.9567901234567902 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "acc = (TP+TN)/(TP+FP+TN+FN)\n",
    "prec = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "spec = 1-(FP/(TN+FP))\n",
    "\n",
    "print(acc,prec, recall, spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score,confusion_matrix,accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support,roc_curve,auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_lr,tpr_lr,_=roc_curve(lab_test,test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "auc_lr=auc(fpr_lr, tpr_lr)\n",
    "plt.plot(fpr_lr, tpr_lr, label=\"AUC= %s\"%(round(auc_lr,3)))\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('1-Specificity',fontsize=12)\n",
    "plt.ylabel('Sensitivity',fontsize=15)\n",
    "plt.title('Receiver Operating Characteristic curves')\n",
    "plt.legend(loc=\"lower right\",fontsize=12)\n",
    "\n",
    "plt.savefig('./210303/ROC_residual_filter4_0303.jpg',dpi=300, facecolor='w', edgecolor='w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function confusion_matrix in module sklearn.metrics._classification:\n",
      "\n",
      "confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)\n",
      "    Compute confusion matrix to evaluate the accuracy of a classification.\n",
      "    \n",
      "    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\n",
      "    is equal to the number of observations known to be in group :math:`i` and\n",
      "    predicted to be in group :math:`j`.\n",
      "    \n",
      "    Thus in binary classification, the count of true negatives is\n",
      "    :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is\n",
      "    :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : array-like of shape (n_samples,)\n",
      "        Ground truth (correct) target values.\n",
      "    \n",
      "    y_pred : array-like of shape (n_samples,)\n",
      "        Estimated targets as returned by a classifier.\n",
      "    \n",
      "    labels : array-like of shape (n_classes), default=None\n",
      "        List of labels to index the matrix. This may be used to reorder\n",
      "        or select a subset of labels.\n",
      "        If ``None`` is given, those that appear at least once\n",
      "        in ``y_true`` or ``y_pred`` are used in sorted order.\n",
      "    \n",
      "    sample_weight : array-like of shape (n_samples,), default=None\n",
      "        Sample weights.\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    normalize : {'true', 'pred', 'all'}, default=None\n",
      "        Normalizes confusion matrix over the true (rows), predicted (columns)\n",
      "        conditions or all the population. If None, confusion matrix will not be\n",
      "        normalized.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    C : ndarray of shape (n_classes, n_classes)\n",
      "        Confusion matrix whose i-th row and j-th\n",
      "        column entry indicates the number of\n",
      "        samples with true label being i-th class\n",
      "        and prediced label being j-th class.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] `Wikipedia entry for the Confusion matrix\n",
      "           <https://en.wikipedia.org/wiki/Confusion_matrix>`_\n",
      "           (Wikipedia and other references may use a different\n",
      "           convention for axes)\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.metrics import confusion_matrix\n",
      "    >>> y_true = [2, 0, 2, 2, 0, 1]\n",
      "    >>> y_pred = [0, 0, 2, 2, 0, 2]\n",
      "    >>> confusion_matrix(y_true, y_pred)\n",
      "    array([[2, 0, 0],\n",
      "           [0, 0, 1],\n",
      "           [1, 0, 2]])\n",
      "    \n",
      "    >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "    >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "    >>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
      "    array([[2, 0, 0],\n",
      "           [0, 0, 1],\n",
      "           [1, 0, 2]])\n",
      "    \n",
      "    In the binary case, we can extract true positives, etc as follows:\n",
      "    \n",
      "    >>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
      "    >>> (tn, fp, fn, tp)\n",
      "    (0, 2, 1, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "predicts=[]\n",
    "for proba in test_results:\n",
    "    if proba > 0.5:\n",
    "        predicts.append(1)\n",
    "    else:\n",
    "        predicts.append(0)\n",
    "print(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fdcbc786780>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "confusionMatrix = confusion_matrix(lab_test, predicts)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix)\n",
    "\n",
    "disp.plot(cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
