{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import warnings\r\n",
    "import os\r\n",
    "warnings.filterwarnings(action='ignore')\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.metrics import roc_auc_score,confusion_matrix,accuracy_score\r\n",
    "from sklearn.metrics import precision_recall_fscore_support,roc_curve,auc\r\n",
    "from sklearn.model_selection import StratifiedKFold\r\n",
    "from sklearn.model_selection import KFold\r\n",
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "from xgboost import XGBClassifier\r\n",
    "from xgboost import plot_importance\r\n",
    "from lightgbm import LGBMClassifier\r\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\r\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.svm import SVC\r\n",
    "from imblearn.over_sampling import SMOTE\r\n",
    "from imblearn.under_sampling import *\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "%matplotlib inline\r\n",
    "import eli5\r\n",
    "from eli5.sklearn import PermutationImportance\r\n",
    "from pystacknet.pystacknet import StackNetClassifier\r\n",
    "from imblearn.combine import SMOTETomek\r\n",
    "import plotly.graph_objects as go\r\n",
    "\r\n",
    "os.environ['CUDA_VISIABLEDIVCES']='4'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('../featureData/32_feature/first_GLCM/GLCM.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Ran_state = 25"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.info"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "데이터 불러오기 끝.\n",
    "데이터 전처리 시작"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_y = df['label']\n",
    "\n",
    "data_x = df.drop('label',1)\n",
    "# print(data_y)\n",
    "# print(data_x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "연속형 변수들 MinMaxScaler 진행"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########Why these indexes are dropped? => 키나 몸무게외 같은 연속성 수치이기 때문에 scaling을 하지 않음\n",
    "DFS = data_x.loc[:,['First_Order_10Percentile','First_Order_90Percentile','First_Order_Energy',\t'First_Order_InterquartileRange',\n",
    "                    'First_Order_Maximum','First_Order_MeanAbsoluteDeviation',\t'First_Order_Mean',\t'First_Order_Median','First_Order_Minimum',\n",
    "                    'First_Order_Range','First_Order_RobustMeanAbsoluteDeviation','First_Order_RootMeanSquared','First_Order_TotalEnergy',\t\n",
    "                    'First_Order_Variance','GLCM_Autocorrelation','GLCM_ClusterProminence','GLCM_SumAverage'\n",
    "]]\n",
    "data_x = data_x.drop(DFS.columns,1)\n",
    "# print(data_x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#########연속형 변수를 제외하고 스케일링을 진행한 후, 제외된 연속된 변수를 다시 포함해줌(feature scaling)\n",
    "#########현 데이터는 필요 없음===> first_ortder의 energy와 total energy의 수치는 매우 크므로 적용\n",
    "MM = MinMaxScaler()\n",
    "DFSMM = MM.fit_transform(DFS)\n",
    "DFS = pd.DataFrame(DFSMM,columns=DFS.columns)\n",
    "data_x = pd.concat([DFS, data_x],1)\n",
    "# data_x.to_csv('./Energy_scaled_all.csv', sep=',')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(data_x)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "다중공선성 높은 변수들 제거"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"features\"] = data_x.columns\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(data_x.values, i) for i in range(data_x.shape[1])]\n",
    "vif_remove = vif.sort_values(ascending=False,by='VIF Factor')[:]\n",
    "vif_remove.to_csv('../Radiomics/sorted_vif_GLCM.csv', sep=',')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vif_remove"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "####10이상의 수치를 가진 것들 제외(하나만 남기고 제외)=>모든 feature가 10미만이 될 때까지?\n",
    "# data_x_outEnergy = data_x.drop(['First_Order_Energy'],1)\n",
    "data_x_out = data_x.drop(['GLCM_ClusterTendency','GLCM_JointAverage','First_Order_Energy','First_Order_Maximum','GLCM_Id','GLCM_Idmn','GLCM_Idn','GLCM_Idm','GLCM_JointEntropy',\n",
    "                          'GLCM_SumEntropy','First_Order_Mean','GLCM_DifferenceAverage','First_Order_RootMeanSquared','GLCM_Imc2','First_Order_Uniformity',\n",
    "                          'GLCM_DifferenceEntropy','GLCM_Correlation','First_Order_Entropy','GLCM_SumSquares','First_Order_MeanAbsoluteDeviation','First_Order_90Percentile',\n",
    "                         'GLCM_SumAverage','GLCM_MCC','First_Order_Median','GLCM_Contrast','First_Order_10Percentile','First_Order_Variance','First_Order_TotalEnergy','GLCM_JointEnergy',\n",
    "                         'First_Order_Range','First_Order_RobustMeanAbsoluteDeviation','GLCM_InverseVariance','GLCM_Imc1','GLCM_DifferenceVariance'],1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(data_x_outEnergy)\n",
    "vif = pd.DataFrame()\n",
    "vif[\"features\"] = data_x_out.columns\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(data_x_out.values, i) for i in range(data_x_out.shape[1])]\n",
    "vif_remove = vif.sort_values(ascending=False,by='VIF Factor')[:]\n",
    "vif_remove.to_csv('../Radiomics/sorted_vif_out.csv', sep=',')\n",
    "vif_remove"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vif_remove['VIF Factor'][22].dtype"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(vif_remove['features'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nan=[]\n",
    "for vf in range(len(vif_remove)):\n",
    "    if vif_remove['VIF Factor'][vf]<=0:\n",
    "        print(vif_remove['features'][vf])\n",
    "        nan.append(vif_remove['features'][vf])"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(nan)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_x,test_x,train_y,test_y = train_test_split(data_x_out,data_y,test_size=0.3,random_state = Ran_state)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#######dropout된 값을 train_x로 "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(train_x)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "하이퍼 파라미터 조정용 GridsearchCV 진행"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "LR_grid={'penalty': ['l1', 'l2'], \n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100],'random_state' : [Ran_state]\n",
    "                   }\n",
    "\n",
    "RF_grid={'max_depth': [4,6,8,10,12,14], # max_depth: The maximum depth of the tree.\n",
    "              'n_estimators': [100,200,400,600], # n_estimators: The number of trees in the forest.\n",
    "              'min_samples_split': [50, 100,25,75,125],'random_state' : [Ran_state]\n",
    "              }\n",
    "\n",
    "XGB_grid={'n_estimators' : [200,400,600],\n",
    "              'learning_rate' : [0.01,0.05,0.10,0.15],\n",
    "              'max_depth' : [4,6,8,10,12],'random_state' : [Ran_state]} \n",
    "\n",
    "SVM_grid = {'gamma': [0.01, 0.1, 1, 10, 100], \n",
    "              'C': [0.01, 0.1, 1, 10, 100],'random_state' : [Ran_state]}\n",
    "\n",
    "LR = LogisticRegression()\n",
    "RF = RandomForestClassifier()\n",
    "XGB = XGBClassifier()\n",
    "SVM = SVC()\n",
    "\n",
    "gridlist = [LR_grid,RF_grid,XGB_grid,SVM_grid]\n",
    "classlist = [LR,RF,XGB,SVM]\n",
    "\n",
    "###GridSearchCV로 파라미터 튜닝을 위해 파라미터값 지정"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def gridsearch(gridlist,classlist,train_x,train_y):\n",
    "    for i in range(4):\n",
    "        grid = GridSearchCV(classlist[i], gridlist[i], scoring = 'roc_auc', cv=7,n_jobs=-1)  \n",
    "        grid.fit(train_x, train_y)\n",
    "        print(grid.best_params_)\n",
    "        print(grid.best_score_)\n",
    "        classlist[i] = grid.best_estimator_\n",
    "    return classlist"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "classlist = gridsearch(gridlist,classlist,train_x,train_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "=================================================================="
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "학습 및 예측"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_x.values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_stacking_data(classlist,train_x,train_y,test_x,n_folds=5): #스태킹 앙상블 모델\n",
    "#     train_x = train_x.as_matrix()\n",
    "#     test_x = test_x.as_matrix()\n",
    "    \n",
    "    train_x = train_x.values     #########\n",
    "    test_x = test_x.values       #########\n",
    "    \n",
    "    test_x = test_x.astype(float)\n",
    "    train_x = train_x.astype(float)\n",
    "    models=[classlist,\n",
    "            [LGBMClassifier(n_estimators=400)],\n",
    "            ]\n",
    "    stk=StackNetClassifier(models, metric=\"auc\", folds=n_folds,\n",
    "        restacking=False,use_retraining=True, use_proba=True, \n",
    "        random_state=Ran_state,n_jobs=-1, verbose=1)\n",
    "    stk.fit(train_x,train_y)\n",
    "    preds=stk.predict_proba(test_x)[:,1]\n",
    "    return preds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_curve(classlist,train_x,train_y,test_x,test_y,save_path):\n",
    "    LR = classlist[0]\n",
    "    RF = classlist[1]\n",
    "    XGB = classlist[2]\n",
    "    SVM = classlist[3]\n",
    "\n",
    "    LR.fit(train_x,train_y)\n",
    "    RF.fit(train_x,train_y)\n",
    "    XGB.fit(train_x,train_y)\n",
    "    SVM.fit(train_x,train_y)\n",
    "    \n",
    "    print('==========================================')\n",
    "    print(train_x.columns)\n",
    "    print(LR.coef_)\n",
    "    print('==========================================')\n",
    "\n",
    "    lr_proba = LR.predict_proba(test_x)[:,1]\n",
    "    rf_proba = RF.predict_proba(test_x)[:,1]\n",
    "    xgb_proba = XGB.predict_proba(test_x)[:,1]\n",
    "    SVM_proba = SVM.decision_function(test_x)\n",
    "    lgbm_proba = get_stacking_data(classlist,train_x,train_y,test_x,5)       #######################\n",
    "\n",
    "    fpr_lr,tpr_lr,_=roc_curve(test_y,lr_proba)\n",
    "    fpr_rf,tpr_rf,_=roc_curve(test_y,rf_proba)\n",
    "    fpr_xgb,tpr_xgb,_=roc_curve(test_y,xgb_proba)\n",
    "    fpr_svm,tpr_svm,_ = roc_curve(test_y,SVM_proba)\n",
    "    fpr_lgbm,tpr_lgbm,_=roc_curve(test_y,lgbm_proba)\n",
    "\n",
    "    auc_lr=auc(fpr_lr, tpr_lr)\n",
    "    auc_xgb=auc(fpr_xgb, tpr_xgb)\n",
    "    auc_rf=auc(fpr_rf, tpr_rf)\n",
    "    auc_svm=auc(fpr_svm, tpr_svm)\n",
    "    auc_lgbm=auc(fpr_lgbm, tpr_lgbm)\n",
    "\n",
    "    plt.plot(fpr_lr, tpr_lr, label=\"LR (AUC= %s)\"%(round(auc_lr,3)))\n",
    "    plt.plot(fpr_rf, tpr_rf, label=\"RF (AUC= %s)\"%(round(auc_rf,3)))\n",
    "    plt.plot(fpr_xgb, tpr_xgb, label=\"XGB (AUC= %s)\"%(round(auc_xgb,3)))\n",
    "    plt.plot(fpr_svm, tpr_svm, label=\"SVM (AUC= %s)\"%(round(auc_svm,3)))\n",
    "    plt.plot(fpr_lgbm, tpr_lgbm, label=\"S.ESB (AUC= %s)\"%(round(auc_lgbm,3)))\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('1-specificity',fontsize=12)\n",
    "    plt.ylabel('sensitivity',fontsize=15)\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\",fontsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "    Importance = pd.DataFrame(train_x.columns,columns=['feature'])\n",
    "    suf = ('_LR','_RF','_XGB','_SVM')\n",
    "    \n",
    "    for i in range(4):\n",
    "        PI = P_Importance(classlist[i],test_x,test_y).loc[:,['feature','weight']]\n",
    "        Importance = pd.merge(Importance,PI,on='feature',suffixes=(suf[i-1],suf[i]))\n",
    "    \n",
    "    Importance['meanscore'] = Importance.apply(lambda x: np.mean(x[1:5]),1)\n",
    "    ftr_top20 = Importance.sort_values(ascending=False,by='meanscore')[:]\n",
    "    \n",
    "    fig = go.Figure([go.Bar(x=ftr_top20.meanscore, y=ftr_top20.feature,orientation='h',marker={'color': ftr_top20.meanscore,'colorscale': 'Oryel'})])\n",
    "    fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=500,\n",
    "    height=800,\n",
    "    yaxis=dict(type='category',tickfont_size=18)\n",
    "    \n",
    "    )\n",
    "    \n",
    "    \n",
    "    fig.show()\n",
    "    plt.savefig(save_path, dpi=300, facecolor='w', edgecolor='w')       ###############\n",
    "    \n",
    "    return Importance"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# importance\n",
    "\n",
    "def P_Importance(model,test_x,test_y): #permutation importance 중요도 뽑기 \n",
    "    perm = PermutationImportance(model, scoring = \"roc_auc\", random_state = Ran_state).fit(test_x, test_y) \n",
    "    ftr_importances = eli5.format_as_dataframe(eli5.explain_weights(perm,top=134, feature_names = test_x.columns.tolist()))\n",
    "    ftr_top = ftr_importances.sort_values(ascending=False,by='feature')[:]\n",
    "    return ftr_top"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ftr_importances = create_curve(classlist,train_x,train_y,test_x,test_y, './featureData/importanced_roc_1219')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ftr_importances"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "위에서 뽑은 importance 이용 Feature selection 과정"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def roc_score(classlist,train_x,train_y,test_x,test_y):\n",
    "    LR = classlist[0]\n",
    "    RF = classlist[1]\n",
    "    XGB = classlist[2]\n",
    "    SVM = classlist[3]\n",
    "    \n",
    "    LR.fit(train_x,train_y)\n",
    "    RF.fit(train_x,train_y)\n",
    "    XGB.fit(train_x,train_y)\n",
    "    SVM.fit(train_x,train_y)\n",
    "    \n",
    "    lr_proba = LR.predict_proba(test_x)[:,1]\n",
    "    rf_proba = RF.predict_proba(test_x)[:,1]\n",
    "    xgb_proba = XGB.predict_proba(test_x)[:,1]\n",
    "    SVM_proba = SVM.decision_function(test_x)\n",
    "    \n",
    "    lr_score = roc_auc_score(test_y, lr_proba)\n",
    "    rf_score = roc_auc_score(test_y, rf_proba)\n",
    "    xgb_score = roc_auc_score(test_y, xgb_proba)\n",
    "    SVM_score = roc_auc_score(test_y, SVM_proba)\n",
    "    \n",
    "    print(lr_score,rf_score,xgb_score,SVM_score)\n",
    "    \n",
    "    Importance = pd.DataFrame(train_x.columns,columns=['feature'])\n",
    "    suf = ('_LR','_RF','_XGB','_SVM')\n",
    "    \n",
    "    for i in range(4):\n",
    "        PI = P_Importance(classlist[i],test_x,test_y).loc[:,['feature','weight']]\n",
    "        Importance = pd.merge(Importance,PI,on='feature',suffixes=(suf[i-1],suf[i]))\n",
    "    \n",
    "    Importance['meanscore'] = Importance.apply(lambda x: np.mean(x[1:5]),1)\n",
    "    \n",
    "    \n",
    "    return np.max([lr_score,rf_score,xgb_score,SVM_score]),Importance"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def featurefind(ftr_importances,data_x,data_y,classlist):\n",
    "    best_score = 0\n",
    "    for i in range(len(ftr_importances.feature)-2):\n",
    "        ftr_bot = ftr_importances.sort_values(ascending=True,by='meanscore')[:1]\n",
    "        data_x = data_x.drop(ftr_bot.feature ,1)\n",
    "        train_x,test_x,train_y,test_y = train_test_split(data_x,data_y,test_size=0.3,random_state = 25)\n",
    "        meanscore,ftr_importances = roc_score(classlist,train_x,train_y,test_x,test_y)\n",
    "        print(i)\n",
    "        if meanscore>=best_score:\n",
    "            best_score = meanscore\n",
    "            best_bot = data_x\n",
    "            print(\"best_score = \",best_score)\n",
    "    return best_bot"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#초회 실행용\n",
    "new_data_x = featurefind(ftr_importances,data_x,data_y,classlist)\n",
    "train_x,test_x,train_y,test_y = train_test_split(new_data_x,data_y,test_size=0.3,random_state = Ran_state)\n",
    "classlist = gridsearch(gridlist,classlist,train_x, train_y)\n",
    "#train_x, train_y = SMOTETomek(random_state=25).fit_sample(train_x, train_y)\n",
    "ftr_importances = create_curve(classlist,train_x,train_y,test_x,test_y,'./featureData/selected_roc1_2020')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(ftr_importances)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#반복용\n",
    "new_data_x = featurefind(ftr_importances,new_data_x,data_y,classlist)\n",
    "train_x,test_x,train_y,test_y = train_test_split(new_data_x,data_y,test_size=0.3,random_state = Ran_state)\n",
    "classlist = gridsearch(gridlist,classlist,train_x, train_y)\n",
    "#train_x, train_y = SMOTETomek(random_state=25).fit_sample(train_x, train_y)\n",
    "ftr_importances = create_curve(classlist,train_x,train_y,test_x,test_y,'./featureData/selected_roc2_1219')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ftr_importances = create_curve(classlist,train_x,train_y,test_x,test_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "남은 변수가 너무 많다싶으면 반복용 코드 반복실행"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#반복용\n",
    "new_data_x = featurefind(ftr_importances,new_data_x,data_y,classlist)\n",
    "train_x,test_x,train_y,test_y = train_test_split(new_data_x,data_y,test_size=0.3,random_state = Ran_state)\n",
    "classlist = gridsearch(gridlist,classlist,train_x, train_y)\n",
    "#train_x, train_y = SMOTETomek(random_state=25).fit_sample(train_x, train_y)\n",
    "ftr_importances = create_curve(classlist,train_x,train_y,test_x,test_y,'./featureData/selected_roc2_1219')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#반복용\n",
    "new_data_x = featurefind(ftr_importances,new_data_x,data_y,classlist)\n",
    "train_x,test_x,train_y,test_y = train_test_split(new_data_x,data_y,test_size=0.3,random_state = Ran_state)\n",
    "classlist = gridsearch(gridlist,classlist,train_x, train_y)\n",
    "#train_x, train_y = SMOTETomek(random_state=25).fit_sample(train_x, train_y)\n",
    "ftr_importances = create_curve(classlist,train_x,train_y,test_x,test_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#반복용\n",
    "new_data_x = featurefind(ftr_importances,new_data_x,data_y,classlist)\n",
    "train_x,test_x,train_y,test_y = train_test_split(new_data_x,data_y,test_size=0.3,random_state = Ran_state)\n",
    "classlist = gridsearch(gridlist,classlist,train_x, train_y)\n",
    "#train_x, train_y = SMOTETomek(random_state=25).fit_sample(train_x, train_y)\n",
    "ftr_importances = create_curve(classlist,train_x,train_y,test_x,test_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#반복용\n",
    "new_data_x = featurefind(ftr_importances,new_data_x,data_y,classlist)\n",
    "train_x,test_x,train_y,test_y = train_test_split(new_data_x,data_y,test_size=0.3,random_state = Ran_state)\n",
    "classlist = gridsearch(gridlist,classlist,train_x, train_y)\n",
    "#train_x, train_y = SMOTETomek(random_state=25).fit_sample(train_x, train_y)\n",
    "ftr_importances = create_curve(classlist,train_x,train_y,test_x,test_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#반복용\n",
    "new_data_x = featurefind(ftr_importances,new_data_x,data_y,classlist)\n",
    "train_x,test_x,train_y,test_y = train_test_split(new_data_x,data_y,test_size=0.3,random_state = Ran_state)\n",
    "classlist = gridsearch(gridlist,classlist,train_x, train_y)\n",
    "#train_x, train_y = SMOTETomek(random_state=25).fit_sample(train_x, train_y)\n",
    "ftr_importances = create_curve(classlist,train_x,train_y,test_x,test_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#반복용\n",
    "new_data_x = featurefind(ftr_importances,new_data_x,data_y,classlist)\n",
    "train_x,test_x,train_y,test_y = train_test_split(new_data_x,data_y,test_size=0.3,random_state = Ran_state)\n",
    "classlist = gridsearch(gridlist,classlist,train_x, train_y)\n",
    "#train_x, train_y = SMOTETomek(random_state=25).fit_sample(train_x, train_y)\n",
    "ftr_importances = create_curve(classlist,train_x,train_y,test_x,test_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#반복용\n",
    "new_data_x = featurefind(ftr_importances,new_data_x,data_y,classlist)\n",
    "train_x,test_x,train_y,test_y = train_test_split(new_data_x,data_y,test_size=0.3,random_state = Ran_state)\n",
    "classlist = gridsearch(gridlist,classlist,train_x, train_y)\n",
    "#train_x, train_y = SMOTETomek(random_state=25).fit_sample(train_x, train_y)\n",
    "ftr_importances = create_curve(classlist,train_x,train_y,test_x,test_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#반복용\n",
    "new_data_x = featurefind(ftr_importances,new_data_x,data_y,classlist)\n",
    "train_x,test_x,train_y,test_y = train_test_split(new_data_x,data_y,test_size=0.3,random_state = Ran_state)\n",
    "classlist = gridsearch(gridlist,classlist,train_x, train_y)\n",
    "#train_x, train_y = SMOTETomek(random_state=25).fit_sample(train_x, train_y)\n",
    "ftr_importances = create_curve(classlist,train_x,train_y,test_x,test_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#반복용\n",
    "new_data_x = featurefind(ftr_importances,new_data_x,data_y,classlist)\n",
    "train_x,test_x,train_y,test_y = train_test_split(new_data_x,data_y,test_size=0.3,random_state = Ran_state)\n",
    "classlist = gridsearch(gridlist,classlist,train_x, train_y)\n",
    "#train_x, train_y = SMOTETomek(random_state=25).fit_sample(train_x, train_y)\n",
    "ftr_importances = create_curve(classlist,train_x,train_y,test_x,test_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#반복용\n",
    "new_data_x = featurefind(ftr_importances,new_data_x,data_y,classlist)\n",
    "train_x,test_x,train_y,test_y = train_test_split(new_data_x,data_y,test_size=0.3,random_state = Ran_state)\n",
    "classlist = gridsearch(gridlist,classlist,train_x, train_y)\n",
    "#train_x, train_y = SMOTETomek(random_state=25).fit_sample(train_x, train_y)\n",
    "ftr_importances = create_curve(classlist,train_x,train_y,test_x,test_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#반복용\n",
    "new_data_x = featurefind(ftr_importances,new_data_x,data_y,classlist)\n",
    "train_x,test_x,train_y,test_y = train_test_split(new_data_x,data_y,test_size=0.3,random_state = Ran_state)\n",
    "classlist = gridsearch(gridlist,classlist,train_x, train_y)\n",
    "#train_x, train_y = SMOTETomek(random_state=25).fit_sample(train_x, train_y)\n",
    "ftr_importances = create_curve(classlist,train_x,train_y,test_x,test_y)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ftr_importances = create_curve(classlist,train_x,train_y,test_x,test_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}